diff --git a/include/onnxruntime/core/providers/etglow/etglow_provider_options.h b/include/onnxruntime/core/providers/etglow/etglow_provider_options.h
new file mode 100644
index 0000000000..5119b691bb
--- /dev/null
+++ b/include/onnxruntime/core/providers/etglow/etglow_provider_options.h
@@ -0,0 +1,70 @@
+// Copyright (c) Microsoft Corporation. All rights reserved.
+// Licensed under the MIT License.
+
+#pragma once
+
+enum OrtEtGlowTraceFlags {
+  ETGLOW_TRACE_NONE = 0x000,                 // Do not trace anything
+  ETGLOW_TRACE_GLOW = 0x001,                 // Enables glow tracing
+  ETGLOW_TRACE_NEURALIZER = 0x002,           // Enables neuralizer (API) tracing
+  ETGLOW_TRACE_RUNTIME = 0x004,              // Enables host runtime (API) tracing + Device events over streams
+  ETGLOW_TRACE_DEVICE_KERNEL_NODES = 0x008,  // Enables device kernel nodes instrumentation. Can cause severe performance hit.
+  ETGLOW_TRACE_DEVICE_KERNEL_INST = 0x010,   // Enables device kernel user instrumentation. Can cause severe performance hit. This trace events are not incorporated into ORT trace.
+  ETGLOW_TRACE_DEFAULT = ETGLOW_TRACE_GLOW | ETGLOW_TRACE_NEURALIZER | ETGLOW_TRACE_RUNTIME,
+  ETGLOW_TRACE_ALL = ETGLOW_TRACE_GLOW | ETGLOW_TRACE_NEURALIZER | ETGLOW_TRACE_RUNTIME | ETGLOW_TRACE_DEVICE_KERNEL_NODES | ETGLOW_TRACE_DEVICE_KERNEL_INST,
+};
+
+struct OrtEtGlowProviderOptions {
+#ifdef __cplusplus
+  OrtEtGlowProviderOptions(
+      int device_id = 0,
+      size_t device_mem_limit = std::numeric_limits<size_t>::max(),
+      int arena_extend_strategy = 1,
+      int et_compile_only = 0,
+      int et_dump_subgraphs = 0,
+      int et_greedy = 0,
+      int et_offline_mode = 0,
+      int et_fail_if_cannot_run_whole_graph = 0,
+      const char* et_onnx_symbols = nullptr,
+      const char* et_glow_api_params = nullptr,
+      const char* et_bundle_cache_prefix = nullptr,
+      const char* et_export_bundle_path = nullptr,
+      const char* et_traces_prefix = nullptr,
+      int et_traces_flags = OrtEtGlowTraceFlags::ETGLOW_TRACE_DEFAULT)
+      : device_id{device_id},
+        device_mem_limit{device_mem_limit},
+        arena_extend_strategy{arena_extend_strategy},
+        et_compile_only{et_compile_only},
+        et_dump_subgraphs{et_dump_subgraphs},
+        et_greedy{et_greedy},
+        et_offline_mode{et_offline_mode},
+        et_fail_if_cannot_run_whole_graph{et_fail_if_cannot_run_whole_graph},
+        et_onnx_symbols{et_onnx_symbols},
+        et_glow_api_params{et_glow_api_params},
+        et_bundle_cache_prefix{et_bundle_cache_prefix},
+        et_export_bundle_path{et_export_bundle_path},
+        et_traces_prefix{et_traces_prefix},
+        et_traces_flags{et_traces_flags} {}
+#endif
+
+  int device_id;                          // et device id.
+  size_t device_mem_limit;                // Device memory limit. Defaults to SIZE_MAX.
+  int arena_extend_strategy;              // Strategy used to grow the memory arena. 0 = kNextPowerOfTwo, 1 = kSameAsRequested. Defaults to 1
+  int et_compile_only;                    // compile models only. Default 0 = false, nonzero = true
+  int et_dump_subgraphs;                  // dump ETGLOW subgraph. Default 0 = false, nonzero = true
+  int et_greedy;                          // accept any graph regardless of ET Compiler support. Default 0 = false, nonzero = true
+  int et_offline_mode;                    // Apply ORT Graph Optimizations (does not compile)
+  int et_fail_if_cannot_run_whole_graph;  // only for EP debugging purposes. Default 0, nonzero = true
+
+  const char* et_onnx_symbols;  // compacted list of onnx symbols
+                                // example: "foo=44;bar=11"
+
+  const char* et_glow_api_params;  // compacted list of GlowAPI parameters
+                                   // example: "device-type=sysemu;placeholder=X;placeholder=Y;implicit-placeholder=Z;extra-etsoc-params='param1=value1|param2=value2|...'")
+
+  const char* et_bundle_cache_prefix;  // specify bundle cache prefix
+  const char* et_export_bundle_path;   // specify location where to copy internally generated bundle
+
+  const char* et_traces_prefix;  // specify trace folder prefix
+  int et_traces_flags;           // Controls trace level of detail
+};
diff --git a/onnxruntime/core/providers/etglow/etglow_allocator.cc b/onnxruntime/core/providers/etglow/etglow_allocator.cc
new file mode 100644
index 0000000000..b326e47225
--- /dev/null
+++ b/onnxruntime/core/providers/etglow/etglow_allocator.cc
@@ -0,0 +1,37 @@
+// Copyright (c) Microsoft Corporation. All rights reserved.
+// Licensed under the MIT License.
+
+#include "core/providers/etglow/etglow_allocator.h"
+#include "core/providers/shared_library/provider_api.h"
+
+#include "runtime/IRuntime.h"
+
+namespace onnxruntime {
+
+void* EtGlowDeviceAllocator::Alloc(size_t size) {
+  void* p = nullptr;
+  if (size > 0) {
+    ORT_TRY {
+      p = rt_->mallocDevice(static_cast<rt::DeviceId>(0), size);
+      LOGS_DEFAULT(INFO) << "[ETGLOW EP] allocated p: " << p << " size: " << size;
+    }
+    ORT_CATCH(const rt::Exception& rt_exception) {
+      // attempted allocation can throw. we want to treat this the same as if it returned nullptr
+      // so swallow the exception.
+      // This allows ORT to react (for instance being less aggressive with the allocation size & retrying)
+      LOGS_DEFAULT(WARNING) << "[ETGLOW EP] device memory allocation of size: " << size << " failed!";
+    }
+  }
+  return p;
+}
+
+void EtGlowDeviceAllocator::Free(void* p) {
+  if (p == nullptr) {
+    LOGS_DEFAULT(WARNING) << "[ETGLOW EP] Cannot Free p: " << p << " is nullptr!";
+    return;
+  }
+  LOGS_DEFAULT(INFO) << "[ETGLOW EP] Free p: " << p;
+  rt_->freeDevice(static_cast<rt::DeviceId>(0), static_cast<std::byte*>(p));
+}
+
+}  // namespace onnxruntime
diff --git a/onnxruntime/core/providers/etglow/etglow_allocator.h b/onnxruntime/core/providers/etglow/etglow_allocator.h
new file mode 100644
index 0000000000..da468abc22
--- /dev/null
+++ b/onnxruntime/core/providers/etglow/etglow_allocator.h
@@ -0,0 +1,27 @@
+#pragma once
+
+#include "core/framework/allocator.h"
+
+namespace rt {
+class IRuntime;
+}
+
+namespace onnxruntime {
+
+class EtGlowDeviceAllocator : public IAllocator {
+ public:
+  EtGlowDeviceAllocator(OrtDevice::DeviceId device_id, const char* name, rt::IRuntime* rt)
+      : IAllocator(
+            OrtMemoryInfo(name, OrtAllocatorType::OrtDeviceAllocator,
+                          OrtDevice(OrtDevice::GPU, OrtDevice::MemType::DEFAULT, device_id),
+                          device_id, OrtMemTypeDefault)),
+        rt_{rt} {}
+
+  void* Alloc(size_t size) override;
+  void Free(void* p) override;
+
+ private:
+  rt::IRuntime* rt_;
+};
+
+}  // namespace onnxruntime
diff --git a/onnxruntime/core/providers/etglow/etglow_compiled_models_registry.h b/onnxruntime/core/providers/etglow/etglow_compiled_models_registry.h
new file mode 100644
index 0000000000..82f621c73f
--- /dev/null
+++ b/onnxruntime/core/providers/etglow/etglow_compiled_models_registry.h
@@ -0,0 +1,43 @@
+// Copyright (c) Microsoft Corporation. All rights reserved.
+// Licensed under the MIT License.
+
+#pragma once
+
+#include <glow/GlowAPI/etsoc.h>
+
+#include <unordered_map>
+#include <memory>
+#include <string>
+
+namespace onnxruntime {
+
+using GlowAPI = etsoc::GlowAPI;
+
+class EtGlowCompiledModelsRegistry {
+ public:
+  bool HasCompiledModel(const std::string& bundle_path) {
+    std::scoped_lock lock(mutex_);
+    return compiled_models_.find(bundle_path) != compiled_models_.end();
+  }
+
+  void StoreCompiledModel(const std::string& bundle_path, std::unique_ptr<GlowAPI::CompiledModel>&& compiled_model) {
+    std::scoped_lock lock(mutex_);
+    compiled_models_.try_emplace(bundle_path, std::move(compiled_model));
+  }
+
+  void RemoveCompiledModel(const std::string& bundle_path) {
+    std::scoped_lock lock(mutex_);
+    compiled_models_.erase(bundle_path);
+  }
+
+  GlowAPI::CompiledModel* GetCompiledModel(const std::string& bundle_path) {
+    std::scoped_lock lock(mutex_);
+    return compiled_models_[bundle_path].get();
+  }
+
+ private:
+  std::mutex mutex_;
+  std::unordered_map<std::string, std::unique_ptr<GlowAPI::CompiledModel>> compiled_models_;
+};
+
+}  // end namespace onnxruntime
diff --git a/onnxruntime/core/providers/etglow/etglow_data_transfer.cc b/onnxruntime/core/providers/etglow/etglow_data_transfer.cc
new file mode 100644
index 0000000000..fdb6640e05
--- /dev/null
+++ b/onnxruntime/core/providers/etglow/etglow_data_transfer.cc
@@ -0,0 +1,99 @@
+// Copyright (c) Microsoft Corporation. All rights reserved.
+// Licensed under the MIT License.
+
+#include "core/providers/shared_library/provider_api.h"
+#include "core/providers/etglow/etglow_provider_factory.h"
+#include "etglow_data_transfer.h"
+#include "etglow_stream_handle.h"
+
+namespace onnxruntime {
+
+bool EtGlowDataTransfer::CanCopy(const OrtDevice& src_device, const OrtDevice& dst_device) const {
+  return src_device.Type() == OrtDevice::GPU || src_device.MemType() == OrtDevice::MemType::DEFAULT ||
+         dst_device.Type() == OrtDevice::GPU || dst_device.MemType() == OrtDevice::MemType::DEFAULT;
+}
+
+common::Status EtGlowDataTransfer::CopyTensor(const Tensor& src, Tensor& dst) const {
+  // 0. Get deviceId
+  const auto& src_device = src.Location().device;
+  const auto& dst_device = dst.Location().device;
+  int device_id = (dst_device.Type() == OrtDevice::GPU) ? dst_device.Id() : src_device.Id();
+
+  rt::StreamId stream_id = streams_reg_.GetStreamForDevice(device_id);
+  LOGS_DEFAULT(INFO) << "[ETGLOW EP] copy stream_id: " << static_cast<int>(stream_id) << " dev_src_id: " << src_device.Id() << " dev_dst_id: " << dst_device.Id();
+
+  // Create ephemeral EtGlowStream handle (doesn't take stream ownership)
+  EtGlowStream stream(stream_id, dst_device, streams_reg_);
+
+  // Perform async copy
+  ORT_RETURN_IF_ERROR(CopyTensorAsync(src, dst, stream));
+
+  stream.Flush();
+
+  LOGS_DEFAULT(INFO) << "[ETGLOW EP] copy stream_id: " << static_cast<int>(stream_id) << " done";
+
+  return Status::OK();
+}
+
+common::Status EtGlowDataTransfer::CopyTensorAsync(const Tensor& src, Tensor& dst, Stream& stream) const {
+  size_t bytes = src.SizeInBytes();
+  const void* src_data = src.DataRaw();
+  void* dst_data = dst.MutableDataRaw();
+
+  const auto& src_device = src.Location().device;
+  const auto& dst_device = dst.Location().device;
+
+  auto& etglow_stream = dynamic_cast<EtGlowStream&>(stream);
+  rt::StreamId etrt_stream_id = etglow_stream.GetStreamId();
+
+  const auto* src_ptr = static_cast<const std::byte*>(src_data);
+  auto* dst_ptr = static_cast<std::byte*>(dst_data);
+
+  auto copy_type_str = [](const OrtDevice::DeviceType& dst_device_t, const OrtDevice::DeviceType& src_device_t) {
+    if (dst_device_t == OrtDevice::GPU && src_device_t == OrtDevice::GPU) {
+      return "GPU->GPU";
+    } else if (dst_device_t == OrtDevice::GPU && src_device_t != OrtDevice::GPU) {
+      return "CPU->GPU";
+    } else if (dst_device_t == OrtDevice::CPU && src_device_t == OrtDevice::GPU) {
+      return "GPU->CPU";
+    } else {
+      return "Other";
+    }
+  };
+  if (src_ptr == nullptr || dst_ptr == nullptr) {
+    LOGS_DEFAULT(WARNING) << "[ETGLOW EP] Cannot async copy " << copy_type_str(dst_device.Type(), src_device.Type())
+                          << " stream_id: " << static_cast<int>(etrt_stream_id)
+                          << " dev_src_id: " << src_device.Id()
+                          << " dev_dst_id: " << dst_device.Id()
+                          << " src_ptr: " << src_ptr
+                          << " dst_ptr: " << dst_ptr
+                          << " size: " << bytes
+                          << " at least one ptr is a nullptr!";
+    return Status::OK();  // do not make the execution crash
+  }
+  LOGS_DEFAULT(INFO) << "[ETGLOW EP] async copy " << copy_type_str(dst_device.Type(), src_device.Type())
+                     << " stream_id: " << static_cast<int>(etrt_stream_id)
+                     << " dev_src_id: " << src_device.Id()
+                     << " dev_dst_id: " << dst_device.Id()
+                     << " src_ptr: " << src_ptr
+                     << " dst_ptr: " << dst_ptr
+                     << " size: " << bytes;
+
+  constexpr bool barrier = true;
+  if (dst_device.Type() == OrtDevice::GPU && src_device.Type() == OrtDevice::GPU) {
+    auto etrt_device_id = static_cast<rt::DeviceId>(dst_device.Id());
+    rt_->memcpyDeviceToDevice(etrt_stream_id, etrt_device_id, src_ptr, dst_ptr, bytes, barrier);
+  } else if (dst_device.Type() == OrtDevice::GPU && src_device.Type() != OrtDevice::GPU) {
+    rt_->memcpyHostToDevice(etrt_stream_id, src_ptr, dst_ptr, bytes, barrier);
+  } else if (dst_device.Type() == OrtDevice::CPU && src_device.Type() == OrtDevice::GPU) {
+    rt_->memcpyDeviceToHost(etrt_stream_id, src_ptr, dst_ptr, bytes, barrier);
+  } else {
+    // sync the stream first to make sure the data arrived
+    ORT_RETURN_IF_ERROR(streams_reg_.Synchronize(etrt_stream_id));
+    std::memcpy(dst_data, src_data, bytes);
+  }
+
+  return Status::OK();
+}
+
+}  // namespace onnxruntime
\ No newline at end of file
diff --git a/onnxruntime/core/providers/etglow/etglow_data_transfer.h b/onnxruntime/core/providers/etglow/etglow_data_transfer.h
new file mode 100644
index 0000000000..0071e70a24
--- /dev/null
+++ b/onnxruntime/core/providers/etglow/etglow_data_transfer.h
@@ -0,0 +1,33 @@
+// Copyright (c) Microsoft Corporation. All rights reserved.
+// Licensed under the MIT License.
+
+#pragma once
+
+#include "core/framework/data_transfer.h"
+#include "runtime/IRuntime.h"
+
+namespace onnxruntime {
+struct EtRt_StreamsRegistry;
+
+class EtGlowDataTransfer : public IDataTransfer {
+ public:
+  explicit EtGlowDataTransfer(rt::IRuntime* rt, EtRt_StreamsRegistry& streams_reg)
+      : rt_{rt}, streams_reg_{streams_reg} {};
+  ~EtGlowDataTransfer() override = default;
+
+  EtGlowDataTransfer(const EtGlowDataTransfer&) = delete;
+  EtGlowDataTransfer& operator=(const EtGlowDataTransfer&) = delete;
+
+  EtGlowDataTransfer(EtGlowDataTransfer&&) noexcept = delete;
+  EtGlowDataTransfer& operator=(EtGlowDataTransfer&&) noexcept = delete;
+
+  [[nodiscard]] bool CanCopy(const OrtDevice& src_device, const OrtDevice& dst_device) const override;
+  common::Status CopyTensor(const Tensor& src, Tensor& dst) const override;
+  common::Status CopyTensorAsync(const Tensor& src, Tensor& dst, Stream& stream) const override;
+
+ private:
+  rt::IRuntime* rt_;
+  EtRt_StreamsRegistry& streams_reg_;
+};
+
+}  // namespace onnxruntime
\ No newline at end of file
diff --git a/onnxruntime/core/providers/etglow/etglow_execution_provider.cc b/onnxruntime/core/providers/etglow/etglow_execution_provider.cc
new file mode 100644
index 0000000000..8bae398246
--- /dev/null
+++ b/onnxruntime/core/providers/etglow/etglow_execution_provider.cc
@@ -0,0 +1,1150 @@
+// Copyright (c) Microsoft Corporation. All rights reserved.
+// Licensed under the MIT License.
+
+#include "etglow_execution_provider.h"
+#include "etglow_execution_provider_utils.h"
+#include "etglow_execution_provider_bundle.h"
+#include "etglow_data_transfer.h"
+#include "etglow_stream_handle.h"
+#include "etglow_allocator.h"
+#include "etglow_profiler.h"
+#include "etglow_provider_factory.h"
+
+#include "core/platform/ort_mutex.h"
+#include "core/providers/shared_library/provider_api.h"
+
+#define ORT_API_MANUAL_INIT
+#include "core/session/onnxruntime_cxx_api.h"
+
+#include <algorithm>
+#include <fstream>
+#include <memory>
+#include <mutex>
+#include <functional>
+#include <string>
+#include <utility>
+
+#include "glow/GlowAPI/etsoc.h"
+
+namespace onnxruntime {
+
+class Memcpy final : public OpKernel {
+ public:
+  using OpKernel::OpKernel;  // Directly inherit parent constructor
+
+  Status Compute(OpKernelContext* ctx) const override {
+    const auto* X_type = ctx->InputType(0);
+    if (X_type->IsTensorType()) {
+      const auto* X = ctx->Input<Tensor>(0);
+      ORT_ENFORCE(X != nullptr, "Memcpy: Input tensor is nullptr.");
+      Tensor* Y = ctx->Output(0, X->Shape());
+      ORT_ENFORCE(Y != nullptr, "Memcpy: Failed to allocate output tensor.");
+      const auto* gpu_data_transfer = Info().GetDataTransferManager().GetDataTransfer(X->Location().device, Y->Location().device);
+      ORT_RETURN_IF_ERROR(gpu_data_transfer->CopyTensorAsync(*X, *Y, *ctx->GetComputeStream()));
+    } else {
+      if (X_type->IsSparseTensorType()) {
+        return {common::ONNXRUNTIME, common::FAIL, "Memcpy: Unsupported SparseTensorType."};
+      } else if (X_type->IsTensorSequenceType()) {
+        const TensorSeq* X = ctx->Input<TensorSeq>(0);
+        ORT_ENFORCE(X != nullptr, "Memcpy: Input tensor sequence is nullptr.");
+        TensorSeq* Y = ctx->Output<TensorSeq>(0);
+        ORT_ENFORCE(Y != nullptr, "Memcpy: Failed to allocate output tensor sequence.");
+        const auto* X_dtype = X->DataType();
+        Y->SetType(X_dtype);
+        AllocatorPtr alloc;
+
+        // If we are copying contents to CUDA, the allocator to use
+        // to allocate the buffers of the new tensors in the sequence
+        // can be temp space allocator associated with the CUDA EP
+        if (Node().OpType() == "MemcpyFromHost") {
+          ORT_RETURN_IF_ERROR(ctx->GetTempSpaceAllocator(&alloc));
+        } else {
+          // If we are copying contents to CPU (op type is "MemcpyToHost"),
+          // the allocator to use to allocate the buffers of the new tensors
+          // in the sequence will be the allocator from the CPU EP
+          ORT_RETURN_IF_ERROR(ctx->GetTempSpaceCPUAllocator(&alloc));
+        }
+        auto X_size = X->Size();
+        Y->Reserve(X_size);
+        for (size_t i = 0; i < X_size; ++i) {
+          const Tensor& source_tensor = X->Get(i);
+          std::unique_ptr<Tensor> target_tensor = Tensor::Create(source_tensor.DataType(), source_tensor.Shape(), alloc);
+          const auto* gpu_data_transfer = Info().GetDataTransferManager().GetDataTransfer(source_tensor.Location().device,
+                                                                                          target_tensor->Location().device);
+          ORT_RETURN_IF_ERROR(gpu_data_transfer->CopyTensorAsync(source_tensor, *target_tensor, *ctx->GetComputeStream()));
+          Y->Add(std::move(*target_tensor));
+        }
+      } else {
+        return {common::ONNXRUNTIME, common::FAIL, "Memcpy: Unsupported input type."};
+      }
+    }
+    return Status::OK();
+  }
+};
+
+ONNX_OPERATOR_KERNEL_EX(
+    MemcpyFromHost,
+    kOnnxDomain,
+    1,
+    kEtGlowExecutionProvider,
+    (*KernelDefBuilder::Create())
+        .InputMemoryType(OrtMemTypeCPUInput, 0)
+        .TypeConstraint("T", DataTypeImpl::AllFixedSizeTensorTypes()),
+    Memcpy);
+
+ONNX_OPERATOR_KERNEL_EX(
+    MemcpyToHost,
+    kOnnxDomain,
+    1,
+    kEtGlowExecutionProvider,
+    (*KernelDefBuilder::Create())
+        .OutputMemoryType(OrtMemTypeCPUOutput, 0)
+        .TypeConstraint("T", DataTypeImpl::AllFixedSizeTensorTypes()),
+    Memcpy);
+
+class ONNX_OPERATOR_KERNEL_CLASS_NAME(kEtGlowExecutionProvider, kOnnxDomain, 1, MemcpyFromHost);
+class ONNX_OPERATOR_KERNEL_CLASS_NAME(kEtGlowExecutionProvider, kOnnxDomain, 1, MemcpyToHost);
+
+#define ETGLOW_KERNEL_DEF(operator, domain, startver, endver, types)                              \
+                                                                                                  \
+  class operator final : public OpKernel {                                                        \
+   public:                                                                                        \
+    using OpKernel::OpKernel; /* Directly inherit parent constructor */                           \
+    Status Compute(OpKernelContext* ctx) const override {                                         \
+      /* Empty implementation. */                                                                 \
+      ORT_UNUSED_PARAMETER(ctx);                                                                  \
+      return Status::OK();                                                                        \
+    }                                                                                             \
+  };                                                                                              \
+                                                                                                  \
+  ONNX_OPERATOR_VERSIONED_KERNEL_EX(operator, domain, startver, endver, kEtGlowExecutionProvider, \
+                                    (*KernelDefBuilder::Create())types, operator);                \
+  class ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kEtGlowExecutionProvider, domain, startver, endver, operator);
+
+#include "etglow_kernels.inc"
+#undef ETGLOW_KERNEL_DEF
+
+// Scan operator needs a special treatment due to the templatized format imposed by the forward declaration in provider_api.h.
+template <int OpSet>
+class Scan final : public OpKernel {
+ public:
+  using OpKernel::OpKernel; /* Directly inherit parent constructor */
+
+  Status Compute(OpKernelContext* ctx) const override {
+    /* Empty implementation. */
+    ORT_UNUSED_PARAMETER(ctx);
+    return Status::OK();
+  }
+};
+
+ONNX_OPERATOR_VERSIONED_KERNEL_EX(Scan, kOnnxDomain, 1, 21, kEtGlowExecutionProvider,
+                                  (*KernelDefBuilder::Create()).TypeConstraint("V", DataTypeImpl::AllTensorTypes()), Scan<9>);
+class ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kEtGlowExecutionProvider, kOnnxDomain, 1, 21, Scan);
+
+static std::shared_ptr<KernelRegistry> s_kernel_registry;
+static std::shared_ptr<KernelRegistry> s_kernel_registry_offline;
+
+void InitializeRegistry() {
+  s_kernel_registry = KernelRegistry::Create();
+
+  static const std::array<BuildKernelCreateInfoFn, 2> function_table = {
+      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kEtGlowExecutionProvider, kOnnxDomain, 1, MemcpyFromHost)>,
+      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kEtGlowExecutionProvider, kOnnxDomain, 1, MemcpyToHost)>,
+  };
+
+  for (const auto& function_table_entry : function_table) {
+    ORT_THROW_IF_ERROR(s_kernel_registry->Register(function_table_entry()));
+  }
+
+  // Initialize registry for offline mode.
+  s_kernel_registry_offline = KernelRegistry::Create();
+  static const std::array<BuildKernelCreateInfoFn, 2 + /* num dummy kernels */ 195> offline_mode_function_table = {
+      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kEtGlowExecutionProvider, kOnnxDomain, 1, MemcpyFromHost)>,
+      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kEtGlowExecutionProvider, kOnnxDomain, 1, MemcpyToHost)>,
+#define ETGLOW_KERNEL_DEF(operator, domain, startver, endver, types) \
+  BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kEtGlowExecutionProvider, domain, startver, endver, operator)>,
+#include "etglow_kernels.inc"
+#undef ETGLOW_KERNEL_DEF
+      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kEtGlowExecutionProvider, kOnnxDomain, 1, 21, Scan)>,
+  };
+
+  for (auto& function_table_entry : offline_mode_function_table) {
+    ORT_THROW_IF_ERROR(s_kernel_registry_offline->Register(function_table_entry()));
+  }
+}
+
+void DeleteRegistry() {
+  s_kernel_registry.reset();
+  s_kernel_registry_offline.reset();
+}
+
+void EtGlowExecutionProvider::addToProfilerThreadMap() {
+  std::scoped_lock lock(thread_map_mutex_);
+  auto syscall_tid = logging::GetThreadId();
+  auto std_tid = std::this_thread::get_id();
+  auto [it, success] = profiler_thread_map_.try_emplace(std_tid, syscall_tid);
+  if (success) {
+    LOG(INFO) << "Adding thread id mapping: [" << std_tid << "->" << syscall_tid << "]";
+  }
+}
+
+std::shared_ptr<KernelRegistry> EtGlowExecutionProvider::GetKernelRegistry() const {
+  return info_.offline_mode ? s_kernel_registry_offline : s_kernel_registry;
+}
+
+EtGlowExecutionProvider::EtGlowExecutionProvider(EtGlowExecutionProviderInfo info, std::unique_ptr<etsoc::GlowAPI> glow, EtRt_StreamsRegistry& streams_reg)
+    : IExecutionProvider{onnxruntime::kEtGlowExecutionProvider, OrtDevice(OrtDevice::GPU, OrtDevice::MemType::DEFAULT, info.device_id)},
+      info_{std::move(info)},
+      glow_api_{std::move(glow)},
+      streams_reg_{streams_reg},
+      op_mgr_{info_.onnx_symbols} {
+  InitProviderOrtApi();
+
+  metadef_id_generator_ = ModelMetadefIdGenerator::Create();
+
+  auto options = EtGlowExecutionProviderInfo::ToProviderOptions(info_);
+  std::string parameters;
+  for (const auto& [key, value] : options) {
+    if (value.empty()) {
+      // do not print options that are empty
+      continue;
+    }
+    parameters += "\n - ";
+    parameters += key;
+    parameters += " = ";
+    parameters += value;
+  }
+  LOGS_DEFAULT(INFO) << "[ETGLOW] EP created with:" << parameters;
+
+  InitEtGlowStateFromEnvVars();
+  addToProfilerThreadMap();
+}
+
+EtGlowExecutionProvider::~EtGlowExecutionProvider() {
+  // wait for any uncompleted work
+  ORT_IGNORE_RETURN_VALUE(GetPerThreadContext().SynchronizeInferences());
+
+  // clean up thread local context caches
+  {
+    std::scoped_lock lock(context_state_.mutex);
+    for (const auto& cache_weak : context_state_.caches_to_update_on_destruction) {
+      const auto cache = cache_weak.lock();
+      if (!cache) continue;
+      ORT_IGNORE_RETURN_VALUE(cache->erase(this));
+    }
+  }
+
+  // unload all models from the device
+  if (auto status = UnloadModels(); status != Status::OK()) {
+    LOGS_DEFAULT(ERROR) << "[ETGLOW] Could not unload models: " << status.ErrorMessage();
+  }
+
+  auto end = std::chrono::steady_clock::now();
+  auto elapsed = std::chrono::duration_cast<std::chrono::milliseconds>(end - start_);
+  LOGS_DEFAULT(VERBOSE) << "[ETGLOW] lifetime: " << elapsed.count() << " ms";
+}
+
+void EtGlowExecutionProvider::InitEtGlowStateFromEnvVars() {
+  auto register_envvar_bool = [](const std::string& envvar_name, bool& knob) {
+    if (const auto envvar_value = GetEnvironmentVar(envvar_name); !envvar_value.empty()) {
+      const bool new_value = std::stoi(envvar_value) != 0;
+      if (knob != new_value) {
+        LOGS_DEFAULT(WARNING) << "[ETGLOW] Detected envvar " << envvar_name << " = " << new_value;
+        knob = new_value;
+      }
+    }
+  };
+  register_envvar_bool(etglow::envvars::kGreedyEp, info_.greedy);
+  register_envvar_bool(etglow::envvars::kOfflineModeEp, info_.offline_mode);
+  register_envvar_bool(etglow::envvars::kFailIfUnsupportedNode, info_.fail_if_cannot_run_whole_graph);
+  register_envvar_bool(etglow::envvars::kDebugLogs, debug_logs_);
+  register_envvar_bool(etglow::envvars::kBundleCacheEnable, use_bundles_);
+
+  auto register_envvar_action = [](const std::string& envvar_name, const auto& action_if_found) {
+    if (const auto envvar_value = GetEnvironmentVar(envvar_name); !envvar_value.empty()) {
+      action_if_found(envvar_name, envvar_value);
+    }
+  };
+  register_envvar_action(etglow::envvars::kCapabilityMode, [this](const std::string& envvar_name, const std::string& envvar_value) {
+    LOGS_DEFAULT(WARNING) << "[ETGLOW] Detected envvar " << envvar_name << " = " << envvar_value;
+    CapabilityMode new_mode = std::from_string(envvar_value);
+    op_mgr_.UpdateCapabilityMode(new_mode);
+  });
+}
+
+std::unique_ptr<IDataTransfer> EtGlowExecutionProvider::GetDataTransfer() const {
+  return std::make_unique<onnxruntime::EtGlowDataTransfer>(glow_api_->getBackendRuntimeInstance(), streams_reg_);
+}
+
+std::unique_ptr<profiling::EpProfiler> EtGlowExecutionProvider::GetProfiler() {
+  return std::make_unique<profiling::EtGlowProfiler>(*glow_api_, info_.traces_dir_prefix, info_.trace_options, profiler_thread_map_);
+};
+
+AllocatorPtr EtGlowExecutionProvider::CreateEtGlowAllocator(OrtDevice::DeviceId device_id, size_t mem_limit, ArenaExtendStrategy arena_extend_strategy, rt::IRuntime* rt) {
+  // lower arena memory limit to max available memory in the device
+  mem_limit = std::min(mem_limit, rt->getDeviceProperties(rt::DeviceId(device_id)).memorySize_);
+  AllocatorCreationInfo device_memory_info(
+      [rt](OrtDevice::DeviceId id) {
+        return std::make_unique<EtGlowDeviceAllocator>(id, ET, rt);
+      },
+      device_id,
+      true /* use_arena */,
+      OrtArenaCfg{mem_limit, static_cast<int>(arena_extend_strategy), -1, -1, -1, -1L},
+      true /* stream aware */,
+      false /* enable cross stream sharing */
+  );
+  return CreateAllocator(device_memory_info);
+}
+
+std::vector<AllocatorPtr> EtGlowExecutionProvider::CreatePreferredAllocators() {
+  AllocatorCreationInfo host_memory_info{[](int) { return std::make_unique<CPUAllocator>(); }};
+  return std::vector<AllocatorPtr>{CreateEtGlowAllocator(info_.device_id, info_.device_mem_limit, info_.arena_extend_strategy, glow_api_->getBackendRuntimeInstance()),
+                                   CreateAllocator(host_memory_info)};
+}
+
+std::vector<std::unique_ptr<ComputeCapability>>
+EtGlowExecutionProvider::GetCapability(const GraphViewer& graph_viewer, const IKernelLookup& /*kernel_lookup*/) const {
+  std::vector<std::unique_ptr<ComputeCapability>> result;
+
+  if (info_.greedy) {
+    LOGS_DEFAULT(WARNING) << "[ETGLOW EP] in greedy mode!";
+  }
+  if (info_.fail_if_cannot_run_whole_graph) {
+    LOGS_DEFAULT(WARNING) << "[ETGLOW EP] fail if cannot run whole graph is enabled!";
+  }
+
+  if (graph_viewer.IsSubgraph()) {
+    LOGS_DEFAULT(WARNING) << "[ETGLOW EP] refusing to return capability to compile subgraph.";
+    return {};  // Do not accept sub-graphs for now
+  }
+  if (info_.offline_mode) {
+    LOGS_DEFAULT(WARNING) << "[ETGLOW EP] in offline mode!";
+    // In offline mode everything is supported (no need to compile).
+    for (const auto& node_index : graph_viewer.GetNodesInTopologicalOrder()) {
+      std::unique_ptr<IndexedSubGraph> sub_graph = IndexedSubGraph::Create();
+      sub_graph->Nodes().push_back(node_index);
+      result.push_back(ComputeCapability::Create(std::move(sub_graph)));
+    }
+    return result;
+  }
+
+  int subgraph_index = 0;
+  int number_of_glow_nodes = 0;
+  const auto supported_node_groups = GetSupportedNodes(graph_viewer);
+  for (const auto& group : supported_node_groups) {
+    if (group.empty()) {
+      continue;
+    }
+
+    auto sub_graph = GetSubGraph(group, graph_viewer, subgraph_index);
+    if (sub_graph != nullptr) {
+      number_of_glow_nodes += sub_graph->Nodes().size();
+      result.push_back(ComputeCapability::Create(std::move(sub_graph)));
+      subgraph_index++;
+    }
+  }
+
+  const int number_of_ort_nodes = graph_viewer.NumberOfNodes();
+  whole_graph_runs_with_glow_ = (number_of_glow_nodes == number_of_ort_nodes) || info_.greedy;
+  const std::string greedy_msg = "but in greedy mode all nodes are accepted";
+  if (number_of_glow_nodes == 0) {
+    std::string message = (info_.greedy) ? "[ETGLOW EP] No graph should run on ETGLOW EP, " + greedy_msg
+                                         : "[ETGLOW EP] No graph will run on ETGLOW EP";
+    LOGS_DEFAULT(WARNING) << message;
+  } else if (number_of_glow_nodes == number_of_ort_nodes) {
+    LOGS_DEFAULT(INFO) << "[ETGLOW EP] Whole graph will run on GLOW execution provider";
+  } else {
+    std::string message = "[ETGLOW EP] Detected " + std::to_string(number_of_glow_nodes) + " nodes supported (graph has " + std::to_string(number_of_ort_nodes) + " nodes) ";
+    message += (info_.greedy) ? greedy_msg
+                              : "graph will be partitioned between ETGLOW EP and other EPs";
+    LOGS_DEFAULT(WARNING) << message;
+  }
+
+  if (info_.dump_subgraphs) {
+    HashValue model_hash = 0;
+    const int metadef_id = metadef_id_generator_->GenerateId(graph_viewer, model_hash);
+    std::string subgraph_name = "ETGLOW_" + std::to_string(model_hash) + "_" + std::to_string(metadef_id) + ".onnx";
+    DoDumpSubgraph(graph_viewer, subgraph_name, false, true);
+  }
+
+  return result;
+}
+
+void EtGlowExecutionProvider::RegisterStreamHandlers(IStreamCommandHandleRegistry& stream_handle_registry, AllocatorMap& allocators) const {
+  auto cpu_allocator = allocators[GetOrtDeviceByMemType(OrtMemTypeCPU)];
+  RegisterEtRuntimeStreamHandles(stream_handle_registry, OrtDevice::GPU, cpu_allocator, streams_reg_);
+}
+
+Status EtGlowExecutionProvider::Compile(const std::vector<FusedNodeAndGraph>& fused_nodes_and_graphs,
+                                        std::vector<NodeComputeInfo>& node_compute_funcs) {
+  for (const auto& fuse_node_graph : fused_nodes_and_graphs) {
+    const GraphViewer& graph_body_viewer = fuse_node_graph.filtered_graph;
+    const Node& fused_node = fuse_node_graph.fused_node;
+
+    if (info_.dump_subgraphs) {
+      std::string subgraph_name = fused_node.Name() + ".onnx";
+      DoDumpSubgraph(graph_body_viewer, subgraph_name);
+    }
+
+    if (debug_logs_) {
+      PrintGraphInputs(graph_body_viewer);
+    }
+
+    OrderedShapeValues input_output_dim_params;
+    ORT_RETURN_IF_ERROR(GetShapeValues(graph_body_viewer, input_output_dim_params));
+
+    EtGlowInferenceInputNames onnx_input_names;
+    ORT_RETURN_IF_ERROR(GetInputNames(graph_body_viewer, fused_node, onnx_input_names));
+
+    EtGlowInferenceOutputNames onnx_output_names;
+    std::unordered_map<std::string, std::vector<int64_t>> ordered_output_dimensions;
+    ORT_RETURN_IF_ERROR(GetOutputNamesAndShapes(fused_node, onnx_output_names, ordered_output_dimensions, input_output_dim_params));
+
+    // construct bundle path
+    const std::size_t bundle_abi = std::hash<BundleABI>{}(BundleABI::CreateWith(graph_body_viewer, info_.api_params, input_output_dim_params, info_.trace_options));
+    auto bundle_path = GetBundlePath(info_.bundle_cache_prefix, bundle_abi, fused_node.Name());
+    LOGS_DEFAULT(INFO) << "[ETGLOW EP] Calculated bundle name: " << bundle_path;
+    if (auto bundle_dir = bundle_path.parent_path(); std::filesystem::create_directories(bundle_dir)) {
+      LOGS_DEFAULT(INFO) << "[ETGLOW EP] Storing bundles in: " << bundle_dir;
+    }
+
+    if (!compiled_models_reg_.HasCompiledModel(bundle_path)) {
+      // EP has not loaded the model yet. Get CompileModel either from the bundle or compile it from scratch
+      std::unique_ptr<GlowAPI::CompiledModel> compilation_result;
+      ORT_RETURN_IF_ERROR(GetCompiledModel(graph_body_viewer, fused_node.Name(), bundle_path, input_output_dim_params, compilation_result));
+
+      compiled_models_reg_.StoreCompiledModel(bundle_path, std::move(compilation_result));
+    }
+
+    if (ShouldExportBundle()) {
+      ExportBundle(bundle_path, info_.export_bundle_path);
+    }
+
+    NodeComputeInfo compute_info;
+    const bool should_execute = !info_.compile_only;
+    compute_info.create_state_func = [this, bundle_path, onnx_input_names, onnx_output_names, ordered_output_dimensions](ComputeContext* context, FunctionState* state) {
+      auto p = std::make_unique<EtGlowFuncState>();
+      *p = {
+          context->allocate_func,
+          context->release_func,
+          context->allocator_handle,
+          context->node_name,
+          glow_api_.get(),
+          compiled_models_reg_.GetCompiledModel(bundle_path),
+          bundle_path,
+          onnx_input_names,
+          onnx_output_names,
+          ordered_output_dimensions,
+      };
+      *state = p.release();
+      return 0;
+    };
+    compute_info.release_state_func = [](gsl::owner<FunctionState> state) {
+      if (state != nullptr) {
+        delete static_cast<EtGlowFuncState*>(state);
+      }
+    };
+    compute_info.compute_func = [should_execute, this](FunctionState state, const OrtApi* /*api*/, OrtKernelContext* context) {
+      Ort::KernelContext ctx(context);
+
+      ORT_TRY {
+        if (!should_execute) {
+          return Status::OK();
+        }
+        auto* etfn_state = static_cast<EtGlowFuncState*>(state);
+        ORT_RETURN_IF(etfn_state->etglow_api == nullptr, "[ETGLOW] Error: glow_api cannot be nullptr");
+
+        InferenceInputDescriptors inputs;
+        ORT_RETURN_IF_ERROR(GetInputTensorDescriptors(ctx, etfn_state->ordered_input_names, inputs));
+
+        ORT_RETURN_IF(etfn_state->ordered_output_dimensions.size() != ctx.GetOutputCount(), "Output size mismatch.");
+        InferenceOutputDescriptors outputs;
+        ORT_RETURN_IF_ERROR(GetOutputTensorDescriptors(ctx, etfn_state->ordered_output_names, etfn_state->ordered_output_dimensions, outputs));
+
+        const GlowAPI::CompiledModel* glow_compiled_model_ptr = etfn_state->compiled_model;
+        ORT_RETURN_IF(glow_compiled_model_ptr == nullptr, "[ETGLOW] Error: expected non-nullptr GlowAPI::CompiledModel* but got a nullptr");
+
+        GlowAPI::RegisteredModel* glow_registered_model_ptr = nullptr;
+        ORT_RETURN_IF_ERROR(loaded_models_reg_.LoadModel(*etfn_state->etglow_api, etfn_state->bundle_name, *glow_compiled_model_ptr, loaded_models_, glow_registered_model_ptr));
+        ORT_RETURN_IF(glow_registered_model_ptr == nullptr, "[ETGLOW] Error: expected non-nullptr GlowAPI::RegisteredModel* but got a nullptr");
+
+        auto device_placeholders = GlowAPI::getOnDevicePlaceholders(*glow_compiled_model_ptr);
+
+        void* stream_type_erased_ptr = ctx.GetGPUComputeStream();
+        const auto* stream_id_ptr = static_cast<rt::StreamId*>(stream_type_erased_ptr);
+        auto stream_id = *stream_id_ptr;
+
+        ORT_RETURN_IF_ERROR(DoInference(*etfn_state->etglow_api, *glow_registered_model_ptr, inputs, std::move(outputs), device_placeholders, stream_id));
+
+        addToProfilerThreadMap();
+      }
+      ORT_CATCH(const std::exception& e) {
+        return ORT_MAKE_STATUS(ONNXRUNTIME, FAIL, e.what());
+      }
+
+      return Status::OK();
+    };
+    node_compute_funcs.push_back(compute_info);
+  }
+
+  return Status::OK();
+}
+
+common::Status EtGlowExecutionProvider::Sync() const {
+  return streams_reg_.Synchronize(rt::DeviceId(info_.device_id));
+}
+
+Status EtGlowExecutionProvider::OnRunStart(const onnxruntime::RunOptions& /*run_options*/) {
+  LOGS_DEFAULT(INFO) << "[ETGLOW EP] OnRunStart()";
+  return Status::OK();
+}
+
+Status EtGlowExecutionProvider::OnRunEnd(bool sync_stream, const onnxruntime::RunOptions& /*run_options*/) {
+  LOGS_DEFAULT(INFO) << "[ETGLOW EP] OnRunEnd()";
+  if (sync_stream) {
+    // only synchronize if we have recorded stream to synchronize to
+    if (auto stream = GetPerThreadContext().GetComputeStream(); stream.has_value()) {
+      ORT_RETURN_IF_ERROR(streams_reg_.Synchronize(stream.value()));
+    }
+  }
+  LOGS_DEFAULT(INFO) << "[ETGLOW EP] OnRunEnd() done.";
+
+  return Status::OK();
+}
+
+std::vector<std::vector<NodeIndex>> EtGlowExecutionProvider::GetSupportedNodes(const GraphViewer& graph_viewer) const {
+  std::vector<std::vector<size_t>> supported_node_vecs;
+  std::vector<size_t> supported_node_vec;
+
+  std::unordered_map<std::string, int> all_nodes_count;
+  std::unordered_map<std::string, int> supported_nodes_count;
+
+  const auto& node_indices = graph_viewer.GetNodesInTopologicalOrder();
+  for (auto node_idx : node_indices) {
+    const auto* node(graph_viewer.GetNode(node_idx));
+    const auto result = op_mgr_.IsNodeSupported(node, graph_viewer);
+    const auto supported = result.supported();
+    if (!supported && !info_.greedy) {
+      LOGS_DEFAULT(WARNING) << "[ETGLOW EP] node not supported: " << node->OpType() << " reason: " << result.reason();
+    }
+
+    if (debug_logs_) {
+      RegisterOperatorCoverage(*node, supported, all_nodes_count, supported_nodes_count);
+    }
+
+    if (supported || info_.greedy) {
+      supported_node_vec.push_back(node_idx);
+    } else {
+      if (!supported_node_vec.empty()) {
+        supported_node_vecs.push_back(std::move(supported_node_vec));
+        supported_node_vec = {};
+      }
+    }
+  }
+
+  if (!supported_node_vec.empty()) {
+    supported_node_vecs.push_back(supported_node_vec);
+  }
+
+  if (debug_logs_) {
+    PrintOperatorCoverage(all_nodes_count, supported_nodes_count);
+  }
+
+  return supported_node_vecs;
+}
+
+void EtGlowExecutionProvider::RegisterOperatorCoverage(const Node& node, const bool supported, std::unordered_map<std::string, int>& all_nodes_count, std::unordered_map<std::string, int>& supported_nodes_count) {
+  auto node_optype_ver = node.OpType() + "_" + std::to_string(node.SinceVersion());
+  all_nodes_count[node_optype_ver]++;
+  if (supported) {
+    supported_nodes_count[node_optype_ver]++;
+  }
+}
+
+void EtGlowExecutionProvider::PrintOperatorCoverage(const std::unordered_map<std::string, int>& all_nodes_count,
+                                                    std::unordered_map<std::string, int>& supported_nodes_count) {
+  int all_counts = 0;
+  int support_counts = 0;
+  for (auto [optype_ver, all_count] : all_nodes_count) {
+    const auto& support_count = supported_nodes_count[optype_ver];
+    all_counts += all_count;
+    support_counts += support_count;
+    LOGS_DEFAULT(INFO) << "Operator type: [" << optype_ver << "] coverage: " << support_count << ":" << all_count
+                       << " percentage: " << static_cast<float>(support_count) / static_cast<float>(all_count);
+  }
+  LOGS_DEFAULT(INFO) << "Total coverage: " << support_counts << ":" << all_counts
+                     << " percentage: " << static_cast<float>(support_counts) / static_cast<float>(all_counts);
+}
+
+std::unique_ptr<IndexedSubGraph> EtGlowExecutionProvider::GetSubGraph(const std::vector<std::size_t>& group,
+                                                                      const GraphViewer& graph_viewer,
+                                                                      int subgraph_index) const {
+  const auto& graph_output_list = graph_viewer.GetOutputs();
+  const std::unordered_set<const NodeArg*> graph_outputs(graph_output_list.cbegin(), graph_output_list.cend());
+
+  std::unordered_set<NodeIndex> node_set;
+  node_set.reserve(group.size());
+  for (const auto& index : group) {
+    node_set.insert(index);
+  }
+
+  // Get parent graph output names
+  std::unordered_set<std::string> graph_output_names;
+  for (const auto* output_arg : graph_viewer.GetOutputs()) {
+    graph_output_names.insert(output_arg->Name());
+  }
+
+  // Find inputs and outputs of the subgraph
+  auto sub_graph = onnxruntime::IndexedSubGraph::Create();
+  std::unordered_set<const NodeArg*> node_outputs;
+  std::unordered_set<const NodeArg*> subgraph_inputs;
+  std::unordered_set<const NodeArg*> subgraph_outputs;
+  std::vector<const NodeArg*> ordered_subgraph_inputs;
+  std::vector<const NodeArg*> ordered_subgraph_outputs;
+
+  for (const auto& index : group) {
+    sub_graph->Nodes().push_back(index);
+    const auto& node = graph_viewer.GetNode(index);
+
+    for (const auto& input : node->InputDefs()) {
+      // if the node input was not produced by this subgraph, add it to the subgraph inputs.
+      if (!input->Exists()) {
+        continue;
+      }
+      if ((node_outputs.count(input) == 0) && (subgraph_inputs.count(input) == 0)) {
+        subgraph_inputs.insert(input);
+        ordered_subgraph_inputs.push_back(input);
+      }
+    }
+
+    const auto& output_defs = node->OutputDefs();
+    for (const auto* output_def : output_defs) {
+      if (!output_def->Exists()) {
+        continue;
+      }
+      node_outputs.insert(output_def);
+      // if output is overall graph output we need to produce it.
+      if (graph_outputs.count(output_def) != 0) {
+        ordered_subgraph_outputs.push_back(output_def);
+      }
+    }
+
+    // if output connects to a node not in this subgraph we need to produce it
+    for (auto it = node->OutputEdgesBegin(), end = node->OutputEdgesEnd(); it != end; ++it) {
+      if (node_set.count(it->GetNode().Index()) != 0) {
+        continue;
+      }
+      const auto* output_def = output_defs[it->GetSrcArgIndex()];
+      if (subgraph_outputs.count(output_def) == 0 && graph_outputs.count(output_def) == 0) {
+        subgraph_outputs.insert(output_def);
+        ordered_subgraph_outputs.push_back(output_def);
+      }
+    }
+  }
+
+  auto meta_def = IndexedSubGraph_MetaDef::Create();
+  meta_def->name() = GenerateMetaDefName(graph_viewer, *metadef_id_generator_, std::nullopt, subgraph_index);
+  meta_def->domain() = kMSDomain;
+  meta_def->since_version() = 1;
+  meta_def->status() = ONNX_NAMESPACE::EXPERIMENTAL;
+  LOGS_DEFAULT(INFO) << "[ETGLOW EP] GLOW subgraph MetaDef name " + meta_def->name();
+
+  auto HasShape = [](const auto* tensor, const std::string& subgraph_name) {
+    if (tensor->Shape() == nullptr) {
+      // if any subgraph input/output shape information is missing, do not accept the subgraph
+      LOGS_DEFAULT(WARNING) << "[ETGLOW EP] subgraph " + subgraph_name + " tensor: '" + tensor->Name() +
+                                   "' has missing shapes!"
+                                   " ETGLOW EP requires input/outputs to have known shapes. Subgraph not supported.";
+      return false;
+    }
+    return true;
+  };
+
+  // Assign inputs and outputs to subgraph's meta_def
+  for (const auto& input : ordered_subgraph_inputs) {
+    meta_def->inputs().push_back(input->Name());
+    if (!HasShape(input, meta_def->name()) && !info_.greedy) {
+      return nullptr;
+    }
+  }
+
+  for (const auto& output : ordered_subgraph_outputs) {
+    meta_def->outputs().push_back(output->Name());
+    if (!HasShape(output, meta_def->name()) && !info_.greedy) {
+      return nullptr;
+    }
+  }
+
+  sub_graph->SetMetaDef(std::move(meta_def));
+
+  return sub_graph;
+}
+
+void EtGlowExecutionProvider::PrintGraphInputs(const GraphViewer& graph) {
+  for (const auto& input : graph.GetInputs()) {
+    const auto* shape = input->Shape();
+    if (shape == nullptr) {
+      continue;
+    }
+
+    if (const auto* type = input->Type(); type != nullptr) {
+      LOGS_DEFAULT(INFO) << "input name: " << input->Name()
+                         << " length: " << std::to_string(type->length())
+                         << " type: " << *type
+                         << " shape (dim_size): " << std::to_string(shape->dim_size());
+    }
+    int dimIdx = 0;
+    for (const auto& dim : shape->dim()) {
+      LOGS_DEFAULT(INFO)
+          << " -- dimIdx: " << dimIdx
+          << " has_dim_param: " << dim.has_dim_param()
+          << " dim_param: " << dim.dim_param()
+          << " has_dim_value: " << dim.has_dim_value()
+          << " dim_value: " << dim.dim_value()
+          << " value_case: " << dim.value_case();
+      dimIdx++;
+    }
+  }
+}
+
+void EtGlowExecutionProvider::DoDumpSubgraph(const GraphViewer& graph_viewer, const std::string& subgraph_name, bool include_initializers, bool include_outer_scope_args) const {
+  // Dump EtGlow subgraphs
+  auto model = graph_viewer.CreateModel(*GetLogger());
+  auto model_proto = model->ToProto();
+  graph_viewer.ToProto(*model_proto->mutable_graph(), include_initializers, include_outer_scope_args);
+  model_proto->set_ir_version(ONNX_NAMESPACE::Version::IR_VERSION);
+  std::fstream dump(subgraph_name, std::ios::out | std::ios::trunc | std::ios::binary);
+  model_proto->SerializeToOstream(dump);
+}
+
+Status EtGlowExecutionProvider::GetShapeValues(const GraphViewer& graph_viewer, OrderedShapeValues& dim_params) const {
+  auto fill_dim_params = [this, &dim_params](const std::vector<const NodeArg*>& nodes) {
+    for (const auto& tensor : nodes) {
+      const auto* shape = tensor->Shape();
+      if (shape == nullptr) {
+        return ORT_MAKE_STATUS(ONNXRUNTIME, FAIL, "[ETGLOW EP] doesn't support models whose input/output Shape(s) are not known at Compile() time. Missing shape for tensor " + tensor->Name());
+      }
+      for (const auto& dim : shape->dim()) {
+        if (!dim.has_dim_param()) {
+          continue;
+        }
+
+        const auto& dim_param = dim.dim_param();
+        auto it = info_.onnx_symbols.find(dim_param);
+        if (it != info_.onnx_symbols.end()) {
+          dim_params.try_emplace(dim_param, std::get<std::size_t>(it->second));
+        } else {
+          LOGS_DEFAULT(ERROR) << "dim_param: " << dim_param << " missing in onnx_symbols. Please provide a value!";
+          return ORT_MAKE_STATUS(ONNXRUNTIME, FAIL, "[ETGLOW EP] cannot fill GlowAPI LoadOptions symbolTable.");
+        }
+      }
+    }
+
+    return Status::OK();
+  };
+
+  auto add_missing_user_params = [this, &dim_params]() {
+    for (const auto& [key, value] : info_.onnx_symbols) {
+      // Copy missing parameters to output dictionary. Must add all of them in case they are not used in the ONNX file
+      // but the user still wishes to include that information in the compiled model.
+      auto [it, result] = dim_params.try_emplace(key, std::get<std::size_t>(value));
+      if (result) {
+        LOGS_DEFAULT(VERBOSE) << "dim_param: " << key << " not used in ONNX file, registering it anyway.";
+      }
+    }
+
+    return Status::OK();
+  };
+
+  ORT_RETURN_IF_ERROR(fill_dim_params(graph_viewer.GetInputs()));
+  ORT_RETURN_IF_ERROR(fill_dim_params(graph_viewer.GetOutputs()));
+  ORT_RETURN_IF_ERROR(add_missing_user_params());
+  LOGS_DEFAULT(INFO) << "dim_params size:" << dim_params.size();
+  return Status::OK();
+}
+
+Status EtGlowExecutionProvider::GetInputNames(const GraphViewer& graph_viewer, const Node& fused_node,
+                                              EtGlowInferenceInputNames& input_names) {
+  const auto& input_defs = fused_node.InputDefs();
+  input_names.reserve(input_defs.size());
+  for (size_t i = 0; i < input_defs.size(); ++i) {
+    const auto* input_def = input_defs[i];
+    const auto& input_name = input_def->Name();
+    if (graph_viewer.IsConstantInitializer(input_name, false)) {
+      LOGS_DEFAULT(VERBOSE) << "Excluding constant initializer: " << input_name;
+      continue;
+    }
+    LOGS_DEFAULT(VERBOSE) << "Keeping input: " << input_name;
+    input_names.try_emplace(input_name, i);
+  }
+  return Status::OK();
+}
+
+Status EtGlowExecutionProvider::GetOutputNamesAndShapes(const Node& fused_node, EtGlowInferenceOutputNames& output_names, std::unordered_map<std::string, std::vector<int64_t>>& ordered_output_dimensions, const OrderedShapeValues& output_dim_params) {
+  const auto& output_defs = fused_node.OutputDefs();
+  output_names.reserve(output_defs.size());
+  for (const auto* output_ptr : output_defs) {
+    const auto& output_ref = *output_ptr;
+    output_names.emplace_back(output_ref.Name());
+    std::vector<int64_t> dimensions;
+    const auto* output_shape = output_ref.Shape();
+    if (output_shape == nullptr) {
+      return ORT_MAKE_STATUS(ONNXRUNTIME, FAIL, "[ETGLOW EP] doesn't support models whose output Shape(s) are not known at Compile() time.");
+    }
+    dimensions.reserve(output_shape->dim_size());
+    for (int j = 0; j < output_shape->dim_size(); j++) {
+      const auto& dim = output_shape->dim(j);
+      if (dim.has_dim_value()) {
+        dimensions.emplace_back(dim.dim_value());
+      } else if (dim.has_dim_param()) {
+        if (auto it = output_dim_params.find(dim.dim_param()); auto dim_value = it->second) {
+          dimensions.emplace_back(dim_value);
+        } else {
+          return ORT_MAKE_STATUS(ONNXRUNTIME, FAIL, "[ETGLOW EP] Didn't find " + dim.dim_param() + " in output_dim_params");
+        }
+      }
+    }
+    ordered_output_dimensions.try_emplace(output_ref.Name(), std::move(dimensions));
+  }
+  return Status::OK();
+}
+
+Status EtGlowExecutionProvider::GetCompiledModel(const GraphViewer& graph_viewer, const std::string& fused_node_name, const fs::path& bundle_path, const OrderedShapeValues& input_dim_params, std::unique_ptr<GlowAPI::CompiledModel>& compilation_result) {
+  if (ShouldReuseCompiledModel(bundle_path)) {
+    ORT_RETURN_IF_ERROR(DoReuseCompiledModelFromBundle(bundle_path, compilation_result));
+  } else {
+    ORT_RETURN_IF_ERROR(DoCompile(bundle_path, graph_viewer, fused_node_name, input_dim_params, compilation_result));
+  }
+  return Status::OK();
+}
+
+bool EtGlowExecutionProvider::ShouldReuseCompiledModel(const fs::path& bundle_path) const {
+  return use_bundles_ && std::filesystem::exists(bundle_path);
+}
+
+Status EtGlowExecutionProvider::DoReuseCompiledModelFromBundle(const fs::path& bundle_path, std::unique_ptr<GlowAPI::CompiledModel>& compilation_result) {
+  // restore bundle
+  LOGS_DEFAULT(INFO) << "[ETGLOW EP] [Hit] Loading bundle: " << bundle_path;
+
+  auto eload_compiled_model = GlowAPI::loadCompiledModel(bundle_path);
+  if (!eload_compiled_model) {
+    return ORT_MAKE_STATUS(ONNXRUNTIME, FAIL, glowApiErrMsg(eload_compiled_model.takeError(), "loading CompiledModel from bundle."));
+  }
+  auto glow_compiled_model = std::move(*eload_compiled_model);
+
+  compilation_result = std::make_unique<GlowAPI::CompiledModel>(std::move(glow_compiled_model));
+
+  return Status::OK();
+}
+
+Status EtGlowExecutionProvider::SetDevicePlaceholders(GlowAPI::Model& glow_loaded_model) const {
+  const bool bundles_for_external_use = info_.compile_only && ShouldExportBundle();
+  std::vector<std::string> device_placeholder_names = bundles_for_external_use ? GetOnDevicePlaceholderNamesFromUserParams(info_.api_params) : GetOnDevicePlaceholderNamesFromGlowAPI(glow_loaded_model);
+  std::vector<std::string> device_implicit_placeholder_names = GetOnDeviceImplicitPlaceholdrNamesFromUserParams(info_.api_params);
+  // remove from device_placeholder_names those arrays that the user already provided as implicit-placeholders
+  device_placeholder_names.erase(
+      std::remove_if(
+          device_placeholder_names.begin(),
+          device_placeholder_names.end(),
+          [&device_implicit_placeholder_names](const std::string& element) {
+            bool should_remove = std::find(device_implicit_placeholder_names.begin(), device_implicit_placeholder_names.end(), element) != device_implicit_placeholder_names.end();
+            if (should_remove) {
+              LOGS_DEFAULT(WARNING) << "Found placeholder that the user wants as implicit-device-placeholder ( " << element << "). Removing it from placeholder's list.";
+            }
+            return should_remove;
+          }),
+      device_placeholder_names.end());
+
+  for (const auto& tensor_name : device_placeholder_names) {
+    if (auto error_set_placeholders = GlowAPI::setOnDevicePlaceholder(glow_loaded_model, tensor_name); error_set_placeholders) {
+      return ORT_MAKE_STATUS(ONNXRUNTIME, FAIL, glowApiErrMsg(std::move(error_set_placeholders), "setting OnDevicePlaceholders with tensor_name:" + tensor_name));
+    }
+  }
+
+  for (const auto& tensor_name : device_implicit_placeholder_names) {
+    if (auto error_set_implicit_pla = GlowAPI::setImplicitOnDevicePlaceholder(glow_loaded_model, tensor_name); error_set_implicit_pla) {
+      return ORT_MAKE_STATUS(ONNXRUNTIME, FAIL, glowApiErrMsg(std::move(error_set_implicit_pla), "setting ImplicitOnDevicePlaceholders with tensor_name: " + tensor_name));
+    }
+  }
+  return Status::OK();
+}
+
+Status EtGlowExecutionProvider::DoCompile(const fs::path& bundle_path, const GraphViewer& graph_viewer, const std::string& fused_node_name, const OrderedShapeValues& input_dim_params, std::unique_ptr<GlowAPI::CompiledModel>& compilation_result) {
+  LOGS_DEFAULT(INFO) << "[ETGLOW EP] [Miss] Could not find bundle: " << bundle_path << " .. compiling.";
+
+  // Reconstruct graph proto from fused node's function body
+  auto model = graph_viewer.CreateModel(*GetLogger());
+  auto model_proto = model->ToProto();
+  graph_viewer.ToProto(*model_proto->mutable_graph(), true, true);
+  model_proto->set_ir_version(ONNX_NAMESPACE::Version::IR_VERSION);
+
+  const auto onnx_model_path = GetModelPath(graph_viewer, fused_node_name);
+  std::ofstream ofs(onnx_model_path);
+  model_proto->SerializeToOstream(ofs);
+  ofs.close();
+
+  GlowAPI::LoadOptions lo = GetGlowLoadOptions(info_.api_params, input_dim_params, graph_viewer, *metadef_id_generator_);
+
+  LOGS_DEFAULT(INFO) << "[ETGLOW EP] Loading the model: " << onnx_model_path;
+
+  auto expected_glow_loaded_model = GlowAPI::loadModelONNX(onnx_model_path, GlowAPI::InputsMap{}, lo);
+  if (!expected_glow_loaded_model) {
+    return ORT_MAKE_STATUS(ONNXRUNTIME, FAIL, glowApiErrMsg(expected_glow_loaded_model.takeError(), "loading onnx model"));
+  }
+  auto glow_loaded_model = std::move(*expected_glow_loaded_model);
+
+  ORT_RETURN_IF_ERROR(SetDevicePlaceholders(glow_loaded_model));
+
+  LOGS_DEFAULT(INFO) << "[ETGLOW EP] compiling the model.";
+  GlowAPI::CompilationOptions co;
+  if (lo.useFP16) {
+    // convert all FP32 internal nodes to FP16
+    co.ctxt.precisionConfig.convertToFP16 = true;
+    // convert constants from FP32 to FP16
+    co.ctxt.precisionConfig.convertConstantsToFP16 = true;
+    // add conversion nodes from FP32 to FP16 to model inputs/outputs
+    co.ctxt.precisionConfig.convertPlaceholdersToFP16 = true;
+  }
+  auto expected_glow_compiled_model = glow_api_->compileModel(glow_loaded_model, co);
+  if (!expected_glow_compiled_model) {
+    return ORT_MAKE_STATUS(ONNXRUNTIME, FAIL, glowApiErrMsg(expected_glow_compiled_model.takeError(), "compiling onnx model"));
+  }
+  auto glow_compiled_model = std::move(*expected_glow_compiled_model);
+
+  if (save_bundles_) {
+    if (fs::exists(bundle_path)) {
+      LOGS_DEFAULT(WARNING) << "[ETGLOW EP] found existing bundle: " << bundle_path << " before saving compiled model! proceeding to remove.";
+      fs::remove(bundle_path);
+    }
+    LOGS_DEFAULT(INFO) << "[ETGLOW EP] saving compiled model.";
+    // this is optional, for maximum re-use of compiled model
+    if (auto error_saved_compiled_model = GlowAPI::saveCompiledModel(glow_compiled_model, bundle_path); error_saved_compiled_model) {
+      return ORT_MAKE_STATUS(ONNXRUNTIME, FAIL, glowApiErrMsg(std::move(error_saved_compiled_model), "saving CompiledModel into bundle."));
+    }
+  }
+
+  LOGS_DEFAULT(INFO) << "[ETGLOW EP] Compilation is done.";
+
+  compilation_result = std::make_unique<GlowAPI::CompiledModel>(std::move(glow_compiled_model));
+
+  return Status::OK();
+}
+
+bool EtGlowExecutionProvider::ShouldExportBundle() const {
+  return !info_.export_bundle_path.empty();
+}
+
+void EtGlowExecutionProvider::ExportBundle(const fs::path& from, const fs::path& to) {
+  fs::path dest = fs::absolute(to);
+  if (auto dest_dir = dest.parent_path(); !fs::exists(dest_dir)) {
+    fs::create_directories(dest_dir);
+  }
+  const auto copyOptions = fs::copy_options::overwrite_existing;
+  fs::copy_file(from, dest, copyOptions);
+}
+
+Status EtGlowExecutionProvider::GetInputTensorDescriptors(const Ort::KernelContext& ctx, const EtGlowInferenceInputNames& ordered_input_names, InferenceInputDescriptors& inputs) {
+  inputs.reserve(ordered_input_names.size());
+  for (const auto& [input_name, input_idx] : ordered_input_names) {
+    auto input_tensor = ctx.GetInput(input_idx);
+    auto tensor_info = input_tensor.GetTensorTypeAndShapeInfo();
+    auto shape = tensor_info.GetShape();
+    const void* input_buffer = input_tensor.GetTensorRawData();
+    inputs.try_emplace(
+        input_name,
+        ort_etglow::OnnxTensorInfo{tensor_info.GetElementType(), shape},
+        input_buffer);
+  }
+  return Status::OK();
+}
+
+Status EtGlowExecutionProvider::GetOutputTensorDescriptors(const Ort::KernelContext& ctx, const EtGlowInferenceOutputNames& ordered_output_names, const std::unordered_map<std::string, std::vector<int64_t>>& ordered_output_dimensions, InferenceOutputDescriptors& outputs) {
+  outputs.reserve(ordered_output_names.size());
+  for (size_t i = 0; i < ordered_output_names.size(); i++) {
+    const auto& output_name = ordered_output_names[i];
+    auto it = ordered_output_dimensions.find(output_name);
+    if (it == ordered_output_dimensions.end()) {
+      return ORT_MAKE_STATUS(ONNXRUNTIME, FAIL, "could not find output_dimensions for output: " + output_name);
+    }
+    auto output_shape = it->second;
+    auto output_tensor = ctx.GetOutput(i, output_shape.data(), output_shape.size());
+    auto tensor_info = output_tensor.GetTensorTypeAndShapeInfo();
+    auto shape = tensor_info.GetShape();
+    void* output_buffer = output_tensor.GetTensorMutableRawData();
+    outputs.try_emplace(
+        output_name,
+        ort_etglow::OnnxOutputTensorData{
+            ort_etglow::OnnxTensorInfo{tensor_info.GetElementType(), shape},
+            output_buffer,
+        });
+  }
+  return Status::OK();
+}
+
+Status EtGlowExecutionProvider::SetDevicePlaceholderAddresses(GlowAPI::ExecutionBindings& execution_bindings, const InferenceInputDescriptors& inputs, const InferenceOutputDescriptors& outputs, const std::vector<std::string>& device_placeholders) const {
+  auto set_device_placeholder_addr = [&execution_bindings, &device_placeholders](const auto& tensors, const std::string& direction) {
+    for (const auto& [tensor_name, tensor_descriptor] : tensors) {
+      LOGS_DEFAULT(VERBOSE) << "[ETGLOW EP] setting onDevicePlaceholder for (" + direction + ") tensor: " << tensor_name << " in address: " << tensor_descriptor.buffer;
+      if (auto it = std::find(device_placeholders.begin(), device_placeholders.end(), tensor_name); it == device_placeholders.end()) {
+        // skip registering not known to Glow
+        continue;
+      }
+      if (auto spaError = GlowAPI::setOnDevicePlaceholderAddress(execution_bindings, tensor_name, const_cast<void*>(tensor_descriptor.buffer)); spaError) {
+        return ORT_MAKE_STATUS(ONNXRUNTIME, FAIL, glowApiErrMsg(std::move(spaError), "setting onDevicePlaceholder address."));
+      }
+    }
+    return Status::OK();
+  };
+  ORT_RETURN_IF_ERROR(set_device_placeholder_addr(inputs, "in"));
+  ORT_RETURN_IF_ERROR(set_device_placeholder_addr(outputs, "out"));
+  return Status::OK();
+}
+
+Status EtGlowExecutionProvider::DoInference(GlowAPI& etglow_api, const GlowAPI::RegisteredModel& glow_reg_model, const InferenceInputDescriptors& inputs, InferenceOutputDescriptors&& outputs, const std::vector<std::string>& device_placeholders, rt::StreamId stream_id) const {
+  LOGS_DEFAULT(INFO) << "[ETGLOW EP] creating execution bindings.";
+
+  auto ebindings = GlowAPI::createExecutionBindings(glow_reg_model);
+  if (!ebindings) {
+    return ORT_MAKE_STATUS(ONNXRUNTIME, FAIL, glowApiErrMsg(ebindings.takeError(), "creating execution bindings."));
+  }
+  auto bindings = std::move(*ebindings);
+
+  LOGS_DEFAULT(INFO) << "[ETGLOW EP] telling glow to use stream_id: " << static_cast<int>(stream_id);
+  GlowAPI::ETSOCDeviceExecutionInfo info{static_cast<GlowAPI::StreamId>(stream_id)};
+  if (auto sdeiError = etglow_api.setDeviceExecutionInfo(bindings, info); sdeiError) {
+    return ORT_MAKE_STATUS(ONNXRUNTIME, FAIL, glowApiErrMsg(std::move(sdeiError), "setting device execution info."));
+  }
+
+  ORT_RETURN_IF_ERROR(SetDevicePlaceholderAddresses(bindings, inputs, outputs, device_placeholders));
+
+  LOGS_DEFAULT(INFO) << "[ETGLOW EP] launching inference on stream_id: " << static_cast<int>(stream_id);
+
+  auto m = std::make_shared<std::mutex>();
+  auto cv = std::make_shared<std::condition_variable>();
+  auto scheduled = std::make_shared<bool>(false);
+  auto completed = std::make_shared<bool>(false);
+
+  const auto inferenceScheduled = [m, cv, scheduled](GlowAPI::InferenceCallId id) {
+    std::unique_lock lock(*m);
+    LOGS_DEFAULT(INFO) << "[ETGLOW EP] launching inference: " << std::to_string(id) << " done! Waking up ort thread";
+    *scheduled = true;
+    //// manual unlocking is done before notifying, to avoid waking up
+    //// the waiting thread only to block again (see notify_one for details)
+    lock.unlock();
+    cv->notify_one();
+  };
+
+  auto inferenceResultPromise = std::make_unique<std::promise<InferenceResult>>();
+  auto* inferenceResultPromisePtr = inferenceResultPromise.get();
+  std::future<InferenceResult> inferenceResultFuture = inferenceResultPromise->get_future();
+  const auto inferenceCompleted = [inferenceResultPromisePtr, m, cv, completed](GlowAPI::InferenceCallId id, GlowAPI::Error error, GlowAPI::ExecutionBindings resultBindings) {
+    LOGS_DEFAULT(INFO) << "[ETGLOW EP] completed inference: " << std::to_string(id) << " done!";
+    inferenceResultPromisePtr->set_value(InferenceResult{.id = id, .error = std::move(error), .bindings = std::move(resultBindings)});
+
+    // if there was any problem and Glow didn't call the `inferenceScheduled` callback, the ORT thread will be still stuck in the condition_variable
+    std::unique_lock lock(*m);
+    *completed = true;
+    cv->notify_one();  // force host thread to wake up
+  };
+  GlowAPI::ExecutionOptions eo = GetGlowExecutionOptions(info_.api_params);
+  auto expectedInferenceId = etglow_api.launchInference(glow_reg_model, std::move(bindings), inferenceCompleted, eo, inferenceScheduled);
+  if (!expectedInferenceId) {
+    return ORT_MAKE_STATUS(ONNXRUNTIME, FAIL, glowApiErrMsg(expectedInferenceId.takeError(), "executing an inference."));
+  }
+  [[maybe_unused]] const auto inferenceId = expectedInferenceId.get();
+
+  LOGS_DEFAULT(INFO) << "[ETGLOW EP] launching inference: " << std::to_string(inferenceId) << " ... waiting for glow to launch kernel";
+  // Block until inference has been scheduled on the device
+  {
+    std::unique_lock lock(*m);
+    cv->wait(lock, [&scheduled, &completed] { return *scheduled || *completed; });
+  }
+
+  GetPerThreadContext().SetComputeStream(stream_id);
+  GetPerThreadContext().AddInferenceInFlight(InferenceInFlight{inferenceId, std::move(inferenceResultPromise), std::move(inferenceResultFuture), std::move(outputs)});
+
+  LOGS_DEFAULT(INFO) << "[ETGLOW EP] launching inference: " << std::to_string(inferenceId) << " done.";
+
+  return Status::OK();
+}
+
+Status onnxruntime::EtGlowExecutionProvider::InferenceInFlight::Sync() {
+  LOGS_DEFAULT(INFO) << "[ETGLOW EP] synchronize inference: " << std::to_string(id);
+  auto inferenceResult = future.get();  // blocks current thread until future is available
+  ORT_RETURN_IF(inferenceResult.error, "[ETGLOW] The inference reports an error: ", glowApiErrMsg(std::move(inferenceResult.error), "executing an inference."));
+  ORT_RETURN_IF_NOT(inferenceResult.id == id, "[ETGLOW] Got a inference result from a future with different InferenceId than the expected!");
+  return Status::OK();
+}
+
+Status EtGlowExecutionProvider::PerThreadContext::SynchronizeInferences() {
+  for (auto it = inferences_in_flight_.begin(); it != inferences_in_flight_.end();) {
+    ORT_RETURN_IF_ERROR(it->Sync());
+    it = inferences_in_flight_.erase(it);
+  }
+  return Status::OK();
+}
+
+Status EtGlowExecutionProvider::UnloadModels() {
+  for (auto it = loaded_models_.begin(); it != loaded_models_.end();) {
+    ORT_RETURN_IF_ERROR(loaded_models_reg_.UnloadModel(*glow_api_, *it));
+    it = loaded_models_.erase(it);
+  }
+  return Status::OK();
+}
+
+EtGlowExecutionProvider::PerThreadContext& EtGlowExecutionProvider::GetPerThreadContext() const {
+  const auto& per_thread_context_cache = PerThreadContextCache();
+
+  // try to use cached context
+  if (auto cached_context_it = per_thread_context_cache->find(this);
+      cached_context_it != per_thread_context_cache->end()) {
+    auto cached_context = cached_context_it->second.lock();
+    ORT_ENFORCE(cached_context);
+    return *cached_context;
+  }
+
+  // get context and update cache
+  std::shared_ptr<PerThreadContext> context;
+  {
+    std::scoped_lock lock(context_state_.mutex);
+
+    // get or create a context
+    if (context_state_.retired_context_pool.empty()) {
+      context = std::make_shared<PerThreadContext>();
+    } else {
+      context = context_state_.retired_context_pool.back();
+      context_state_.retired_context_pool.pop_back();
+    }
+
+    // insert into active_contexts, should not already be present
+    const auto [active_contexts_insert_it, active_contexts_insert_success] = context_state_.active_contexts.insert(context);
+    ORT_UNUSED_PARAMETER(active_contexts_insert_it);
+    ORT_ENFORCE(active_contexts_insert_success);
+
+    // insert into caches_to_update_on_destruction, may already be present
+    ORT_IGNORE_RETURN_VALUE(context_state_.caches_to_update_on_destruction.insert(per_thread_context_cache));
+  }
+
+  per_thread_context_cache->insert(std::make_pair(this, context));
+
+  return *context;
+}
+
+void EtGlowExecutionProvider::ReleasePerThreadContext() const {
+  const auto& per_thread_context_cache = PerThreadContextCache();
+
+  auto cached_context_it = per_thread_context_cache->find(this);
+  ORT_ENFORCE(cached_context_it != per_thread_context_cache->end());
+  auto cached_context = cached_context_it->second.lock();
+  ORT_ENFORCE(cached_context);
+
+  {
+    std::scoped_lock lock(context_state_.mutex);
+    context_state_.active_contexts.erase(cached_context);
+    context_state_.retired_context_pool.push_back(cached_context);
+  }
+
+  per_thread_context_cache->erase(cached_context_it);
+}
+
+}  // namespace onnxruntime
diff --git a/onnxruntime/core/providers/etglow/etglow_execution_provider.h b/onnxruntime/core/providers/etglow/etglow_execution_provider.h
new file mode 100644
index 0000000000..7a86cbea6c
--- /dev/null
+++ b/onnxruntime/core/providers/etglow/etglow_execution_provider.h
@@ -0,0 +1,252 @@
+// Copyright (c) Microsoft Corporation. All rights reserved.
+// Licensed under the MIT License.
+
+#pragma once
+
+#include <glow/GlowAPI/etsoc.h>
+#include <runtime/IRuntime.h>
+
+#include <utility>
+#include <core/framework/arena_extend_strategy.h>
+
+#include "core/providers/shared_library/provider_api.h"
+#include "core/session/onnxruntime_c_api.h"
+#include "etglow_execution_provider_info.h"
+#include "etglow_op_manager.h"
+#include "etglow_compiled_models_registry.h"
+#include "etglow_loaded_models_registry.h"
+
+#include <mutex>   // for std::mutex
+#include <thread>  // std::thread::id
+
+using GlowAPI = etsoc::GlowAPI;
+
+namespace Ort {
+struct KernelContext;
+}
+
+namespace onnxruntime {
+struct EtRt_StreamsRegistry;
+
+namespace ort_etglow {
+
+// borrow from coreml/dnnl ep's data structures to organize data handle, shape and data
+struct OnnxTensorInfo {
+  int32_t data_type;  // Uses TensorProto::DataType
+  std::vector<int64_t> shape;
+};
+
+struct OnnxInputTensorData {
+  OnnxTensorInfo tensor_info;
+  const void* buffer;
+  OnnxInputTensorData(OnnxTensorInfo tensor_info, const void* buffer)
+      : tensor_info{std::move(tensor_info)}, buffer{buffer} {}
+};
+struct OnnxOutputTensorData {
+  OnnxTensorInfo tensor_info;
+  void* buffer{nullptr};
+};
+
+}  // namespace ort_etglow
+
+using EtGlowInferenceInputNames = std::unordered_map<std::string /* name */, size_t /* idx in graph */>;
+using EtGlowInferenceOutputNames = std::vector<std::string>;
+
+// Information to construct kernel function state.
+struct EtGlowFuncState {
+  AllocateFunc allocate_func = nullptr;
+  DestroyFunc release_func = nullptr;
+  AllocatorHandle allocator = nullptr;
+  std::string node_name;
+  GlowAPI* etglow_api = nullptr;
+  GlowAPI::CompiledModel* compiled_model = nullptr;
+  std::string bundle_name;
+
+  EtGlowInferenceInputNames ordered_input_names;
+  EtGlowInferenceOutputNames ordered_output_names;
+  std::unordered_map<std::string, std::vector<int64_t>> ordered_output_dimensions;
+};
+
+// Logical device representation.
+class EtGlowExecutionProvider : public IExecutionProvider {
+ public:
+  EtGlowExecutionProvider(EtGlowExecutionProviderInfo info, std::unique_ptr<etsoc::GlowAPI> glow, EtRt_StreamsRegistry& streams_reg);
+  ~EtGlowExecutionProvider() override;
+
+  std::unique_ptr<IDataTransfer> GetDataTransfer() const override;
+  std::unique_ptr<profiling::EpProfiler> GetProfiler() override;
+
+  std::vector<std::unique_ptr<ComputeCapability>>
+  GetCapability(const GraphViewer& graph,
+                const IKernelLookup& /*kernel_lookup*/) const override;
+
+  std::shared_ptr<KernelRegistry> GetKernelRegistry() const override;
+
+  int GetDeviceId() const override { return info_.device_id; }
+
+  ProviderOptions GetProviderOptions() const override {
+    return EtGlowExecutionProviderInfo::ToProviderOptions(info_);
+  }
+
+  common::Status OnSessionInitializationEnd() override {
+    if (info_.fail_if_cannot_run_whole_graph && !whole_graph_runs_with_glow_) {
+      return {common::ONNXRUNTIME, common::NOT_IMPLEMENTED,
+              "ETGLOW EP cannot run the whole graph. (as instructed) Failing execution on purpose."};
+    }
+    return Status::OK();
+  }
+
+  common::Status Compile(const std::vector<FusedNodeAndGraph>& fused_nodes_and_graphs,
+                         std::vector<NodeComputeInfo>& node_compute_funcs) override;
+
+  common::Status Sync() const override;
+  common::Status OnRunStart(const onnxruntime::RunOptions& /*run_options*/) override;
+  common::Status OnRunEnd(bool sync_stream, const onnxruntime::RunOptions& /*run_options*/) override;
+
+  void RegisterStreamHandlers(IStreamCommandHandleRegistry& stream_handle_registry, AllocatorMap& allocators) const override;
+
+  static AllocatorPtr CreateEtGlowAllocator(OrtDevice::DeviceId device_id, size_t mem_limit, ArenaExtendStrategy arena_extend_strategy, rt::IRuntime* rt);
+  std::vector<AllocatorPtr> CreatePreferredAllocators() override;
+
+ private:
+  void InitEtGlowStateFromEnvVars();
+
+  std::vector<std::vector<NodeIndex>> GetSupportedNodes(const GraphViewer& graph_viewer) const;
+  std::unique_ptr<IndexedSubGraph> GetSubGraph(const std::vector<std::size_t>& group,
+                                               const GraphViewer& graph_viewer, int subgraph_index) const;
+
+  static void RegisterOperatorCoverage(const Node& node, bool supported, std::unordered_map<std::string, int>& all_nodes_count, std::unordered_map<std::string, int>& supported_nodes_count);
+  static void PrintOperatorCoverage(const std::unordered_map<std::string, int>& all_nodes_count, std::unordered_map<std::string, int>& supported_nodes_count);
+  static void PrintGraphInputs(const GraphViewer& graph);
+
+  void DoDumpSubgraph(const GraphViewer& graph_viewer, const std::string& subgraph_name, bool include_initializers = true, bool include_outer_scope_args = true) const;
+
+  // Gets the user-provided replacements for ONNX parametric shapes.
+  // Returns OK + a map of {input + replacement_value} or an error if the user didn't provide a value for a given shape.
+  using OrderedShapeValues = std::map<std::string, std::size_t, std::less<>>;
+  Status GetShapeValues(const GraphViewer& graph_viewer, OrderedShapeValues& dim_params) const;
+  static Status GetInputNames(const GraphViewer& graph_viewer, const Node& fused_node, EtGlowInferenceInputNames& input_names);
+  static Status GetOutputNamesAndShapes(const Node& fused_node, EtGlowInferenceOutputNames& output_names, std::unordered_map<std::string, std::vector<int64_t>>& ordered_output_dimensions, const OrderedShapeValues& output_dim_params);
+
+  // Gets compiled model from existing bundle or compiles the model from original ONNX
+  Status GetCompiledModel(const GraphViewer& graph_viewer, const std::string& fused_node_name, const fs::path& bundle_path, const OrderedShapeValues& input_dim_params, std::unique_ptr<GlowAPI::CompiledModel>& compilation_result);
+  bool ShouldReuseCompiledModel(const fs::path& bundle_path) const;
+  static Status DoReuseCompiledModelFromBundle(const fs::path& bundle_path, std::unique_ptr<GlowAPI::CompiledModel>& compilation_result);
+  Status SetDevicePlaceholders(GlowAPI::Model& glow_loaded_model) const;
+  Status DoCompile(const fs::path& bundle_path, const GraphViewer& graph_viewer, const std::string& fused_node_name, const OrderedShapeValues& input_dim_params, std::unique_ptr<GlowAPI::CompiledModel>& compilation_result);
+
+  bool ShouldExportBundle() const;
+  static void ExportBundle(const fs::path& from, const fs::path& to);
+
+  using InferenceInputDescriptors = std::unordered_map<std::string, const ort_etglow::OnnxInputTensorData>;
+  using InferenceOutputDescriptors = std::unordered_map<std::string, const ort_etglow::OnnxOutputTensorData>;
+  static Status GetInputTensorDescriptors(const Ort::KernelContext& ctx, const EtGlowInferenceInputNames& ordered_input_names, InferenceInputDescriptors& inputs);
+  static Status GetOutputTensorDescriptors(const Ort::KernelContext& ctx, const EtGlowInferenceOutputNames& ordered_output_names, const std::unordered_map<std::string, std::vector<int64_t>>& ordered_output_dimensions, InferenceOutputDescriptors& outputs);
+  Status SetDevicePlaceholderAddresses(GlowAPI::ExecutionBindings& execution_bindings, const InferenceInputDescriptors& inputs, const InferenceOutputDescriptors& outputs, const std::vector<std::string>& device_placeholders) const;
+  Status DoInference(GlowAPI& etglow_api, const GlowAPI::RegisteredModel& glow_reg_model, const InferenceInputDescriptors& inputs, InferenceOutputDescriptors&& outputs, const std::vector<std::string>& device_placeholders, rt::StreamId stream_id) const;
+  void addToProfilerThreadMap();
+
+  struct InferenceResult {
+    GlowAPI::InferenceCallId id;
+    GlowAPI::Error error;
+    GlowAPI::ExecutionBindings bindings;
+  };
+  struct InferenceInFlight {
+    GlowAPI::InferenceCallId id;
+    std::unique_ptr<std::promise<InferenceResult>> promise;
+    std::future<InferenceResult> future;
+    InferenceOutputDescriptors output_descriptors;
+    InferenceInFlight(GlowAPI::InferenceCallId id, std::unique_ptr<std::promise<InferenceResult>> promise, std::future<InferenceResult> f, InferenceOutputDescriptors d)
+        : id{id}, promise{std::move(promise)}, future{std::move(f)}, output_descriptors{std::move(d)} {}
+
+    Status Sync();
+  };
+  Status UnloadModels();
+
+  EtGlowExecutionProviderInfo info_;
+  std::unique_ptr<etsoc::GlowAPI> glow_api_;
+  EtRt_StreamsRegistry& streams_reg_;
+
+  EtGlowCompiledModelsRegistry compiled_models_reg_;
+  EtGlowLoadedModelsRegistry loaded_models_reg_;
+  std::unordered_set<std::string> loaded_models_;
+
+  mutable bool whole_graph_runs_with_glow_ = true;
+
+  bool use_bundles_ = true;
+  bool save_bundles_ = true;
+  bool debug_logs_ = false;
+
+  EtGlowOpManager op_mgr_;
+  std::unique_ptr<ModelMetadefIdGenerator> metadef_id_generator_;
+
+  std::chrono::steady_clock::time_point start_ = std::chrono::steady_clock::now();
+  // Renaming map to unify the thread ids across ORT, glow, runtime and neuralizer profile events.
+  std::unordered_map<std::thread::id, unsigned int> profiler_thread_map_;
+  std::mutex thread_map_mutex_;
+
+  class PerThreadContext final {
+   public:
+    PerThreadContext() = default;
+    ~PerThreadContext() {
+      ORT_IGNORE_RETURN_VALUE(SynchronizeInferences());
+    };
+    ORT_DISALLOW_COPY_ASSIGNMENT_AND_MOVE(PerThreadContext);
+
+    void SetComputeStream(rt::StreamId stream) { compute_stream_ = stream; }
+    std::optional<rt::StreamId> GetComputeStream() const { return compute_stream_; }
+
+    void AddInferenceInFlight(InferenceInFlight&& iif) {
+      inferences_in_flight_.emplace_back(std::move(iif));
+    }
+
+    Status SynchronizeInferences();
+
+   private:
+    std::optional<rt::StreamId> compute_stream_;
+    std::list<InferenceInFlight> inferences_in_flight_;
+  };
+
+  using PerThreadContextMap = std::unordered_map<const EtGlowExecutionProvider*, std::weak_ptr<PerThreadContext>>;
+  // thread local PerThreadContext cache
+
+  struct ContextCacheHolder {
+    ContextCacheHolder() {
+      // Keep a weak pointer to the object, if the weak pointer can be locked, then the shared pointer is still around, so we can reset it
+      RunOnUnload([this, weak_p_ = std::weak_ptr<PerThreadContextMap>(p)] {
+        if (auto lock = weak_p_.lock())
+          p.reset();
+      });
+    }
+    std::shared_ptr<PerThreadContextMap> p = std::make_shared<PerThreadContextMap>();
+  };
+
+  static const std::shared_ptr<PerThreadContextMap>& PerThreadContextCache() {
+    thread_local const ContextCacheHolder per_thread_context_cache;
+    return per_thread_context_cache.p;
+  }
+
+  struct PerThreadContextState {
+    // contexts that are currently active
+    std::set<std::shared_ptr<PerThreadContext>, std::owner_less<std::shared_ptr<PerThreadContext>>> active_contexts;
+    // contexts available for reuse
+    std::vector<std::shared_ptr<PerThreadContext>> retired_context_pool;
+    // weak references to thread local caches from which this EtGlowExecutionProvider instance's entry should be removed
+    // upon destruction
+    std::set<std::weak_ptr<PerThreadContextMap>, std::owner_less<std::weak_ptr<PerThreadContextMap>>>
+        caches_to_update_on_destruction;
+    // synchronizes access to PerThreadContextState members
+    OrtMutex mutex;
+  };
+
+  // The execution provider maintains the PerThreadContexts in this structure.
+  // Synchronization is required to update the contained structures.
+  // On the other hand, access to an individual PerThreadContext is assumed to be from a single thread at a time,
+  // so synchronization is not required for that.
+  mutable PerThreadContextState context_state_;
+
+  PerThreadContext& GetPerThreadContext() const;
+  void ReleasePerThreadContext() const;
+};
+
+}  // namespace onnxruntime
diff --git a/onnxruntime/core/providers/etglow/etglow_execution_provider_bundle.cc b/onnxruntime/core/providers/etglow/etglow_execution_provider_bundle.cc
new file mode 100644
index 0000000000..9a7feb0971
--- /dev/null
+++ b/onnxruntime/core/providers/etglow/etglow_execution_provider_bundle.cc
@@ -0,0 +1,28 @@
+// Copyright (c) Microsoft Corporation. All rights reserved.
+// Licensed under the MIT License.
+
+#include "core/providers/etglow/etglow_execution_provider_bundle.h"
+#include "core/providers/etglow/etglow_execution_provider.h"
+
+namespace onnxruntime {
+
+BundleABI BundleABI::CreateWith(const GraphViewer& graph_viewer, const GlowAPIOptions& api_params, const OrderedShapeValues& dim_params, const TraceOptions& trace_options) {
+  auto to_bool = [](const std::string_view& s) {
+    return s != "0";
+  };
+  return BundleABI{
+      .graph_viewer = graph_viewer,
+      .useFP16 = to_bool(api_params.get_safe<std::string>(etglow::api_params::load_options::kUsefp16).value_or("0")),
+      .useInt32Indices = to_bool(api_params.get_safe<std::string>(etglow::api_params::load_options::kUseInt32Indices).value_or("0")),
+      .onnxZip = to_bool(api_params.get_safe<std::string>(etglow::api_params::load_options::kOnnxZip).value_or("0")),
+      .symbolTable = dim_params,
+
+      .placeholders = api_params.get_safe<std::vector<std::string>>(etglow::api_params::before_compile_options::kDevicePlaceholder).value_or(std::vector<std::string>{}),
+      .placeholders_implicit = api_params.get_safe<std::vector<std::string>>(etglow::api_params::before_compile_options::kImplicitDevicePlaceholder).value_or(std::vector<std::string>{}),
+
+      .trace_device_kernel_nodes_enable = trace_options.trace_device_kernel_nodes_enable,
+      .trace_device_kernel_inst_enable = trace_options.trace_device_kernel_inst_enable,
+  };
+}
+
+}  // namespace onnxruntime
\ No newline at end of file
diff --git a/onnxruntime/core/providers/etglow/etglow_execution_provider_bundle.h b/onnxruntime/core/providers/etglow/etglow_execution_provider_bundle.h
new file mode 100644
index 0000000000..0dc199511c
--- /dev/null
+++ b/onnxruntime/core/providers/etglow/etglow_execution_provider_bundle.h
@@ -0,0 +1,211 @@
+// Copyright (c) Microsoft Corporation. All rights reserved.
+// Licensed under the MIT License.
+
+#pragma once
+
+#include "core/providers/shared_library/provider_api.h"
+#include "core/providers/etglow/etglow_execution_provider_info.h"
+#include "core/framework/murmurhash3.h"
+
+#include <algorithm>
+#include <array>
+#include <memory>
+#include <vector>
+#include <string>
+#include <map>
+
+namespace onnxruntime {
+class GraphViewer;
+
+// List of parameters that affect the bundle
+struct BundleABI {
+  const GraphViewer& graph_viewer;
+
+  // LoadOptions
+  bool useFP16;
+  bool useInt32Indices;
+  bool onnxZip;
+  using OrderedShapeValues = std::map<std::string, std::size_t, std::less<>>;
+  OrderedShapeValues symbolTable;
+  // CompilationOptions
+  // ..
+  std::vector<std::string> placeholders;
+  std::vector<std::string> placeholders_implicit;
+  // TraceOptions that affect CompiledModel
+  bool trace_device_kernel_nodes_enable;
+  bool trace_device_kernel_inst_enable;
+
+  static BundleABI CreateWith(const GraphViewer& graph_viewer, const GlowAPIOptions& api_params, const OrderedShapeValues& dim_params, const TraceOptions& trace_options);
+};
+
+}  // end namespace onnxruntime
+
+namespace std {
+template <>
+struct hash<onnxruntime::BundleABI> {
+  std::uint64_t operator()(const onnxruntime::BundleABI& b) const {
+    std::uint64_t bundle_hash = 0;
+
+    // find the top level graph
+    const onnxruntime::Graph* curr_graph = &b.graph_viewer.GetGraph();
+    while (curr_graph->IsSubgraph()) {
+      curr_graph = curr_graph->ParentGraph();
+    }
+
+    std::array<uint32_t, 4> hash = {0, 0, 0, 0};
+    const onnxruntime::Graph& main_graph = *curr_graph;
+    auto hash_combine = [&hash](const std::string_view& str) {
+      onnxruntime::MurmurHash3::x86_128(str.data(), gsl::narrow_cast<int32_t>(str.size()), hash[0], &hash);
+    };
+
+    if (main_graph.ModelPath().has_filename()) {
+      std::string model_name = onnxruntime::PathToUTF8String(main_graph.ModelPath().filename());
+
+      const size_t model_name_length = model_name.size();
+      constexpr size_t hash_string_length = 500;
+      std::string repeat_model_name = model_name;
+      for (size_t i = model_name_length; i > 0 && i < hash_string_length; i += model_name_length) {
+        repeat_model_name += model_name;
+      }
+      hash_combine(repeat_model_name);
+    }
+
+    // fingerprint current graph by hashing graph inputs
+    for (const auto* node_arg : b.graph_viewer.GetInputsIncludingInitializers()) {
+      hash_combine(node_arg->Name());
+
+      // include inputs type in the fingerprint
+      hash_combine(*node_arg->Type());
+
+      // include inputs shape in the fingerprint
+      const auto* input_shape = node_arg->Shape();
+      if (input_shape == nullptr) {
+        continue;  // skip those that are nullptr
+      }
+      for (const auto& dim : input_shape->dim()) {
+        if (dim.has_dim_param()) {
+          hash_combine(dim.dim_param());
+        } else {
+          hash_combine(std::to_string(dim.dim_value()));
+        }
+      }
+    }
+    // fingerprint initialized tensors (including weights) (Currently disabled)
+    // SW-21049: This may prevent us to distinguish fine-tuned models with the same name as the base architecture
+    if (const auto& initialized_tensors = b.graph_viewer.GetAllInitializedTensors(); !initialized_tensors.empty()) {
+      std::ostringstream message("Not incorporating initialized tensors (weights) as part of the bundle hash. Be aware!");
+      if (const auto envvar_value = onnxruntime::GetEnvironmentVar(onnxruntime::etglow::envvars::kBundleCacheEnable); envvar_value != "0") {
+        message << " consider using " << onnxruntime::etglow::envvars::kBundleCacheEnable << "=0 envvar";
+      }
+      LOGS_DEFAULT(WARNING) << message.str();
+    }
+
+    // hashing all information of a node
+    // - name
+    // - domain
+    // - op type
+    // - all attributes (name and values)
+    // - all output names
+    const int number_of_ort_nodes = b.graph_viewer.NumberOfNodes();
+    std::vector<size_t> nodes_vector(number_of_ort_nodes);
+    std::iota(std::begin(nodes_vector), std::end(nodes_vector), 0);
+    const std::vector<onnxruntime::NodeIndex>& node_index = b.graph_viewer.GetNodesInTopologicalOrder();
+    for (const auto& index : nodes_vector) {
+      const auto& node = b.graph_viewer.GetNode(node_index[index]);
+      hash_combine(node->Name());
+      hash_combine(node->Domain());
+      hash_combine(node->OpType());
+      hash_combine(std::to_string(node->SinceVersion()));
+      for (const auto& [node_attr_name, node_attr_proto] : node->GetAttributes()) {
+        hash_combine(node_attr_name);
+        hash_combine(node_attr_proto.name());
+        hash_combine(to_string(node_attr_proto.type()));
+
+        switch (node_attr_proto.type()) {
+          case ONNX_NAMESPACE::AttributeProto_AttributeType_FLOAT:
+            hash_combine(std::to_string(node_attr_proto.f()));
+            break;
+          case ONNX_NAMESPACE::AttributeProto_AttributeType_FLOATS:
+            for (int i = 0; i < node_attr_proto.floats_size(); i++) {
+              hash_combine(std::to_string(node_attr_proto.floats(i)));
+            }
+            break;
+          case ONNX_NAMESPACE::AttributeProto_AttributeType_INT:
+            hash_combine(std::to_string(node_attr_proto.i()));
+            break;
+          case ONNX_NAMESPACE::AttributeProto_AttributeType_INTS:
+            for (int i = 0; i < node_attr_proto.ints_size(); i++) {
+              hash_combine(std::to_string(node_attr_proto.ints(i)));
+            }
+            break;
+          case ONNX_NAMESPACE::AttributeProto_AttributeType_STRING:
+            hash_combine(node_attr_proto.s());
+            break;
+          case ONNX_NAMESPACE::AttributeProto_AttributeType_STRINGS: {
+            for (int i = 0; i < node_attr_proto.ints_size(); i++) {
+              hash_combine(node_attr_proto.strings(i));
+            }
+            break;
+          }
+          // AttributeTypes without helpers to access attr contents
+          // BundleAbi is then not accurate on those cases.
+          // Proceeding as best effort.
+          case ONNX_NAMESPACE::AttributeProto_AttributeType_TENSOR:
+          case ONNX_NAMESPACE::AttributeProto_AttributeType_GRAPH:
+          case ONNX_NAMESPACE::AttributeProto_AttributeType_GRAPHS:
+#if !defined(DISABLE_SPARSE_TENSORS)
+          case ONNX_NAMESPACE::AttributeProto_AttributeType_SPARSE_TENSOR:
+          case ONNX_NAMESPACE::AttributeProto_AttributeType_SPARSE_TENSORS:
+#endif
+            LOGS_DEFAULT(WARNING) << "Model contain AttributeProto " << node_attr_proto.name()
+                                  << " of type " << to_string(node_attr_proto.type())
+                                  << " which cannot be incorporated into the Bundle hash. "
+                                  << "consider disabling bundle cache with "
+                                  << onnxruntime::etglow::envvars::kBundleCacheEnable << "=0 envvar";
+            break;
+          // Don't know if AttributeType UNDEFINED should be considered as 1) malformed Model or 2) as previous group
+          // Choosing 1) for now.
+          case ONNX_NAMESPACE::AttributeProto_AttributeType_UNDEFINED:
+          default:
+            ORT_THROW("Unsupported attribute value type of ", node_attr_proto.type(), " cannot compute BundleAbi");
+        }
+      }
+      for (const auto* node_arg : node->OutputDefs()) {
+        if (node_arg->Exists()) {
+          hash_combine(node_arg->Name());
+        }
+      }
+    }
+
+    // hashing all GlowAPI parameters that affect the bundle
+    hash_combine(std::to_string(static_cast<int>(b.useFP16)));
+    hash_combine(std::to_string(static_cast<int>(b.useInt32Indices)));
+    hash_combine(std::to_string(static_cast<int>(b.onnxZip)));
+    for (auto&& [key, value] : b.symbolTable) {
+      hash_combine(key);
+      hash_combine(std::to_string(value));
+    }
+    for (auto&& placeholder : b.placeholders) {
+      hash_combine(placeholder);
+    }
+    for (auto&& placeholder_iml : b.placeholders_implicit) {
+      hash_combine(placeholder_iml);
+    }
+    hash_combine(std::to_string(static_cast<int>(b.trace_device_kernel_nodes_enable)));
+    hash_combine(std::to_string(static_cast<int>(b.trace_device_kernel_inst_enable)));
+
+#ifdef __linux__
+    hash_combine("LINUX");
+#elif defined(_WIN32)
+    hash_combine("WINDOWS");
+#endif
+#ifdef ORT_VERSION
+    hash_combine(ORT_VERSION);
+#endif
+
+    constexpr uint32_t NUMBER_OF_BITS_TO_SHIFT = 32u;
+    bundle_hash = hash[0] | (static_cast<uint64_t>(hash[1]) << NUMBER_OF_BITS_TO_SHIFT);
+    return bundle_hash;
+  }
+};
+}  // namespace std
diff --git a/onnxruntime/core/providers/etglow/etglow_execution_provider_info.cc b/onnxruntime/core/providers/etglow/etglow_execution_provider_info.cc
new file mode 100644
index 0000000000..c008b183b3
--- /dev/null
+++ b/onnxruntime/core/providers/etglow/etglow_execution_provider_info.cc
@@ -0,0 +1,345 @@
+// Copyright (c) Microsoft Corporation. All rights reserved.
+// Licensed under the MIT License.
+
+#include "core/providers/etglow/etglow_execution_provider_info.h"
+#include "core/providers/etglow/etglow_provider_options.h"
+
+#include "core/common/common.h"
+#include "core/common/make_string.h"
+#include "core/common/parse_string.h"
+#include "core/framework/provider_options_utils.h"
+
+#include <gsl/gsl>
+
+#include <string>
+#include <cctype>
+#include <algorithm>
+
+namespace onnxruntime {
+
+template <typename>
+inline constexpr bool always_false_v = false;
+
+bool IsKnownGlowAPIParameter(const std::string& param) {
+  std::vector<std::string_view> known_options{
+      onnxruntime::etglow::api_params::kDeviceType,
+      onnxruntime::etglow::api_params::kRunDir,
+      onnxruntime::etglow::api_params::kActiveShires,
+      onnxruntime::etglow::api_params::kKernelLaunchTimeout,
+      onnxruntime::etglow::api_params::kGlowThreads,
+      onnxruntime::etglow::api_params::kTraceRuntimePath,
+      onnxruntime::etglow::api_params::kTraceNeuralizerPath,
+      onnxruntime::etglow::api_params::kTraceNeuralizerNodes,
+      onnxruntime::etglow::api_params::kTraceDescriptorsPath,
+      onnxruntime::etglow::api_params::kEnableCoreDump,
+      onnxruntime::etglow::api_params::kExtraEtsocParams,
+      onnxruntime::etglow::api_params::host_config::kMaxActiveRequests,
+      onnxruntime::etglow::api_params::host_config::kMaxQueueSize,
+      onnxruntime::etglow::api_params::host_config::kExecutorThreads,
+      onnxruntime::etglow::api_params::device_config::kDeviceMemory,
+      onnxruntime::etglow::api_params::device_config::kBackendParameters,
+      onnxruntime::etglow::api_params::before_compile_options::kDevicePlaceholder,
+      onnxruntime::etglow::api_params::before_compile_options::kImplicitDevicePlaceholder,
+      onnxruntime::etglow::api_params::load_options::kUsefp16,
+      onnxruntime::etglow::api_params::load_options::kUseInt32Indices,
+      onnxruntime::etglow::api_params::load_options::kOnnxZip,
+      onnxruntime::etglow::api_params::exec_options::kPriority,
+      // FILLME with any extra option added to etglow_execution_provider_info.h
+  };
+  return std::find(known_options.begin(), known_options.end(), param) != known_options.end();
+};
+
+Status ParseKeyValueStringWithClassicLocale(std::string_view from, ONNXSymbolsShapes& to) {
+  if (from.empty()) {
+    return Status::OK();
+  }
+  std::stringstream ss(std::string(from), std::ios_base::in);
+  std::string tmp;
+  std::vector<std::string> shape_params;
+  while (std::getline(ss, tmp, ';')) {
+    tmp.erase(std::remove_if(tmp.begin(), tmp.end(), [](unsigned char x) { return std::isspace(x); }), tmp.end());
+    shape_params.emplace_back(tmp);
+  }
+  for (const auto& api_param : shape_params) {
+    auto sep_pos = api_param.find('=');
+    if (sep_pos == std::string::npos || sep_pos == 0 || sep_pos == api_param.length()) {
+      return ORT_MAKE_STATUS(ONNXRUNTIME, INVALID_ARGUMENT,
+                             "The value for the keys in '" + std::string{from} +
+                                 "' should have '=' separator."
+                                 " Found: " +
+                                 api_param + "\n");
+    }
+    auto shape_param_key = api_param.substr(0, sep_pos);
+    auto shape_param_value = api_param.substr(sep_pos + 1);
+
+    to.try_emplace(shape_param_key, static_cast<size_t>(std::stoi(shape_param_value)));
+  }
+  return Status::OK();
+}
+
+Status ParseKeyValueStringWithClassicLocale(std::string_view str, GlowAPIOptions& value) {
+  if (str.empty()) {
+    return Status::OK();
+  }
+  std::stringstream ss(std::string(str), std::ios_base::in);
+  std::string tmp;
+  std::vector<std::string> api_params;
+  while (std::getline(ss, tmp, ';')) {
+    api_params.emplace_back(tmp);
+  }
+  for (auto shape_param : api_params) {
+    // remove (if any) single-quotes from string to avoid problems parsing keys and values
+    shape_param.erase(remove(shape_param.begin(), shape_param.end(), '\''), shape_param.end());
+
+    auto sep_pos = shape_param.find('=');
+    if (sep_pos == std::string::npos || sep_pos == 0 || sep_pos == shape_param.length()) {
+      return ORT_MAKE_STATUS(ONNXRUNTIME, INVALID_ARGUMENT,
+                             "The value for the keys in '" + std::string{str} +
+                                 "' should have '=' separator."
+                                 " Found: " +
+                                 shape_param + "\n");
+    }
+    auto shape_param_key = shape_param.substr(0, sep_pos);
+    auto shape_param_value = shape_param.substr(sep_pos + 1);
+
+    if (!IsKnownGlowAPIParameter(shape_param_key)) {
+      return ORT_MAKE_STATUS(ONNXRUNTIME, INVALID_ARGUMENT, "The provided value `" + shape_param_key + " is not known to ETGLOW EP.");
+    }
+
+    const bool isExtraEtsocParams = shape_param_key == etglow::api_params::kExtraEtsocParams;
+    const bool isDevicePlaceholder = shape_param_key == etglow::api_params::before_compile_options::kDevicePlaceholder;
+    const bool isImplicitDevicePlaceholder = shape_param_key == etglow::api_params::before_compile_options::kImplicitDevicePlaceholder;
+    if (isExtraEtsocParams) {
+      std::stringstream ss2(shape_param_value, std::ios_base::in);
+      std::string tmp2;
+      std::vector<std::string> extra_etsoc_params;
+      while (std::getline(ss2, tmp2, '|')) {
+        extra_etsoc_params.emplace_back(tmp2);
+      }
+      for (auto extra_etsoc_param : extra_etsoc_params) {
+        auto string_starts_with = [](const std::string& word, std::string_view substr) { return word.rfind(substr, 0) == 0; };
+        // glow unconditionally adds quotes to 'dev' so if the user also provides them we might end-up with double quotes on 'dev' param which might confuse users
+        // attempt removing quotes from parameter to avoid double-quoting issues
+        if (string_starts_with(extra_etsoc_param, "dev")) {
+          extra_etsoc_param.erase(std::remove(extra_etsoc_param.begin(), extra_etsoc_param.end(), '\"'), extra_etsoc_param.end());
+        }
+        // fill std::vector<std::string>
+        auto it = value.find(shape_param_key);
+        if (it != value.end()) {
+          std::get<std::vector<std::string>>(it->second).emplace_back(extra_etsoc_param);
+        } else {
+          value.try_emplace(shape_param_key, std::vector<std::string>{extra_etsoc_param});
+        }
+      }
+    } else if (isDevicePlaceholder || isImplicitDevicePlaceholder) {
+      // fill std::vector<std::string>
+      auto it = value.find(shape_param_key);
+      if (it != value.end()) {
+        std::get<std::vector<std::string>>(it->second).emplace_back(shape_param_value);
+      } else {
+        value.try_emplace(shape_param_key, std::vector<std::string>{shape_param_value});
+      }
+    } else {
+      // fill std::string
+      value.try_emplace(shape_param_key, shape_param_value);
+    }
+  }
+  return Status::OK();
+}
+
+ONNXSymbolsShapes ParseONNXSymbolsFromUserString(const char* s) {
+  ONNXSymbolsShapes r;
+  if (s == nullptr) {
+    return r;
+  }
+  std::string str{s};
+  ORT_THROW_IF_ERROR(ParseKeyValueStringWithClassicLocale(str, r));
+  return r;
+}
+GlowAPIOptions ParseGlowAPIOptionsFromUserString(const char* s) {
+  GlowAPIOptions r;
+  if (s == nullptr) {
+    return r;
+  }
+  std::string str{s};
+  ORT_THROW_IF_ERROR(ParseKeyValueStringWithClassicLocale(str, r));
+  return r;
+}
+
+class OptionsToStringVisitor {
+ public:
+  std::string operator()(std::size_t arg) const {
+    return std::to_string(arg);
+  }
+  std::string operator()(std::string arg) const {
+    return arg;
+  }
+  std::string operator()(const std::vector<std::string>& arg) const {
+    std::string rr;
+    for (const auto& piece : arg) {
+      if (!rr.empty()) {
+        rr += "|";
+      }
+      rr += piece;
+    }
+    char delim = '\'';
+    char escape = '\\';
+    std::stringstream ss;
+    ss << std::quoted(rr, delim, escape);
+    rr = ss.str();
+    return rr;
+  }
+};
+
+EtGlowExecutionProviderInfo EtGlowExecutionProviderInfo::FromProviderOptions(const onnxruntime::ProviderOptions& options) {
+  EtGlowExecutionProviderInfo info{};
+  ORT_THROW_IF_ERROR(
+      ProviderOptionsParser{}
+          .AddAssignmentToReference(etglow::provider_option_names::kDeviceId, info.device_id)
+          .AddAssignmentToReference(etglow::provider_option_names::kDeviceMemLimit, info.device_mem_limit)
+          .AddAssignmentToEnumReference(etglow::provider_option_names::kArenaExtendStrategy, etglow::provider_option_enum_mappings::arena_extend_strategy_mapping, info.arena_extend_strategy)
+          .AddAssignmentToReference(etglow::provider_option_names::kCompileOnly, info.compile_only)
+          .AddAssignmentToReference(etglow::provider_option_names::kDumpSubgraphs, info.dump_subgraphs)
+          .AddAssignmentToReference(etglow::provider_option_names::kGreedy, info.greedy)
+          .AddAssignmentToReference(etglow::provider_option_names::kOfflineMode, info.offline_mode)
+          .AddAssignmentToReference(etglow::provider_option_names::kFailIfCannotRunWholeGraph, info.fail_if_cannot_run_whole_graph)
+          .AddValueParser(
+              etglow::provider_option_names::kOnnxShapeParams,
+              [&info](const std::string& value_str) -> Status {
+                ORT_RETURN_IF_ERROR(ParseKeyValueStringWithClassicLocale(value_str, info.onnx_symbols));
+                return Status::OK();
+              })
+          .AddValueParser(
+              etglow::provider_option_names::kApiParams,
+              [&info](const std::string& value_str) -> Status {
+                ORT_RETURN_IF_ERROR(ParseKeyValueStringWithClassicLocale(value_str, info.api_params));
+                return Status::OK();
+              })
+          .AddAssignmentToReference(etglow::provider_option_names::kBundleCachePrefix, info.bundle_cache_prefix)
+          .AddAssignmentToReference(etglow::provider_option_names::kExportBundlePath, info.export_bundle_path)
+          .AddAssignmentToReference(etglow::provider_option_names::kTracesPrefix, info.traces_dir_prefix)
+          .AddValueParser(
+              etglow::provider_option_names::kTracesFlags,
+              [&info](const std::string& value_str) -> Status {
+                info.trace_options = TraceOptions(std::stoi(value_str));
+                return Status::OK();
+              })
+          .Parse(options));
+
+  return info;
+}
+
+ProviderOptions EtGlowExecutionProviderInfo::ToProviderOptions(const EtGlowExecutionProviderInfo& info) {
+  auto with_quotes = [](const std::string& s) {
+    if (s.empty()) {
+      return s;
+    }
+    std::stringstream ss;
+    ss << std::quoted(s);
+    return ss.str();
+  };
+
+  const std::string onnx_symbols = info.onnx_symbols.string(OptionsToStringVisitor{});
+  const std::string api_params = info.api_params.string(OptionsToStringVisitor{});
+  const std::string bundle_cache_prefix = info.bundle_cache_prefix.string();
+  const std::string export_bundle_path = info.export_bundle_path.string();
+  const std::string trace_prefix = info.traces_dir_prefix.string();
+
+  ProviderOptions options{
+      {etglow::provider_option_names::kDeviceId, MakeStringWithClassicLocale(info.device_id)},
+      {etglow::provider_option_names::kDeviceMemLimit, MakeStringWithClassicLocale(info.device_mem_limit)},
+      {etglow::provider_option_names::kArenaExtendStrategy, EnumToName(etglow::provider_option_enum_mappings::arena_extend_strategy_mapping, info.arena_extend_strategy)},
+      {etglow::provider_option_names::kCompileOnly, MakeStringWithClassicLocale(info.compile_only)},
+      {etglow::provider_option_names::kDumpSubgraphs, MakeStringWithClassicLocale(info.dump_subgraphs)},
+      {etglow::provider_option_names::kGreedy, MakeStringWithClassicLocale(info.greedy)},
+      {etglow::provider_option_names::kOfflineMode, MakeStringWithClassicLocale(info.offline_mode)},
+      {etglow::provider_option_names::kFailIfCannotRunWholeGraph, MakeStringWithClassicLocale(info.fail_if_cannot_run_whole_graph)},
+      {etglow::provider_option_names::kTracesFlags, MakeStringWithClassicLocale(info.trace_options.as_int())},
+  };
+  auto add_option_if_non_empty = [&options](std::string&& key, std::string&& value) {
+    if (!value.empty()) {
+      options.try_emplace(std::move(key), std::move(value));
+    }
+  };
+  add_option_if_non_empty(etglow::provider_option_names::kOnnxShapeParams, with_quotes(onnx_symbols));
+  add_option_if_non_empty(etglow::provider_option_names::kApiParams, with_quotes(api_params));
+  add_option_if_non_empty(etglow::provider_option_names::kBundleCachePrefix, with_quotes(bundle_cache_prefix));
+  add_option_if_non_empty(etglow::provider_option_names::kExportBundlePath, with_quotes(export_bundle_path));
+  add_option_if_non_empty(etglow::provider_option_names::kTracesPrefix, with_quotes(trace_prefix));
+  return options;
+}
+
+ProviderOptions EtGlowExecutionProviderInfo::ToProviderOptions(const OrtEtGlowProviderOptions& info) {
+  auto with_quotes = [](const std::string& s) {
+    if (s.empty()) {
+      return s;
+    }
+    std::stringstream ss;
+    ss << std::quoted(s);
+    return ss.str();
+  };
+  auto empty_if_null = [](const char* s) { return s != nullptr ? std::string{s} : std::string{}; };
+  const std::string onnx_symbols = with_quotes(empty_if_null(info.et_onnx_symbols));
+  const std::string api_params = with_quotes(empty_if_null(info.et_glow_api_params));
+  const std::string bundle_cache_prefix = with_quotes(empty_if_null(info.et_bundle_cache_prefix));
+  const std::string export_bundle_path = with_quotes(empty_if_null(info.et_export_bundle_path));
+  const std::string traces_prefix = with_quotes(empty_if_null(info.et_traces_prefix));
+
+  ProviderOptions options{
+      {etglow::provider_option_names::kDeviceId, MakeStringWithClassicLocale(info.device_id)},
+      {etglow::provider_option_names::kDeviceMemLimit, MakeStringWithClassicLocale(info.device_mem_limit)},
+      {etglow::provider_option_names::kArenaExtendStrategy, MakeStringWithClassicLocale(info.arena_extend_strategy)},
+      {etglow::provider_option_names::kCompileOnly, MakeStringWithClassicLocale(info.et_compile_only)},
+      {etglow::provider_option_names::kDumpSubgraphs, MakeStringWithClassicLocale(info.et_dump_subgraphs)},
+      {etglow::provider_option_names::kGreedy, MakeStringWithClassicLocale(info.et_greedy)},
+      {etglow::provider_option_names::kOfflineMode, MakeStringWithClassicLocale(info.et_offline_mode)},
+      {etglow::provider_option_names::kFailIfCannotRunWholeGraph, MakeStringWithClassicLocale(info.et_fail_if_cannot_run_whole_graph)},
+      {etglow::provider_option_names::kOnnxShapeParams, onnx_symbols},
+      {etglow::provider_option_names::kApiParams, api_params},
+      {etglow::provider_option_names::kBundleCachePrefix, bundle_cache_prefix},
+      {etglow::provider_option_names::kExportBundlePath, export_bundle_path},
+      {etglow::provider_option_names::kTracesPrefix, traces_prefix},
+      {etglow::provider_option_names::kTracesFlags, MakeStringWithClassicLocale(info.et_traces_flags)},
+  };
+  return options;
+}
+
+void EtGlowExecutionProviderInfo::UpdateProviderOptions(void* provider_options, const ProviderOptions& options) {
+  if (provider_options == nullptr) {
+    return;
+  }
+  auto copy_string = [&](std::string&& s_in) {
+    auto str_size = s_in.size();
+    if (str_size == 0) {
+      return static_cast<const char*>(nullptr);
+    }
+    gsl::owner<char*> dest = new char[str_size + 1];
+#ifdef _MSC_VER
+    strncpy_s(dest, str_size + 1, s_in.c_str(), str_size);
+#else
+    strncpy(dest, s_in.c_str(), str_size);
+#endif
+    dest[str_size] = '\0';
+    return static_cast<const char*>(dest);
+  };
+
+  EtGlowExecutionProviderInfo internal_options = onnxruntime::EtGlowExecutionProviderInfo::FromProviderOptions(options);
+
+  auto& etglow_provider_options = *reinterpret_cast<OrtEtGlowProviderOptions*>(provider_options);
+  etglow_provider_options.device_id = internal_options.device_id;
+  etglow_provider_options.device_mem_limit = internal_options.device_mem_limit;
+  etglow_provider_options.arena_extend_strategy = std::stoi(EnumToName(etglow::provider_option_enum_mappings::arena_extend_strategy_mapping, internal_options.arena_extend_strategy));
+  etglow_provider_options.et_compile_only = internal_options.compile_only ? 1 : 0;
+  etglow_provider_options.et_dump_subgraphs = internal_options.dump_subgraphs ? 1 : 0;
+  etglow_provider_options.et_greedy = internal_options.greedy ? 1 : 0;
+  etglow_provider_options.et_offline_mode = internal_options.offline_mode ? 1 : 0;
+  etglow_provider_options.et_fail_if_cannot_run_whole_graph = internal_options.fail_if_cannot_run_whole_graph ? 1 : 0;
+  etglow_provider_options.et_onnx_symbols = copy_string(internal_options.onnx_symbols.string(OptionsToStringVisitor{}));
+  etglow_provider_options.et_glow_api_params = copy_string(internal_options.api_params.string(OptionsToStringVisitor{}));
+  etglow_provider_options.et_bundle_cache_prefix = copy_string(internal_options.bundle_cache_prefix.string());
+  etglow_provider_options.et_export_bundle_path = copy_string(internal_options.export_bundle_path.string());
+  etglow_provider_options.et_traces_prefix = copy_string(internal_options.traces_dir_prefix.string());
+  etglow_provider_options.et_traces_flags = internal_options.trace_options.as_int();
+}
+
+}  // end namespace onnxruntime
diff --git a/onnxruntime/core/providers/etglow/etglow_execution_provider_info.h b/onnxruntime/core/providers/etglow/etglow_execution_provider_info.h
new file mode 100644
index 0000000000..10efed5c7d
--- /dev/null
+++ b/onnxruntime/core/providers/etglow/etglow_execution_provider_info.h
@@ -0,0 +1,228 @@
+// Copyright (c) Microsoft Corporation. All rights reserved.
+// Licensed under the MIT License.
+
+#pragma once
+
+#include "core/framework/arena_extend_strategy.h"
+#include "core/framework/ortdevice.h"
+#include "core/framework/provider_options.h"
+#include "core/framework/provider_options_utils.h"
+#include "core/session/onnxruntime_c_api.h"
+#include "core/providers/etglow/etglow_provider_options.h"
+
+#include <utility>
+#include <variant>
+#include <optional>
+
+#ifndef __has_include
+static_assert(false, "__has_include not supported");
+#else
+#if __cplusplus >= 201703L && __has_include(<filesystem>)
+#include <filesystem>
+namespace fs = std::filesystem;
+#elif __has_include(<experimental/filesystem>)
+#include <experimental/filesystem>
+namespace fs = std::experimental::filesystem;
+#else
+static_assert(false, "filesystem includ not available");
+#endif
+#endif
+
+namespace onnxruntime {
+namespace etglow {
+namespace provider_option_names {
+constexpr const char* kDeviceId = "device_id";
+constexpr const char* kDeviceMemLimit = "device_mem_limit";
+constexpr const char* kArenaExtendStrategy = "arena_extend_strategy";
+constexpr const char* kCompileOnly = "etglow_compile_only";
+constexpr const char* kDumpSubgraphs = "etglow_dump_subgraphs";
+constexpr const char* kGreedy = "etglow_greedy";
+constexpr const char* kOfflineMode = "etglow_offline_mode";
+constexpr const char* kFailIfCannotRunWholeGraph = "etglow_fail_if_cannot_run_whole_graph";
+constexpr const char* kOnnxShapeParams = "etglow_onnx_shape_params";
+constexpr const char* kApiParams = "etglow_api_params";
+constexpr const char* kBundleCachePrefix = "etglow_bundle_cache_prefix";
+constexpr const char* kExportBundlePath = "etglow_export_bundle_path";
+constexpr const char* kTracesPrefix = "etglow_traces_prefix";
+constexpr const char* kTracesFlags = "etglow_traces_flags";
+}  // namespace provider_option_names
+
+namespace provider_option_enum_mappings {
+const EnumNameMapping<ArenaExtendStrategy> arena_extend_strategy_mapping{
+    {ArenaExtendStrategy::kNextPowerOfTwo, "0"},
+    {ArenaExtendStrategy::kSameAsRequested, "1"},
+};
+}  // namespace provider_option_enum_mappings
+
+namespace api_params {
+// OBS: when adding new options, please update IsKnownGlowAPIParameter
+// globals
+constexpr const char* kDeviceType = "device-type";
+constexpr const char* kRunDir = "runDir";
+constexpr const char* kActiveShires = "active-shires";
+constexpr const char* kKernelLaunchTimeout = "kernel-launch-timeout";
+constexpr const char* kGlowThreads = "glow-threads";
+constexpr const char* kTraceRuntimePath = "trace-runtime-path";
+constexpr const char* kTraceNeuralizerPath = "trace-neuralizer-path";
+constexpr const char* kTraceNeuralizerNodes = "trace-neuralizer-nodes";
+constexpr const char* kTraceDescriptorsPath = "trace-descriptors-path";
+constexpr const char* kEnableCoreDump = "enableCoreDump";
+constexpr const char* kExtraEtsocParams = "extra-etsoc-params";
+namespace host_config {
+constexpr const char* kMaxActiveRequests = "maxActiveRequests";
+constexpr const char* kMaxQueueSize = "maxQueueSize";
+constexpr const char* kExecutorThreads = "executorThreads";
+}  // namespace host_config
+namespace device_config {
+constexpr const char* kDeviceMemory = "deviceMemory";
+constexpr const char* kBackendParameters = "parameters";
+}  // namespace device_config
+namespace before_compile_options {
+constexpr const char* kDevicePlaceholder = "placeholder";
+constexpr const char* kImplicitDevicePlaceholder = "implicit-placeholder";
+}  // namespace before_compile_options
+namespace load_options {
+constexpr const char* kUsefp16 = "useFP16";
+constexpr const char* kUseInt32Indices = "useInt32Indices";
+constexpr const char* kOnnxZip = "onnxZip";
+}  // namespace load_options
+namespace exec_options {
+constexpr const char* kPriority = "priority";
+}  // namespace exec_options
+}  // namespace api_params
+
+namespace envvars {
+constexpr const char* kGreedyEp = "ORT_ETGLOW_GREEDY";
+constexpr const char* kOfflineModeEp = "ORT_ETGLOW_OFFLINE_MODE";
+constexpr const char* kFailIfUnsupportedNode = "ORT_ETGLOW_FAIL_IF_UNSUPPORTED_NODE";
+constexpr const char* kDebugLogs = "ORT_ETGLOW_DEBUG_LOGS";
+constexpr const char* kBundleCacheEnable = "ORT_ETGLOW_BUNDLE_CACHE_ENABLE";
+constexpr const char* kCapabilityMode = "ORT_ETGLOW_CAPABILITY_MODE";
+
+namespace api_params {
+constexpr const char* kDeviceType = "ORT_ETGLOW_DEVICE_TYPE";
+}
+}  // namespace envvars
+}  // namespace etglow
+
+template <typename Key, typename... OptionValueTypes>
+class OptionsContainer {
+ public:
+  using options_types = std::variant<OptionValueTypes...>;
+  using options_storage = typename std::unordered_map<Key, options_types>;
+
+  using size_type = typename std::unordered_map<Key, options_types>::size_type;
+
+ private:
+  options_storage options_;
+
+ public:
+  typename options_storage::iterator begin() { return options_.begin(); }
+  typename options_storage::const_iterator begin() const { return options_.begin(); }
+  typename options_storage::const_iterator cbegin() const { return options_.cbegin(); }
+  typename options_storage::iterator end() { return options_.end(); }
+  typename options_storage::const_iterator end() const { return options_.end(); }
+  typename options_storage::const_iterator cend() const { return options_.cend(); }
+
+  [[nodiscard]] bool empty() const noexcept { return options_.empty(); }
+  typename options_storage::size_type size() const noexcept { return options_.size(); }
+
+  template <class... Args>
+  std::pair<typename options_storage::iterator, bool> try_emplace(const typename options_storage::key_type& k, Args&&... args) {
+    return options_.try_emplace(k, args...);
+  }
+  typename options_storage::iterator find(const typename options_storage::key_type& key) { return options_.find(key); }
+  typename options_storage::const_iterator find(const typename options_storage::key_type& key) const { return options_.find(key); }
+
+  template <typename T>
+  std::optional<T> get_safe(std::string_view name) const {
+    if (auto it = options_.find(std::string{name}); it != options_.end()) {
+      if (std::holds_alternative<T>(it->second)) {
+        return std::get<T>(it->second);
+      }
+      return std::nullopt;
+    }
+    return std::nullopt;
+  }
+
+  template <typename ToStringVisitor>
+  std::string string(ToStringVisitor&& v) const {
+    std::string result;
+    for (const auto& [key, value] : options_) {
+      if (!result.empty()) {
+        result += ";";
+      }
+      result += key;
+      result += "=";
+      result += std::visit(std::forward<ToStringVisitor>(v), value);
+    }
+    return result;
+  }
+};
+
+using ONNXSymbolsShapes = OptionsContainer<std::string, std::size_t>;
+using GlowAPIOptions = OptionsContainer<std::string, std::string, std::vector<std::string>>;
+
+ONNXSymbolsShapes ParseONNXSymbolsFromUserString(const char* s);
+GlowAPIOptions ParseGlowAPIOptionsFromUserString(const char* s);
+struct TraceOptions {
+  // Knobs to control what components are traced
+  bool trace_glow_enable;
+  bool trace_neuralizer_enable;
+  bool trace_runtime_enable;
+  bool trace_device_kernel_nodes_enable;
+  bool trace_device_kernel_inst_enable;
+
+  TraceOptions(int et_traces_flags = OrtEtGlowTraceFlags::ETGLOW_TRACE_DEFAULT) {
+    trace_glow_enable = et_traces_flags & ETGLOW_TRACE_GLOW;
+    trace_neuralizer_enable = et_traces_flags & ETGLOW_TRACE_NEURALIZER;
+    trace_runtime_enable = et_traces_flags & ETGLOW_TRACE_RUNTIME;
+    trace_device_kernel_nodes_enable = et_traces_flags & ETGLOW_TRACE_DEVICE_KERNEL_NODES;
+    trace_device_kernel_inst_enable = et_traces_flags & ETGLOW_TRACE_DEVICE_KERNEL_INST;
+  }
+
+  int as_int() const {
+    int flags = ETGLOW_TRACE_NONE;  // Initialize with no flags
+    if (trace_glow_enable) {
+      flags |= ETGLOW_TRACE_GLOW;
+    }
+    if (trace_neuralizer_enable) {
+      flags |= ETGLOW_TRACE_NEURALIZER;
+    }
+    if (trace_runtime_enable) {
+      flags |= ETGLOW_TRACE_RUNTIME;
+    }
+    if (trace_device_kernel_nodes_enable) {
+      flags |= ETGLOW_TRACE_DEVICE_KERNEL_NODES;
+    }
+    if (trace_device_kernel_inst_enable) {
+      flags |= ETGLOW_TRACE_DEVICE_KERNEL_INST;
+    }
+    return flags;
+  }
+};
+
+// Information needed to construct ETGLOW execution provider
+struct EtGlowExecutionProviderInfo {
+  OrtDevice::DeviceId device_id{0};
+  size_t device_mem_limit{std::numeric_limits<size_t>::max()};  // Will be over-ridden by max device memory
+  ArenaExtendStrategy arena_extend_strategy{ArenaExtendStrategy::kSameAsRequested};
+  bool compile_only = false;
+  bool dump_subgraphs = false;
+  bool greedy = false;
+  bool offline_mode = false;
+  bool fail_if_cannot_run_whole_graph = false;
+  ONNXSymbolsShapes onnx_symbols;
+  GlowAPIOptions api_params;
+  fs::path bundle_cache_prefix;
+  fs::path export_bundle_path;
+  fs::path traces_dir_prefix;
+  TraceOptions trace_options;
+
+  static EtGlowExecutionProviderInfo FromProviderOptions(const onnxruntime::ProviderOptions& options);
+  static onnxruntime::ProviderOptions ToProviderOptions(const EtGlowExecutionProviderInfo& info);
+  static onnxruntime::ProviderOptions ToProviderOptions(const OrtEtGlowProviderOptions& info);
+  static void UpdateProviderOptions(void* provider_options, const ProviderOptions& options);
+};
+
+}  // end namespace onnxruntime
diff --git a/onnxruntime/core/providers/etglow/etglow_execution_provider_utils.h b/onnxruntime/core/providers/etglow/etglow_execution_provider_utils.h
new file mode 100644
index 0000000000..b1c6f2c6eb
--- /dev/null
+++ b/onnxruntime/core/providers/etglow/etglow_execution_provider_utils.h
@@ -0,0 +1,224 @@
+// Copyright (c) Microsoft Corporation. All rights reserved.
+// Licensed under the MIT License.
+
+#pragma once
+
+#include "core/providers/shared_library/provider_api.h"
+#include "etglow_execution_provider.h"
+
+#include <algorithm>
+
+namespace onnxruntime {
+
+// remove invalid characters
+inline void legalizeName(std::string& name) {
+  auto invalid_characters = {'#', '%', '&', '{', '}', '<', '>'};
+  for (auto c : invalid_characters) {
+    std::replace(name.begin(), name.end(), c, '-');
+  }
+}
+
+inline std::string GenerateMetaDefName(const GraphViewer& graph_viewer, const ModelMetadefIdGenerator& metadef_id_generator,
+                                       std::optional<uint64_t> opt_extra_prefix = std::nullopt,
+                                       std::optional<int> opt_extra_postfix = std::nullopt,
+                                       HashValue model_hash = 0) {
+  std::string name;
+  if (opt_extra_prefix.has_value()) {
+    std::stringstream ss;
+    constexpr int k_prefix_width = 16;
+    ss << std::setfill('0') << std::setw(k_prefix_width) << opt_extra_prefix.value() << "_";
+    name += ss.str();
+  }
+  const std::string graph_type = graph_viewer.IsSubgraph() ? "subgraph" : "graph";
+  const int metadef_id = metadef_id_generator.GenerateId(graph_viewer, model_hash);
+  auto subgraph_name = graph_viewer.Name();
+  std::replace(subgraph_name.begin(), subgraph_name.end(), ' ', '_');
+  auto subgraph_id = std::to_string(model_hash) + "_" + std::to_string(metadef_id);
+  if (opt_extra_postfix.has_value()) {
+    subgraph_id += "_" + std::to_string(opt_extra_postfix.value());
+  }
+  name += "GLOW_" + graph_type + "_" + subgraph_name + "_" + subgraph_id;
+  legalizeName(name);
+  return name;
+}
+
+inline void RegisterApiParamOrEnvVar(const GlowAPIOptions& api_params, const std::string& param, const std::string& envvar, GlowAPI::StringMap<std::string>& etsoc_param) {
+  if (const std::string envvar_value = onnxruntime::GetEnvironmentVar(envvar); !envvar_value.empty()) {
+    etsoc_param.try_emplace(param, envvar_value);
+  } else if (auto it = api_params.find(param); it != api_params.end()) {
+    etsoc_param.try_emplace(it->first, std::get<std::string>(it->second));
+  }
+}
+
+inline void RegisterApiParam(const GlowAPIOptions& api_params, const std::string& param, GlowAPI::StringMap<std::string>& etsoc_param) {
+  if (auto it = api_params.find(param); it != api_params.end()) {
+    etsoc_param.try_emplace(it->first, std::get<std::string>(it->second));
+  }
+}
+
+inline GlowAPI::StringMap<std::string> GetGlowGlobalParams(const GlowAPIOptions& api_params) {
+  GlowAPI::StringMap<std::string> etsoc_params{};
+  RegisterApiParamOrEnvVar(api_params, etglow::api_params::kDeviceType, etglow::envvars::api_params::kDeviceType, etsoc_params);
+  RegisterApiParam(api_params, etglow::api_params::kRunDir, etsoc_params);
+  RegisterApiParam(api_params, etglow::api_params::kActiveShires, etsoc_params);
+  RegisterApiParam(api_params, etglow::api_params::kKernelLaunchTimeout, etsoc_params);
+  RegisterApiParam(api_params, etglow::api_params::kGlowThreads, etsoc_params);
+  RegisterApiParam(api_params, etglow::api_params::kTraceRuntimePath, etsoc_params);
+  RegisterApiParam(api_params, etglow::api_params::kTraceNeuralizerPath, etsoc_params);
+  RegisterApiParam(api_params, etglow::api_params::kTraceNeuralizerNodes, etsoc_params);
+  RegisterApiParam(api_params, etglow::api_params::kTraceDescriptorsPath, etsoc_params);
+  RegisterApiParam(api_params, etglow::api_params::kEnableCoreDump, etsoc_params);
+  if (auto it = api_params.find(etglow::api_params::kExtraEtsocParams); it != api_params.end()) {
+    const auto& extra_etsoc_params = std::get<std::vector<std::string>>(it->second);
+    for (const auto& extra_etsoc_param : extra_etsoc_params) {
+      LOGS_DEFAULT(WARNING) << "[ETGLOW EP] extra_etsoc_param: " << extra_etsoc_param;
+
+      auto sep_pos = extra_etsoc_param.find('=');
+      if (sep_pos == std::string::npos || sep_pos == 0 || sep_pos == extra_etsoc_param.length()) {
+        ORT_THROW("[ERROR] [EtGlow] Provided extra_etsoc_param doesn't have key=value form!");
+      }
+      auto extra_etsoc_param_key = extra_etsoc_param.substr(0, sep_pos);
+      auto extra_etsoc_param_value = extra_etsoc_param.substr(sep_pos + 1);
+      LOGS_DEFAULT(WARNING) << "[ETGLOW EP] Registering extra-etsoc-param: key: " << extra_etsoc_param_key << " value: " << extra_etsoc_param_value;
+      etsoc_params.try_emplace(extra_etsoc_param_key, extra_etsoc_param_value);
+    }
+  }
+  return etsoc_params;
+}
+
+inline GlowAPI::HostConfig GetGlowHostConfig(const GlowAPIOptions& api_params) {
+  GlowAPI::HostConfig hc{};
+  if (auto it = api_params.find(etglow::api_params::host_config::kMaxActiveRequests); it != api_params.end()) {
+    hc.maxActiveRequests = std::stoul(std::get<std::string>(it->second));
+  }
+  if (auto it = api_params.find(etglow::api_params::host_config::kMaxQueueSize); it != api_params.end()) {
+    hc.maxQueueSize = std::stoul(std::get<std::string>(it->second));
+  }
+  if (auto it = api_params.find(etglow::api_params::host_config::kExecutorThreads); it != api_params.end()) {
+    hc.executorThreads = std::stoul(std::get<std::string>(it->second));
+  }
+  return hc;
+}
+
+inline GlowAPI::LoadOptions GetGlowLoadOptions(const GlowAPIOptions& api_params, const std::map<std::string, std::size_t, std::less<>>& input_dim_params, const GraphViewer& graph_viewer, const ModelMetadefIdGenerator& metadef_id_generator) {
+  GlowAPI::LoadOptions lo;
+  if (auto it = api_params.find(etglow::api_params::load_options::kUsefp16); it != api_params.end()) {
+    const auto& param = std::get<std::string>(it->second);
+    lo.useFP16 = (param == "1") || (param == "true") || (param == "True");
+  }
+  if (auto it = api_params.find(etglow::api_params::load_options::kUseInt32Indices); it != api_params.end()) {
+    const auto& param = std::get<std::string>(it->second);
+    lo.useInt32Indices = (param == "1") || (param == "true") || (param == "True");
+  }
+  if (auto it = api_params.find(etglow::api_params::load_options::kOnnxZip); it != api_params.end()) {
+    const auto& param = std::get<std::string>(it->second);
+    lo.onnxZip = (param == "1") || (param == "true") || (param == "True");
+  }
+  for (auto&& [key, value] : input_dim_params) {
+    lo.symbolTable.insert({key, std::to_string(value)});
+  }
+  // Tell glow to use unique function name from ORT to avoid issues with GlowAPI unique name generation
+  // include a timestampt to get different CompiledNetwork folders for identical executions
+  const auto ts = std::chrono::steady_clock::now();
+  lo.modelName = GenerateMetaDefName(graph_viewer, metadef_id_generator, ts.time_since_epoch().count());
+  return lo;
+}
+
+inline std::vector<std::string> GetOnDevicePlaceholderNamesFromUserParams(const GlowAPIOptions& api_params) {
+  std::vector<std::string> result;
+  if (auto it = api_params.find(etglow::api_params::before_compile_options::kDevicePlaceholder); it != api_params.end()) {
+    LOGS_DEFAULT(INFO) << "[ETGLOW EP] define device placeholders (from user provider list): ";
+    // If the user provides a custom list of placeholders, use that (even if it might be wrong).
+    // It is user responsability to provide a correct list.
+    result = std::get<std::vector<std::string>>(it->second);
+  }
+  return result;
+}
+
+inline std::vector<std::string> GetOnDevicePlaceholderNamesFromGlowAPI(const GlowAPI::Model& glow_loaded_model) {
+  std::vector<std::string> result;
+  auto [in_dev_placeholder_names, out_dev_placeholder_names] = GlowAPI::getPlaceholders(glow_loaded_model);
+  result.reserve(in_dev_placeholder_names.size() + out_dev_placeholder_names.size());
+  result.insert(
+      result.end(),
+      std::make_move_iterator(in_dev_placeholder_names.begin()),
+      std::make_move_iterator(in_dev_placeholder_names.end()));
+  result.insert(
+      result.end(),
+      std::make_move_iterator(out_dev_placeholder_names.begin()),
+      std::make_move_iterator(out_dev_placeholder_names.end()));
+  return result;
+}
+
+inline std::vector<std::string> GetOnDeviceImplicitPlaceholdrNamesFromUserParams(const GlowAPIOptions& api_params) {
+  std::vector<std::string> result;
+  if (auto it = api_params.find(etglow::api_params::before_compile_options::kImplicitDevicePlaceholder); it != api_params.end()) {
+    LOGS_DEFAULT(INFO) << "[ETGLOW EP] define implicit device placeholders (from user provided list):";
+    result = std::get<std::vector<std::string>>(it->second);
+  }
+  return result;
+}
+
+inline GlowAPI::ExecutionOptions GetGlowExecutionOptions(const GlowAPIOptions& api_params) {
+  GlowAPI::ExecutionOptions eo;
+  if (auto it = api_params.find(etglow::api_params::exec_options::kPriority); it != api_params.end()) {
+    eo.priority = std::stoul(std::get<std::string>(it->second));
+  }
+  return eo;
+}
+
+inline std::string glowApiErrMsg(etsoc::GlowAPI::Error&& error, std::string_view msg) {
+  auto error_value = glow::detail::takeErrorValue(std::move(error));
+  auto error_msg = (error_value != nullptr) ? error_value->message() : "";
+  return "[ETGLOW EP] Glow API error: {" + error_msg + "} " + std::string(msg);
+}
+
+inline bool HasExternalInitializer(const InitializedTensorSet& initializers) {
+  auto initializersIt = std::find_if(initializers.begin(), initializers.end(), [](const auto& v) {
+    const auto* tensor = v.second;
+    return (tensor->has_data_location() &&
+            tensor->data_location() == ONNX_NAMESPACE::TensorProto_DataLocation_EXTERNAL);
+  });
+  return initializersIt != initializers.end();
+}
+
+inline std::filesystem::path GetModelPath(const GraphViewer& graph_body_viewer,
+                                          const std::string& fused_node_name) {
+  auto onnx_parent_path = std::filesystem::current_path();  // cwd
+  if (HasExternalInitializer(graph_body_viewer.GetAllInitializedTensors())) {
+    LOGS_DEFAULT(INFO) << "[ETGLOW EP] Model initializers contain external data. Dumping intermediate onnx into "
+                          "original model directory to not break GlowAPI filesystem assumptions.";
+    // search orignal model parent path
+    const auto& model_path = graph_body_viewer.ModelPath();
+    if (model_path.empty()) {
+      LOGS_DEFAULT(WARNING) << "[ETGLOW EP] Model path is empty.";
+    }
+    onnx_parent_path = model_path.parent_path();
+  }
+  return onnx_parent_path / std::filesystem::path{fused_node_name + ".onnx"};
+}
+
+inline std::filesystem::path GetBundlePath(fs::path bundle_cache_prefix,
+                                           size_t bundle_abi,
+                                           const std::string& fused_node_name) {
+  if (bundle_cache_prefix.empty()) {
+    bundle_cache_prefix = fs::current_path();
+  }
+  auto bundle_dir = bundle_cache_prefix / "onnxruntime_bundles";
+  auto bundle_path = bundle_dir / fused_node_name;
+  bundle_path += "_" + std::to_string(bundle_abi);
+  bundle_path += ".bundle";
+  return bundle_path;
+}
+
+inline std::string to_string(const ort_etglow::OnnxTensorInfo& tensor_info) {
+  std::stringstream shape_sstream;
+  std::string separator = " x ";
+  std::copy(tensor_info.shape.begin(), tensor_info.shape.end(), std::ostream_iterator<int64_t>(shape_sstream, separator.c_str()));
+  auto tmp = shape_sstream.str();
+  std::string dtype = std::to_string(tensor_info.data_type);
+  std::string shape = "<" + tmp.substr(0, tmp.size() - separator.size()) + ">";
+  return "dtype: " + dtype + " shape: " + shape;
+}
+
+}  // end namespace onnxruntime
\ No newline at end of file
diff --git a/onnxruntime/core/providers/etglow/etglow_kernels.inc b/onnxruntime/core/providers/etglow/etglow_kernels.inc
new file mode 100644
index 0000000000..701665672b
--- /dev/null
+++ b/onnxruntime/core/providers/etglow/etglow_kernels.inc
@@ -0,0 +1,201 @@
+#ifndef ETGLOW_KERNEL_DEF
+#error The macro ETGLOW_KERNEL_DEF was not declared.
+#endif
+///////////////// OPERATOR                             	  	   DOMAIN         START_VERSION           END_VERSION                                                                                                                                                                                                        SUPPORTED TYPES
+ETGLOW_KERNEL_DEF(Abs,                                    kOnnxDomain,                    1,                   13,                                                                                                                                                                               .TypeConstraint("T", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(Acos,                                   kOnnxDomain,                    1,                   22,                                                                                                                                                                               .TypeConstraint("T", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(Acosh,                                  kOnnxDomain,                    1,                   22,                                                                                                                                                                               .TypeConstraint("T", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(Add,                                    kOnnxDomain,                    1,                   14,                                                                                                                                                                               .TypeConstraint("T", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(AffineGrid,                             kOnnxDomain,                    1,                   20,                                                                                                                         .TypeConstraint("T1", DataTypeImpl::AllTensorTypes()).TypeConstraint("T2", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(And,                                    kOnnxDomain,                    1,                    7,                                                                                                                                                                               .TypeConstraint("T", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(ArgMax,                                 kOnnxDomain,                    1,                   13,                                                                                                                                                                               .TypeConstraint("T", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(ArgMin,                                 kOnnxDomain,                    1,                   13,                                                                                                                                                                               .TypeConstraint("T", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(Asin,                                   kOnnxDomain,                    1,                   22,                                                                                                                                                                               .TypeConstraint("T", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(Asinh,                                  kOnnxDomain,                    1,                   22,                                                                                                                                                                               .TypeConstraint("T", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(Atan,                                   kOnnxDomain,                    1,                   22,                                                                                                                                                                               .TypeConstraint("T", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(Atanh,                                  kOnnxDomain,                    1,                   22,                                                                                                                                                                               .TypeConstraint("T", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(AveragePool,                            kOnnxDomain,                    1,                   22,                                                                                                                                                                               .TypeConstraint("T", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(BatchNormalization,                     kOnnxDomain,                    1,                   15,                                                                     .TypeConstraint("T", DataTypeImpl::AllTensorTypes()).TypeConstraint("T1", DataTypeImpl::AllTensorTypes()).TypeConstraint("T2", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(Bernoulli,                              kOnnxDomain,                    1,                   22,                                                                                                                                                                              .TypeConstraint("T1", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(BitShift,                               kOnnxDomain,                    1,                   11,                                                                                                                                                                               .TypeConstraint("T", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(BitwiseAnd,                             kOnnxDomain,                    1,                   18,                                                                                                                                                                               .TypeConstraint("T", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(BitwiseNot,                             kOnnxDomain,                    1,                   18,                                                                                                                                                                               .TypeConstraint("T", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(BitwiseOr,                              kOnnxDomain,                    1,                   18,                                                                                                                                                                               .TypeConstraint("T", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(BitwiseXor,                             kOnnxDomain,                    1,                   18,                                                                                                                                                                               .TypeConstraint("T", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(BlackmanWindow,                         kOnnxDomain,                    1,                   17,                                                                                                                                                                              .TypeConstraint("T1", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(Cast,                                   kOnnxDomain,                    1,                   21,                                                                                                                                                                              .TypeConstraint("T1", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(CastLike,                               kOnnxDomain,                    1,                   21,                                                                                                                         .TypeConstraint("T1", DataTypeImpl::AllTensorTypes()).TypeConstraint("T2", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(Ceil,                                   kOnnxDomain,                    1,                   13,                                                                                                                                                                               .TypeConstraint("T", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(Celu,                                   kOnnxDomain,                    1,                   12,                                                                                                                                                                               .TypeConstraint("T", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(CenterCropPad,                          kOnnxDomain,                    1,                   18,                                                                                                                        .TypeConstraint("T", DataTypeImpl::AllTensorTypes()).TypeConstraint("Tind", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(Clip,                                   kOnnxDomain,                    1,                   13,                                                                                                                                                                               .TypeConstraint("T", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(Col2Im,                                 kOnnxDomain,                    1,                   18,                                                                                                                                                                               .TypeConstraint("T", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(Compress,                               kOnnxDomain,                    1,                   11,                                                                                                                          .TypeConstraint("T", DataTypeImpl::AllTensorTypes()).TypeConstraint("T1", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(Concat,                                 kOnnxDomain,                    1,                   13,                                                                                                                                                                               .TypeConstraint("T", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(ConcatFromSequence,                     kOnnxDomain,                    1,                   11,                                                                                                                                                                               .TypeConstraint("S", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(Constant,                               kOnnxDomain,                    1,                   21,                                                                                                                                                                                                                                   )
+ETGLOW_KERNEL_DEF(ConstantOfShape,                        kOnnxDomain,                    1,                   21,                                                                                                                                                                              .TypeConstraint("T1", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(Conv,                                   kOnnxDomain,                    1,                   22,                                                                                                                                                                               .TypeConstraint("T", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(ConvInteger,                            kOnnxDomain,                    1,                   10,                                                                                                                         .TypeConstraint("T1", DataTypeImpl::AllTensorTypes()).TypeConstraint("T2", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(ConvTranspose,                          kOnnxDomain,                    1,                   22,                                                                                                                                                                               .TypeConstraint("T", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(Cos,                                    kOnnxDomain,                    1,                   22,                                                                                                                                                                               .TypeConstraint("T", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(Cosh,                                   kOnnxDomain,                    1,                   22,                                                                                                                                                                               .TypeConstraint("T", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(CumSum,                                 kOnnxDomain,                    1,                   14,                                                                                                                          .TypeConstraint("T", DataTypeImpl::AllTensorTypes()).TypeConstraint("T2", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(DFT,                                    kOnnxDomain,                    1,                   20,                                                                                                                         .TypeConstraint("T1", DataTypeImpl::AllTensorTypes()).TypeConstraint("T2", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(DeformConv,                             kOnnxDomain,                    1,                   22,                                                                                                                                                                               .TypeConstraint("T", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(DepthToSpace,                           kOnnxDomain,                    1,                   13,                                                                                                                                                                               .TypeConstraint("T", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(DequantizeLinear,                       kOnnxDomain,                    1,                   21,                                                                                                                         .TypeConstraint("T1", DataTypeImpl::AllTensorTypes()).TypeConstraint("T2", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(Det,                                    kOnnxDomain,                    1,                   22,                                                                                                                                                                               .TypeConstraint("T", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(Div,                                    kOnnxDomain,                    1,                   14,                                                                                                                                                                               .TypeConstraint("T", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(Dropout,                                kOnnxDomain,                    1,                   22,                                                                     .TypeConstraint("T", DataTypeImpl::AllTensorTypes()).TypeConstraint("T1", DataTypeImpl::AllTensorTypes()).TypeConstraint("T2", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(DynamicQuantizeLinear,                  kOnnxDomain,                    1,                   11,                                                                                                                                                                              .TypeConstraint("T1", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(Einsum,                                 kOnnxDomain,                    1,                   12,                                                                                                                                                                               .TypeConstraint("T", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(Elu,                                    kOnnxDomain,                    1,                   22,                                                                                                                                                                               .TypeConstraint("T", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(Equal,                                  kOnnxDomain,                    1,                   19,                                                                                                                                                                               .TypeConstraint("T", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(Erf,                                    kOnnxDomain,                    1,                   13,                                                                                                                                                                               .TypeConstraint("T", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(Exp,                                    kOnnxDomain,                    1,                   13,                                                                                                                                                                               .TypeConstraint("T", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(Expand,                                 kOnnxDomain,                    1,                   13,                                                                                                                                                                               .TypeConstraint("T", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(EyeLike,                                kOnnxDomain,                    1,                   22,                                                                                                                                                                              .TypeConstraint("T1", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(Flatten,                                kOnnxDomain,                    1,                   21,                                                                                                                                                                               .TypeConstraint("T", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(Floor,                                  kOnnxDomain,                    1,                   13,                                                                                                                                                                               .TypeConstraint("T", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(GRU,                                    kOnnxDomain,                    1,                   22,                                                                                                                          .TypeConstraint("T", DataTypeImpl::AllTensorTypes()).TypeConstraint("T1", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(Gather,                                 kOnnxDomain,                    1,                   13,                                                                                                                        .TypeConstraint("T", DataTypeImpl::AllTensorTypes()).TypeConstraint("Tind", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(GatherElements,                         kOnnxDomain,                    1,                   13,                                                                                                                        .TypeConstraint("T", DataTypeImpl::AllTensorTypes()).TypeConstraint("Tind", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(GatherND,                               kOnnxDomain,                    1,                   13,                                                                                                                                                                               .TypeConstraint("T", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(Gelu,                                   kOnnxDomain,                    1,                   20,                                                                                                                                                                               .TypeConstraint("T", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(Gemm,                                   kOnnxDomain,                    1,                   13,                                                                                                                                                                               .TypeConstraint("T", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(GlobalAveragePool,                      kOnnxDomain,                    1,                   22,                                                                                                                                                                               .TypeConstraint("T", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(GlobalLpPool,                           kOnnxDomain,                    1,                   22,                                                                                                                                                                               .TypeConstraint("T", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(GlobalMaxPool,                          kOnnxDomain,                    1,                   22,                                                                                                                                                                               .TypeConstraint("T", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(Greater,                                kOnnxDomain,                    1,                   13,                                                                                                                                                                               .TypeConstraint("T", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(GreaterOrEqual,                         kOnnxDomain,                    1,                   16,                                                                                                                                                                               .TypeConstraint("T", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(GridSample,                             kOnnxDomain,                    1,                   22,                                                                                                                         .TypeConstraint("T1", DataTypeImpl::AllTensorTypes()).TypeConstraint("T2", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(GroupNormalization,                     kOnnxDomain,                    1,                   21,                                                                                                                                                                               .TypeConstraint("T", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(HammingWindow,                          kOnnxDomain,                    1,                   17,                                                                                                                                                                              .TypeConstraint("T1", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(HannWindow,                             kOnnxDomain,                    1,                   17,                                                                                                                                                                              .TypeConstraint("T1", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(HardSigmoid,                            kOnnxDomain,                    1,                   22,                                                                                                                                                                               .TypeConstraint("T", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(HardSwish,                              kOnnxDomain,                    1,                   22,                                                                                                                                                                               .TypeConstraint("T", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(Hardmax,                                kOnnxDomain,                    1,                   13,                                                                                                                                                                               .TypeConstraint("T", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(Identity,                               kOnnxDomain,                    1,                   21,                                                                                                                                                                               .TypeConstraint("V", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(If,                                     kOnnxDomain,                    1,                   21,                                                                                                                                                                               .TypeConstraint("B", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(ImageDecoder,                           kOnnxDomain,                    1,                   20,                                                                                                                                                                              .TypeConstraint("T1", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(InstanceNormalization,                  kOnnxDomain,                    1,                   22,                                                                                                                                                                               .TypeConstraint("T", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(IsInf,                                  kOnnxDomain,                    1,                   20,                                                                                                                                                                              .TypeConstraint("T1", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(IsNaN,                                  kOnnxDomain,                    1,                   20,                                                                                                                                                                              .TypeConstraint("T1", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(LRN,                                    kOnnxDomain,                    1,                   13,                                                                                                                                                                               .TypeConstraint("T", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(LSTM,                                   kOnnxDomain,                    1,                   22,                                                                                                                          .TypeConstraint("T", DataTypeImpl::AllTensorTypes()).TypeConstraint("T1", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(LayerNormalization,                     kOnnxDomain,                    1,                   17,                                                                                                                                                                               .TypeConstraint("T", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(LeakyRelu,                              kOnnxDomain,                    1,                   16,                                                                                                                                                                               .TypeConstraint("T", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(Less,                                   kOnnxDomain,                    1,                   13,                                                                                                                                                                               .TypeConstraint("T", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(LessOrEqual,                            kOnnxDomain,                    1,                   16,                                                                                                                                                                               .TypeConstraint("T", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(Log,                                    kOnnxDomain,                    1,                   13,                                                                                                                                                                               .TypeConstraint("T", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(LogSoftmax,                             kOnnxDomain,                    1,                   13,                                                                                                                                                                               .TypeConstraint("T", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(Loop,                                   kOnnxDomain,                    1,                   21,                                                                       .TypeConstraint("I", DataTypeImpl::AllTensorTypes()).TypeConstraint("B", DataTypeImpl::AllTensorTypes()).TypeConstraint("V", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(LpNormalization,                        kOnnxDomain,                    1,                   22,                                                                                                                                                                               .TypeConstraint("T", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(LpPool,                                 kOnnxDomain,                    1,                   22,                                                                                                                                                                               .TypeConstraint("T", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(MatMul,                                 kOnnxDomain,                    1,                   13,                                                                                                                                                                               .TypeConstraint("T", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(MatMulInteger,                          kOnnxDomain,                    1,                   10,                                                                                                                         .TypeConstraint("T1", DataTypeImpl::AllTensorTypes()).TypeConstraint("T2", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(Max,                                    kOnnxDomain,                    1,                   13,                                                                                                                                                                               .TypeConstraint("T", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(MaxPool,                                kOnnxDomain,                    1,                   22,                                                                                                                                                                               .TypeConstraint("T", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(MaxRoiPool,                             kOnnxDomain,                    1,                   22,                                                                                                                                                                               .TypeConstraint("T", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(MaxUnpool,                              kOnnxDomain,                    1,                   22,                                                                                                                         .TypeConstraint("T1", DataTypeImpl::AllTensorTypes()).TypeConstraint("T2", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(Mean,                                   kOnnxDomain,                    1,                   13,                                                                                                                                                                               .TypeConstraint("T", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(MeanVarianceNormalization,              kOnnxDomain,                    1,                   13,                                                                                                                                                                               .TypeConstraint("T", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(MelWeightMatrix,                        kOnnxDomain,                    1,                   17,                                                                                                                         .TypeConstraint("T1", DataTypeImpl::AllTensorTypes()).TypeConstraint("T2", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(Min,                                    kOnnxDomain,                    1,                   13,                                                                                                                                                                               .TypeConstraint("T", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(Mish,                                   kOnnxDomain,                    1,                   22,                                                                                                                                                                               .TypeConstraint("T", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(Mod,                                    kOnnxDomain,                    1,                   13,                                                                                                                                                                               .TypeConstraint("T", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(Mul,                                    kOnnxDomain,                    1,                   14,                                                                                                                                                                               .TypeConstraint("T", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(Multinomial,                            kOnnxDomain,                    1,                   22,                                                                                                                                                                              .TypeConstraint("T1", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(Neg,                                    kOnnxDomain,                    1,                   13,                                                                                                                                                                               .TypeConstraint("T", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(NegativeLogLikelihoodLoss,              kOnnxDomain,                    1,                   22,                                                                                                                        .TypeConstraint("T", DataTypeImpl::AllTensorTypes()).TypeConstraint("Tind", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(NonMaxSuppression,                      kOnnxDomain,                    1,                   11,                                                                                                                                                                                                                                   )
+ETGLOW_KERNEL_DEF(NonZero,                                kOnnxDomain,                    1,                   13,                                                                                                                                                                               .TypeConstraint("T", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(Not,                                    kOnnxDomain,                    1,                    1,                                                                                                                                                                               .TypeConstraint("T", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(OneHot,                                 kOnnxDomain,                    1,                   11,                                                                    .TypeConstraint("T1", DataTypeImpl::AllTensorTypes()).TypeConstraint("T2", DataTypeImpl::AllTensorTypes()).TypeConstraint("T3", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(Optional,                               kOnnxDomain,                    1,                   15,                                                                                                                                                                               .TypeConstraint("V", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(OptionalGetElement,                     kOnnxDomain,                    1,                   18,                                                                                                                                                                               .TypeConstraint("O", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(OptionalHasElement,                     kOnnxDomain,                    1,                   18,                                                                                                                                                                               .TypeConstraint("O", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(Or,                                     kOnnxDomain,                    1,                    7,                                                                                                                                                                               .TypeConstraint("T", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(PRelu,                                  kOnnxDomain,                    1,                   16,                                                                                                                                                                               .TypeConstraint("T", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(Pad,                                    kOnnxDomain,                    1,                   21,                                                                                                                        .TypeConstraint("T", DataTypeImpl::AllTensorTypes()).TypeConstraint("Tind", DataTypeImpl::AllTensorTypes()))
+//ETGLOW_KERNEL_DEF(Pow,                                    kOnnxDomain,                    1,                   11,                                                                                                                                                                               .TypeConstraint("T", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(Pow,                                    kOnnxDomain,                   12,                   15,                                                                                                                          .TypeConstraint("T", DataTypeImpl::AllTensorTypes()).TypeConstraint("T1", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(QLinearConv,                            kOnnxDomain,                    1,                   10,               .TypeConstraint("T1", DataTypeImpl::AllTensorTypes()).TypeConstraint("T2", DataTypeImpl::AllTensorTypes()).TypeConstraint("T3", DataTypeImpl::AllTensorTypes()).TypeConstraint("T4", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(QLinearMatMul,                          kOnnxDomain,                    1,                   21,               .TypeConstraint("T1", DataTypeImpl::AllTensorTypes()).TypeConstraint("TS", DataTypeImpl::AllTensorTypes()).TypeConstraint("T2", DataTypeImpl::AllTensorTypes()).TypeConstraint("T3", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(QuantizeLinear,                         kOnnxDomain,                    1,                   21,                                                                                                                         .TypeConstraint("T1", DataTypeImpl::AllTensorTypes()).TypeConstraint("T2", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(RNN,                                    kOnnxDomain,                    1,                   22,                                                                                                                          .TypeConstraint("T", DataTypeImpl::AllTensorTypes()).TypeConstraint("T1", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(RandomNormal,                           kOnnxDomain,                    1,                   22,                                                                                                                                                                                                                                   )
+ETGLOW_KERNEL_DEF(RandomNormalLike,                       kOnnxDomain,                    1,                   22,                                                                                                                                                                              .TypeConstraint("T1", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(RandomUniform,                          kOnnxDomain,                    1,                   22,                                                                                                                                                                                                                                   )
+ETGLOW_KERNEL_DEF(RandomUniformLike,                      kOnnxDomain,                    1,                   22,                                                                                                                                                                              .TypeConstraint("T1", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(Range,                                  kOnnxDomain,                    1,                   11,                                                                                                                                                                               .TypeConstraint("T", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(Reciprocal,                             kOnnxDomain,                    1,                   13,                                                                                                                                                                               .TypeConstraint("T", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(ReduceL1,                               kOnnxDomain,                    1,                   18,                                                                                                                                                                               .TypeConstraint("T", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(ReduceL2,                               kOnnxDomain,                    1,                   18,                                                                                                                                                                               .TypeConstraint("T", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(ReduceLogSum,                           kOnnxDomain,                    1,                   18,                                                                                                                                                                               .TypeConstraint("T", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(ReduceLogSumExp,                        kOnnxDomain,                    1,                   18,                                                                                                                                                                               .TypeConstraint("T", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(ReduceMax,                              kOnnxDomain,                    1,                   20,                                                                                                                                                                               .TypeConstraint("T", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(ReduceMean,                             kOnnxDomain,                    1,                   18,                                                                                                                                                                               .TypeConstraint("T", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(ReduceMin,                              kOnnxDomain,                    1,                   20,                                                                                                                                                                               .TypeConstraint("T", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(ReduceProd,                             kOnnxDomain,                    1,                   18,                                                                                                                                                                               .TypeConstraint("T", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(ReduceSum,                              kOnnxDomain,                    1,                   13,                                                                                                                                                                               .TypeConstraint("T", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(ReduceSumSquare,                        kOnnxDomain,                    1,                   18,                                                                                                                                                                               .TypeConstraint("T", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(RegexFullMatch,                         kOnnxDomain,                    1,                   20,                                                                                                                                                                              .TypeConstraint("T1", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(Relu,                                   kOnnxDomain,                    1,                   14,                                                                                                                                                                               .TypeConstraint("T", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(Reshape,                                kOnnxDomain,                    1,                   21,                                                                                                                                                                               .TypeConstraint("T", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(Resize,                                 kOnnxDomain,                    1,                   19,                                                                                                                         .TypeConstraint("T1", DataTypeImpl::AllTensorTypes()).TypeConstraint("T2", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(ReverseSequence,                        kOnnxDomain,                    1,                   10,                                                                                                                                                                               .TypeConstraint("T", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(RoiAlign,                               kOnnxDomain,                    1,                   22,                                                                                                                         .TypeConstraint("T1", DataTypeImpl::AllTensorTypes()).TypeConstraint("T2", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(Round,                                  kOnnxDomain,                    1,                   22,                                                                                                                                                                               .TypeConstraint("T", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(STFT,                                   kOnnxDomain,                    1,                   17,                                                                                                                         .TypeConstraint("T1", DataTypeImpl::AllTensorTypes()).TypeConstraint("T2", DataTypeImpl::AllTensorTypes()))
+//ETGLOW_KERNEL_DEF(Scan,                                   kOnnxDomain,                    1,                   21,                                                                                                                                                                               .TypeConstraint("V", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(Scatter,                                kOnnxDomain,                    1,                   11,                                                                                                                        .TypeConstraint("T", DataTypeImpl::AllTensorTypes()).TypeConstraint("Tind", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(ScatterElements,                        kOnnxDomain,                    1,                   18,                                                                                                                        .TypeConstraint("T", DataTypeImpl::AllTensorTypes()).TypeConstraint("Tind", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(ScatterND,                              kOnnxDomain,                    1,                   18,                                                                                                                                                                               .TypeConstraint("T", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(Selu,                                   kOnnxDomain,                    1,                   22,                                                                                                                                                                               .TypeConstraint("T", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(SequenceAt,                             kOnnxDomain,                    1,                   11,                                                                                                                           .TypeConstraint("S", DataTypeImpl::AllTensorTypes()).TypeConstraint("I", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(SequenceConstruct,                      kOnnxDomain,                    1,                   11,                                                                                                                                                                               .TypeConstraint("T", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(SequenceEmpty,                          kOnnxDomain,                    1,                   11,                                                                                                                                                                                                                                   )
+ETGLOW_KERNEL_DEF(SequenceErase,                          kOnnxDomain,                    1,                   11,                                                                                                                           .TypeConstraint("S", DataTypeImpl::AllTensorTypes()).TypeConstraint("I", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(SequenceInsert,                         kOnnxDomain,                    1,                   11,                                                                       .TypeConstraint("S", DataTypeImpl::AllTensorTypes()).TypeConstraint("T", DataTypeImpl::AllTensorTypes()).TypeConstraint("I", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(SequenceLength,                         kOnnxDomain,                    1,                   11,                                                                                                                                                                               .TypeConstraint("S", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(SequenceMap,                            kOnnxDomain,                    1,                   17,                                                                                                                           .TypeConstraint("S", DataTypeImpl::AllTensorTypes()).TypeConstraint("V", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(Shape,                                  kOnnxDomain,                    1,                   21,                                                                                                                                                                               .TypeConstraint("T", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(Shrink,                                 kOnnxDomain,                    1,                    9,                                                                                                                                                                               .TypeConstraint("T", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(Sigmoid,                                kOnnxDomain,                    1,                   13,                                                                                                                                                                               .TypeConstraint("T", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(Sign,                                   kOnnxDomain,                    1,                   13,                                                                                                                                                                               .TypeConstraint("T", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(SimplifiedLayerNormalization,           kOnnxDomain,                    1,                    1,                                                                                                                           .TypeConstraint("T", DataTypeImpl::AllTensorTypes()).TypeConstraint("V", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(Sin,                                    kOnnxDomain,                    1,                   22,                                                                                                                                                                               .TypeConstraint("T", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(Sinh,                                   kOnnxDomain,                    1,                   22,                                                                                                                                                                               .TypeConstraint("T", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(Size,                                   kOnnxDomain,                    1,                   21,                                                                                                                                                                               .TypeConstraint("T", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(Slice,                                  kOnnxDomain,                    1,                   13,                                                                                                                        .TypeConstraint("T", DataTypeImpl::AllTensorTypes()).TypeConstraint("Tind", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(Softmax,                                kOnnxDomain,                    1,                   13,                                                                                                                                                                               .TypeConstraint("T", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(SoftmaxCrossEntropyLoss,                kOnnxDomain,                    1,                   13,                                                                                                                        .TypeConstraint("T", DataTypeImpl::AllTensorTypes()).TypeConstraint("Tind", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(Softplus,                               kOnnxDomain,                    1,                   22,                                                                                                                                                                               .TypeConstraint("T", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(Softsign,                               kOnnxDomain,                    1,                   22,                                                                                                                                                                               .TypeConstraint("T", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(SpaceToDepth,                           kOnnxDomain,                    1,                   13,                                                                                                                                                                               .TypeConstraint("T", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(Split,                                  kOnnxDomain,                    1,                   18,                                                                                                                                                                               .TypeConstraint("T", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(SplitToSequence,                        kOnnxDomain,                    1,                   11,                                                                                                                           .TypeConstraint("T", DataTypeImpl::AllTensorTypes()).TypeConstraint("I", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(Sqrt,                                   kOnnxDomain,                    1,                   13,                                                                                                                                                                               .TypeConstraint("T", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(Squeeze,                                kOnnxDomain,                    1,                   21,                                                                                                                                                                               .TypeConstraint("T", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(StringConcat,                           kOnnxDomain,                    1,                   20,                                                                                                                                                                               .TypeConstraint("T", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(StringNormalizer,                       kOnnxDomain,                    1,                   10,                                                                                                                                                                                                                                   )
+ETGLOW_KERNEL_DEF(StringSplit,                            kOnnxDomain,                    1,                   20,                                                                                                                                                                              .TypeConstraint("T1", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(Sub,                                    kOnnxDomain,                    1,                   14,                                                                                                                                                                               .TypeConstraint("T", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(Sum,                                    kOnnxDomain,                    1,                   13,                                                                                                                                                                               .TypeConstraint("T", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(Tan,                                    kOnnxDomain,                    1,                   22,                                                                                                                                                                               .TypeConstraint("T", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(Tanh,                                   kOnnxDomain,                    1,                   13,                                                                                                                                                                               .TypeConstraint("T", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(TfIdfVectorizer,                        kOnnxDomain,                    1,                    9,                                                                                                                                                                               .TypeConstraint("T", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(ThresholdedRelu,                        kOnnxDomain,                    1,                   22,                                                                                                                                                                               .TypeConstraint("T", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(Tile,                                   kOnnxDomain,                    1,                   13,                                                                                                                          .TypeConstraint("T", DataTypeImpl::AllTensorTypes()).TypeConstraint("T1", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(TopK,                                   kOnnxDomain,                    1,                   11,                                                                                                                                                                               .TypeConstraint("T", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(Transpose,                              kOnnxDomain,                    1,                   21,                                                                                                                                                                               .TypeConstraint("T", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(Trilu,                                  kOnnxDomain,                    1,                   14,                                                                                                                                                                               .TypeConstraint("T", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(Unique,                                 kOnnxDomain,                    1,                   11,                                                                                                                                                                               .TypeConstraint("T", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(Unsqueeze,                              kOnnxDomain,                    1,                   21,                                                                                                                                                                               .TypeConstraint("T", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(Upsample,                               kOnnxDomain,                    1,                   10,                                                                                                                                                                               .TypeConstraint("T", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(Where,                                  kOnnxDomain,                    1,                   16,                                                                                                                           .TypeConstraint("B", DataTypeImpl::AllTensorTypes()).TypeConstraint("T", DataTypeImpl::AllTensorTypes()))
+ETGLOW_KERNEL_DEF(Xor,                                    kOnnxDomain,                    1,                    7,                                                                                                                                                                               .TypeConstraint("T", DataTypeImpl::AllTensorTypes()))
+///////////////// Contrib supported kernels
+ETGLOW_KERNEL_DEF(MatMulNBits,                              kMSDomain,                    1,                    1,               .TypeConstraint("T1", DataTypeImpl::AllTensorTypes()).TypeConstraint("T2", DataTypeImpl::AllTensorTypes()).TypeConstraint("T3", DataTypeImpl::AllTensorTypes()).TypeConstraint("T4", DataTypeImpl::AllTensorTypes()))
diff --git a/onnxruntime/core/providers/etglow/etglow_loaded_models_registry.h b/onnxruntime/core/providers/etglow/etglow_loaded_models_registry.h
new file mode 100644
index 0000000000..ec3c5c5a6a
--- /dev/null
+++ b/onnxruntime/core/providers/etglow/etglow_loaded_models_registry.h
@@ -0,0 +1,82 @@
+// Copyright (c) Microsoft Corporation. All rights reserved.
+// Licensed under the MIT License.
+
+#pragma once
+
+#include <glow/GlowAPI/etsoc.h>
+
+#include <unordered_map>
+#include <memory>
+#include <string>
+
+#include "core/providers/shared_library/provider_api.h"
+
+namespace onnxruntime {
+
+using GlowAPI = etsoc::GlowAPI;
+
+class EtGlowLoadedModelsRegistry {
+ public:
+  Status LoadModel(GlowAPI& etglow_api, const std::string& bundle_path, const GlowAPI::CompiledModel& compiled_model, std::unordered_set<std::string>& callee_loaded_models, GlowAPI::RegisteredModel*& loaded_model) {
+    std::scoped_lock lock(mutex_);
+    if (IsLoaded(loaded_models_, bundle_path)) {
+      const bool should_increment_refcount = !IsLoaded(callee_loaded_models, bundle_path);
+      loaded_model = GetLoadedModel(bundle_path, should_increment_refcount);
+      callee_loaded_models.insert(bundle_path);
+      return Status::OK();
+    }
+    LOGS_DEFAULT(INFO) << "[ETGLOW EP] registering model to the device.";
+    auto expected_glow_reg_model = etglow_api.registerModel(compiled_model);
+    if (!expected_glow_reg_model) {
+      return ORT_MAKE_STATUS(ONNXRUNTIME, FAIL, "uploading code to the device");
+    }
+    auto glow_reg_model = std::move(*expected_glow_reg_model);
+    const auto& [insertion_it, insertion_took_place] = loaded_models_.try_emplace(bundle_path, std::make_unique<GlowAPI::RegisteredModel>(std::move(glow_reg_model)));
+    ORT_RETURN_IF_NOT(insertion_took_place, "[ETGLOW] failed to insert GlowAPI::RegisteredModel into loaded_models_");
+    loaded_model = insertion_it->second.registered_model_ptr.get();
+    callee_loaded_models.insert(bundle_path);
+    return Status::OK();
+  }
+
+  Status UnloadModel(GlowAPI& etglow_api, const std::string& bundle_path) {
+    std::scoped_lock lock(mutex_);
+    auto it = loaded_models_.find(bundle_path);
+    ORT_RETURN_IF_NOT(it != loaded_models_.end(), "Trying to unload a non-loaded model");
+    it->second.ref_count--;
+    if (it->second.ref_count <= 0) {
+      LOGS_DEFAULT(INFO) << "[ETGLOW EP] unregistering model " << bundle_path;
+      auto unload_error = etglow_api.unregisterModel(std::move(*it->second.registered_model_ptr));
+      ORT_RETURN_IF(unload_error, "Error unloaded model from glow api");
+      LOGS_DEFAULT(INFO) << "[ETGLOW EP] unregistering model " << bundle_path << " done.";
+      loaded_models_.erase(it);
+    }
+    return Status::OK();
+  }
+
+ private:
+  template <typename T>
+  bool IsLoaded(const T& model_list, const std::string& bundle_path) const {
+    return model_list.find(bundle_path) != model_list.end();
+  }
+
+  GlowAPI::RegisteredModel* GetLoadedModel(const std::string& bundle_path, bool increment_refcount) {
+    auto it = loaded_models_.find(bundle_path);
+    if (increment_refcount) {
+      it->second.ref_count++;
+    }
+    return it->second.registered_model_ptr.get();
+  }
+
+ private:
+  std::mutex mutex_;
+
+  struct LoadedModel {
+    explicit LoadedModel(std::unique_ptr<GlowAPI::RegisteredModel> registered_model_ptr) : registered_model_ptr{std::move(registered_model_ptr)}, ref_count{1} {}
+
+    std::unique_ptr<GlowAPI::RegisteredModel> registered_model_ptr{nullptr};
+    std::atomic<int> ref_count{0};
+  };
+  std::unordered_map<std::string, LoadedModel> loaded_models_;
+};
+
+}  // end namespace onnxruntime
diff --git a/onnxruntime/core/providers/etglow/etglow_node_capability.cc b/onnxruntime/core/providers/etglow/etglow_node_capability.cc
new file mode 100644
index 0000000000..d4ac5b19bd
--- /dev/null
+++ b/onnxruntime/core/providers/etglow/etglow_node_capability.cc
@@ -0,0 +1,2773 @@
+// Copyright (c) Microsoft Corporation. All rights reserved.
+// Licensed under the MIT License.
+
+#include "etglow_node_capability.h"
+
+#include <cmath>
+
+namespace std {
+
+std::string to_string(onnxruntime::ORT_DataType t) {
+  std::string result;
+#define TO_STRING(type) \
+  case type: {          \
+    result = #type;     \
+    break;              \
+  }
+
+  using ORT_DataType = onnxruntime::ORT_DataType;
+  switch (t) {
+    TO_STRING(ORT_DataType::type_undefined)
+    TO_STRING(ORT_DataType::type_float32)
+    TO_STRING(ORT_DataType::type_uint8)
+    TO_STRING(ORT_DataType::type_int8)
+    TO_STRING(ORT_DataType::type_uint16)
+    TO_STRING(ORT_DataType::type_int16)
+    TO_STRING(ORT_DataType::type_int32)
+    TO_STRING(ORT_DataType::type_int64)
+    TO_STRING(ORT_DataType::type_string)
+    TO_STRING(ORT_DataType::type_bool)
+    TO_STRING(ORT_DataType::type_float16)
+    TO_STRING(ORT_DataType::type_double)
+    TO_STRING(ORT_DataType::type_uint32)
+    TO_STRING(ORT_DataType::type_uint64)
+    TO_STRING(ORT_DataType::type_complex64)
+    TO_STRING(ORT_DataType::type_complex128)
+    TO_STRING(ORT_DataType::type_bfloat16)
+    TO_STRING(ORT_DataType::type_float8e3m3fn)
+    TO_STRING(ORT_DataType::type_float8e4m3fnuz)
+    TO_STRING(ORT_DataType::type_float8e5m2)
+    TO_STRING(ORT_DataType::type_float8e5m2fnuz)
+    TO_STRING(ORT_DataType::type_uint4)
+    TO_STRING(ORT_DataType::type_int4)
+    default:
+      result = "Unknown type";
+      break;
+  }
+  return result;
+}
+
+std::string to_string(const std::vector<onnxruntime::ORT_DataType>& t) {
+  std::string result;
+  bool first = true;
+  for (auto elem : t) {
+    if (first) {
+      first = false;
+    } else {
+      result += " ";
+    }
+    result += std::to_string(elem);
+  }
+  return result;
+}
+
+std::string to_string(onnxruntime::CapabilityMode mode) {
+  std::string result;
+#define TO_STRING(type) \
+  case type: {          \
+    result = #type;     \
+    break;              \
+  }
+
+  using CapabilityMode = onnxruntime::CapabilityMode;
+  switch (mode) {
+    TO_STRING(CapabilityMode::DEFAULT)
+    TO_STRING(CapabilityMode::STRICT)
+    default:
+      result = "Unknown type";
+      break;
+  }
+  return result;
+}
+
+onnxruntime::CapabilityMode from_string(const std::string& str) {
+  using CapabilityMode = onnxruntime::CapabilityMode;
+  CapabilityMode result = CapabilityMode::DEFAULT;
+  std::string tmp = str;
+  std::transform(tmp.begin(), tmp.end(), tmp.begin(), [](unsigned char c) { return std::tolower(c); });
+  if (tmp == "strict") {
+    result = CapabilityMode::STRICT;
+  }
+  return result;
+}
+
+}  // end namespace std
+
+namespace onnxruntime {
+
+////////////////////
+
+using TensorShapePtr = ONNX_NAMESPACE::TensorShapeProto const*;
+
+bool HasShape(const TensorShapePtr node_shape) {
+  return node_shape != nullptr;
+}
+bool HasShape(const NodeArg& node_arg) {
+  return HasShape(node_arg.Shape());
+}
+bool HasShape(const NodeArg* node_arg) {
+  if (node_arg == nullptr) {
+    return false;
+  }
+  return HasShape(node_arg->Shape());
+}
+
+bool SkipShapeChecks(CapabilityMode mode) {
+  return mode != CapabilityMode::STRICT;
+}
+
+bool SkipTypeChecks(CapabilityMode mode) {
+  return mode != CapabilityMode::STRICT;
+}
+
+bool HasEmptyShape(const TensorShapePtr node_shape) {
+  return node_shape->dim_size() == 0;
+}
+
+bool IsScalar(const NodeArg& node_arg) {
+  const auto* node_shape = node_arg.Shape();
+  if (node_shape == nullptr) {
+    // shape is nullptr, we cannot really tell if it's a scalar or not
+    // assuming it is not
+    return false;
+  }
+  if (HasEmptyShape(node_shape)) {
+    return true;
+  }
+  for (int j = 0; j < node_shape->dim_size(); ++j) {
+    if (node_shape->dim(j).dim_value() != 1) {
+      return false;
+    }
+  }
+  return true;
+}
+bool IsScalar(const NodeArg* node_arg) {
+  if (node_arg == nullptr) return false;
+  return IsScalar(*node_arg);
+}
+
+EtGlowNodeSupportResult make_supported() { return EtGlowNodeSupportResult{supported_t{}}; }
+EtGlowNodeSupportResult make_unsupported(std::string&& reason) { return EtGlowNodeSupportResult{std::move(reason)}; }
+
+////////////////////
+////////////////////
+
+#define ORT_RETURN_IF_NO_SHAPE(node_arg, mode, ...)                                                        \
+  do {                                                                                                     \
+    if (!HasShape(node_arg->Shape())) {                                                                    \
+      if (SkipShapeChecks(mode)) {                                                                         \
+        return make_supported();                                                                           \
+      }                                                                                                    \
+      return make_unsupported(node_arg->Name() + " cannot verify shape (nullptr). Assume not supported."); \
+    }                                                                                                      \
+  } while (false)
+
+#define ORT_CONTINUE_IF_NO_SHAPE(node_arg, mode, op_type, ...)                                                                  \
+  if (!HasShape(node_arg->Shape())) {                                                                                           \
+    if (SkipShapeChecks(mode)) {                                                                                                \
+      LOGS_DEFAULT(INFO) << "(" << op_type << ") Missing shape for tensor: " << input->Name() << " skip shape related checks."; \
+      continue;                                                                                                                 \
+    }                                                                                                                           \
+    return make_unsupported(node_arg->Name() + " cannot verify shape (nullptr). Assume not supported.");                        \
+  }                                                                                                                             \
+  static_assert(true, "force semicolon after macro")
+
+EtGlowNodeSupportResult HasValueForParametricDim(const onnx::TensorShapeProto_Dimension& dim, const ONNXSymbolsShapes& user_parametric_shapes_replacements, const std::string& name) {
+  if (dim.has_dim_param() && user_parametric_shapes_replacements.find(dim.dim_param()) == user_parametric_shapes_replacements.end()) {
+    // has parametric value & no user replacement --> not supported
+    return make_unsupported(name + " has parametric shape: " + dim.dim_param() + " but replacement value has not been provided. Not supported.");
+  }
+  return make_supported();
+}
+
+EtGlowNodeSupportResult HasNonZeroPositiveValueDims(const NodeArg* node_arg, const ONNXSymbolsShapes& user_parametric_shapes_replacements, CapabilityMode mode) {
+  for (const auto& dim : node_arg->Shape()->dim()) {
+    ORT_RETURN_IF_NOT_SUPPORTED(HasValueForParametricDim(dim, user_parametric_shapes_replacements, node_arg->Name()));
+
+    if (!dim.has_dim_param() && !dim.has_dim_value()) {
+      ORT_UNUSED_PARAMETER(mode);
+      // Not skipping shape checks here even if in 'CapabilityMode:Default' mode to avoid lowering ONNX Backend tests passrate
+      return make_unsupported(node_arg->Name() + " does not have dim_param nor dim_value. Cannot validate shape dims.");
+    }
+    auto dim_value = dim.has_dim_value() ? dim.dim_value() : user_parametric_shapes_replacements.get_safe<std::size_t>(dim.dim_param()).value_or(0);
+    if (dim_value < 1) {
+      return make_unsupported(node_arg->Name() + " dimensions can only be greater or equal to 1, but found " + std::to_string(dim_value));
+    }
+  }
+  return make_supported();
+};
+
+EtGlowNodeSupportResult HasNonZeroPositiveValueDimsSafe(const NodeArg* node_arg, const ONNXSymbolsShapes& user_parametric_shapes_replacements, CapabilityMode mode) {
+  ORT_RETURN_IF_NO_SHAPE(node_arg, mode);
+  return HasNonZeroPositiveValueDims(node_arg, user_parametric_shapes_replacements, mode);
+};
+
+EtGlowNodeSupportResult HasSameDims(const NodeArg& lhs, const NodeArg& rhs, const ONNXSymbolsShapes& user_parametric_shapes_replacements) {
+  const auto lhs_dim_size = lhs.Shape()->dim_size();
+  if (const auto rhs_dim_size = rhs.Shape()->dim_size(); lhs_dim_size != rhs_dim_size) {
+    return make_unsupported(lhs.Name() + " and " + rhs.Name() +
+                            " shapes do not match. "
+                            "(" +
+                            std::to_string(lhs_dim_size) + "!=" + std::to_string(rhs_dim_size) + ")");
+  }
+  for (int i = 0; i < lhs_dim_size; ++i) {
+    const auto& lhs_dim = lhs.Shape()->dim(i);
+    const auto& rhs_dim = rhs.Shape()->dim(i);
+
+    auto lhs_dim_value = lhs_dim.has_dim_value() ? lhs_dim.dim_value() : user_parametric_shapes_replacements.get_safe<std::size_t>(lhs_dim.dim_param()).value_or(0);
+    auto rhs_dim_value = rhs_dim.has_dim_value() ? rhs_dim.dim_value() : user_parametric_shapes_replacements.get_safe<std::size_t>(rhs_dim.dim_param()).value_or(0);
+    if (lhs_dim_value != rhs_dim_value) {
+      return make_unsupported("dimension " + std::to_string(i) + " of" + lhs.Name() + " and " + rhs.Name() +
+                              " do not match. "
+                              "(" +
+                              std::to_string(lhs_dim_value) + "!=" + std::to_string(rhs_dim_value) + ")");
+    }
+  }
+  return make_supported();
+};
+
+EtGlowNodeSupportResult IsScalarConstant(const NodeArg& input, const std::string& input_name_alias, const GraphViewer& graph_viewer) {
+  if (!IsScalar(input)) {
+    return make_unsupported(input.Name() + " (" + input_name_alias + ") must be a scalar");
+  }
+  if (!graph_viewer.IsConstantInitializer(input.Name(), true)) {
+    return make_unsupported(input.Name() + " (" + input_name_alias + ") must be a constant");
+  }
+  return make_supported();
+};
+
+EtGlowNodeSupportResult IsScalarConstantSafe(const NodeArg* input, const std::string& input_name_alias, const GraphViewer& graph_viewer, CapabilityMode mode) {
+  if (input == nullptr) {
+    return make_unsupported("found NodeArg* nullptr");
+  }
+  ORT_RETURN_IF_NO_SHAPE(input, mode);
+  return IsScalarConstant(*input, input_name_alias, graph_viewer);
+};
+
+////////////////////
+////////////////////
+
+EtGlowNotSupportedNodeCapability::EtGlowNotSupportedNodeCapability(std::string user_provided_message)
+    : unsupported_message_{std::move(user_provided_message)} {};
+
+EtGlowNodeSupportResult EtGlowNotSupportedNodeCapability::Supported(const Node& node, const GraphViewer& graph_viewer) const {
+  ORT_UNUSED_PARAMETER(node);
+  ORT_UNUSED_PARAMETER(graph_viewer);
+  std::string message = "this node is marked as unconditionally not supported";
+  if (unsupported_message_.has_value()) {
+    message += " (" + unsupported_message_.value() + ")";
+  }
+  return make_unsupported(std::string{message});
+}
+
+////////////////////
+
+EtGlowDefaultNodeCapability::EtGlowDefaultNodeCapability(CapabilityMode mode)
+    : mode_{mode} {
+          // intentionally empty
+      };
+
+EtGlowDefaultNodeCapability::EtGlowDefaultNodeCapability(CapabilityMode mode, const ONNXSymbolsShapes& user_parametric_shapes_replacements)
+    : mode_{mode}, user_parametric_shapes_replacements_{user_parametric_shapes_replacements} {
+  // intentionally empty
+}
+
+EtGlowDefaultNodeCapability::EtGlowDefaultNodeCapability(CapabilityMode mode, const ONNXSymbolsShapes& user_parametric_shapes_replacements,
+                                                         std::vector<ORT_DataType> inputTypes)
+    : mode_{mode}, user_parametric_shapes_replacements_{user_parametric_shapes_replacements}, inputTypes_{std::move(inputTypes)} {
+  // intentionally empty
+}
+
+EtGlowDefaultNodeCapability::EtGlowDefaultNodeCapability(CapabilityMode mode, const ONNXSymbolsShapes& user_parametric_shapes_replacements,
+                                                         std::vector<ORT_DataType> inputTypes,
+                                                         std::vector<ORT_DataType> outputTypes)
+    : mode_{mode}, user_parametric_shapes_replacements_{user_parametric_shapes_replacements}, inputTypes_{std::move(inputTypes)}, outputTypes_{std::move(outputTypes)} {
+  // intentionally empty
+}
+
+EtGlowNodeSupportResult EtGlowDefaultNodeCapability::Supported(const Node& node, const GraphViewer& graph_viewer) const {
+  ORT_RETURN_IF_NOT_SUPPORTED(IsTypeSupported(node));
+  ORT_RETURN_IF_NOT_SUPPORTED(IsDimensionSupported(node, graph_viewer));
+  ORT_RETURN_IF_NOT_SUPPORTED(IsAttributeSupported(node, graph_viewer));
+  return make_supported();
+}
+
+EtGlowNodeSupportResult EtGlowDefaultNodeCapability::IsTypeSupported(const Node& node) const {
+  if (auto node_inputs = node.InputDefs(); !node_inputs.empty() && node_inputs[0]->TypeAsProto() != nullptr) {
+    const auto* input_0 = node_inputs[0];
+    auto node_datatype = static_cast<ORT_DataType>(input_0->TypeAsProto()->tensor_type().elem_type());
+    // node_input[0] type supported if inputTypes contains same type
+    if (bool is_supported_in_type = std::find(inputTypes_.begin(), inputTypes_.end(), node_datatype) != inputTypes_.end(); !is_supported_in_type) {
+      return make_unsupported("input[0] (" + input_0->Name() + ") is of type: " + std::to_string(node_datatype) + " which is not in the list of supported types.");
+    }
+  }
+  if (!outputTypes_.empty()) {
+    auto node_outputs = node.OutputDefs();
+    if (!node_outputs.empty() && node_outputs[0]->TypeAsProto() != nullptr) {
+      const auto* output_0 = node_outputs[0];
+      auto node_datatype = static_cast<ORT_DataType>(output_0->TypeAsProto()->tensor_type().elem_type());
+      // node_outputs[0] type supported if outputTypes contains same type
+      if (bool is_supported_out_type = std::find(outputTypes_.begin(), outputTypes_.end(), node_datatype) != outputTypes_.end(); !is_supported_out_type) {
+        return make_unsupported("output[0] (" + output_0->Name() + ") is of type: " + std::to_string(node_datatype) + " which is not in the list of supported types.");
+      }
+    }
+  }
+  return make_supported();
+}
+
+EtGlowNodeSupportResult EtGlowDefaultNodeCapability::IsDimensionSupported(const Node& node, const GraphViewer& graph_viewer) const {
+  ORT_UNUSED_PARAMETER(graph_viewer);
+  auto node_inputs = node.InputDefs();
+  for (const auto* input : node_inputs) {
+    ORT_CONTINUE_IF_NO_SHAPE(input, GetCapabilityMode(), node.OpType());
+
+    const auto& shape = *input->Shape();
+    if (const int input_dim_size = shape.dim_size(); input_dim_size > max_tensor_dimensions_) {
+      return make_unsupported("input: " + input->Name() + "has " + std::to_string(input_dim_size) + " dims. Glow only supports tensors with max 6 dims.");
+    }
+    for (const auto& dim : shape.dim()) {
+      if (!(dim.has_dim_param() || dim.has_dim_value())) {
+        if (!SkipShapeChecks(GetCapabilityMode())) {
+          return make_unsupported("input: " + input->Name() + " shape is neither dim_param or dim_value. Not supported");
+        }
+        LOGS_DEFAULT(INFO) << "(" << node.OpType() << ") shape of tensor: " << input->Name() << " is neither dim_param or dim_value. skip shape related checks.";
+        continue;
+      }
+      ORT_RETURN_IF_NOT_SUPPORTED(HasValueForParametricDim(dim, GetParametricShapeReplacements(), "input: " + input->Name()));
+    }
+  }
+  return make_supported();
+}
+
+EtGlowNodeSupportResult EtGlowDefaultNodeCapability::IsAttributeSupported(const Node& node, const GraphViewer& graph_viewer) const {
+  ORT_UNUSED_PARAMETER(node);
+  ORT_UNUSED_PARAMETER(graph_viewer);
+  return make_supported();
+}
+
+////////////////////
+
+EtGlowPoolNodeCapability::EtGlowPoolNodeCapability(CapabilityMode mode, const ONNXSymbolsShapes& user_parametric_shapes_replacements)
+    : EtGlowDefaultNodeCapability(mode, user_parametric_shapes_replacements, {type_float32, type_int8, type_float16}) {
+  // intentionally empty
+}
+
+EtGlowNodeSupportResult EtGlowPoolNodeCapability::Supported(const Node& node, const GraphViewer& graph_viewer) const {
+  ORT_RETURN_IF_NOT_SUPPORTED(IsTypeSupported(node));
+  ORT_RETURN_IF_NOT_SUPPORTED(IsDimensionSupported(node, graph_viewer));
+  ORT_RETURN_IF_NOT_SUPPORTED(IsAttributeSupported(node, graph_viewer));
+  return make_supported();
+}
+
+EtGlowNodeSupportResult EtGlowPoolNodeCapability::IsDimensionSupported(const Node& node, const GraphViewer& graph_viewer) const {
+  ORT_RETURN_IF_NOT_SUPPORTED(EtGlowDefaultNodeCapability::IsDimensionSupported(node, graph_viewer));
+
+  auto node_inputs = node.InputDefs();
+  if (!HasShape(node_inputs[0]->Shape())) {
+    return !SkipShapeChecks(GetCapabilityMode()) ? make_unsupported("shape is nullptr. Not supported.")
+                                                 : make_supported();
+  }
+
+  // check shapes
+  // only 2D pooling is supported
+  if (const int input_dim_size = node_inputs[0]->Shape()->dim_size(); input_dim_size != 4) {
+    return make_unsupported("input dim_size: " + std::to_string(input_dim_size) + " is not 4. Only 2D pooling is supported.");
+  }
+
+  return make_supported();
+}
+
+EtGlowNodeSupportResult EtGlowPoolNodeCapability::IsAttributeSupported(const Node& node, const GraphViewer& graph_viewer) const {
+  ORT_UNUSED_PARAMETER(graph_viewer);
+  const NodeAttributes& attributes = node.GetAttributes();
+  if (auto attr = attributes.find("kernel_shape"); attr != attributes.end() && attr->second().ints_size() != 2) {
+    return make_unsupported("kernel_shape attr shape is not 2D (3D is not supported).");
+  }
+  return make_supported();
+}
+
+////////////////////
+
+EtGlowAddNodeCapability::EtGlowAddNodeCapability(CapabilityMode mode, const ONNXSymbolsShapes& user_parametric_shapes_replacements)
+    : EtGlowDefaultNodeCapability(mode, user_parametric_shapes_replacements,
+                                  std::vector<ORT_DataType>{type_float32, type_float16, type_bfloat16, type_int64, type_int32, type_int8}) {
+  // intentionally empty
+}
+
+EtGlowNodeSupportResult EtGlowAddNodeCapability::Supported(const Node& node, const GraphViewer& graph_viewer) const {
+  ORT_RETURN_IF_NOT_SUPPORTED(IsTypeSupported(node));
+  ORT_RETURN_IF_NOT_SUPPORTED(IsDimensionSupported(node, graph_viewer));
+  return make_supported();
+}
+
+EtGlowNodeSupportResult EtGlowAddNodeCapability::IsDimensionSupported(const Node& node, const GraphViewer& graph_viewer) const {
+  ORT_RETURN_IF_NOT_SUPPORTED(EtGlowDefaultNodeCapability::IsDimensionSupported(node, graph_viewer));
+
+  const auto& node_inputs = node.InputDefs();
+  ORT_RETURN_IF_NOT_SUPPORTED(HasNonZeroPositiveValueDimsSafe(node_inputs[0], GetParametricShapeReplacements(), GetCapabilityMode()));
+  ORT_RETURN_IF_NOT_SUPPORTED(HasNonZeroPositiveValueDimsSafe(node_inputs[1], GetParametricShapeReplacements(), GetCapabilityMode()));
+
+  auto node_outputs = node.OutputDefs();
+  ORT_RETURN_IF_NOT_SUPPORTED(HasNonZeroPositiveValueDimsSafe(node_outputs[0], GetParametricShapeReplacements(), GetCapabilityMode()));
+
+  return make_supported();
+}
+
+////////////////////
+
+EtGlowNodeSupportResult EtGlowOneLayerNodeCapability::IsAttributeSupported(const Node& node, const GraphViewer& graph_viewer) const {
+  ORT_UNUSED_PARAMETER(graph_viewer);
+  const NodeAttributes& attributes = node.GetAttributes();
+
+  if (auto attr_activations = attributes.find("activations"); attr_activations != attributes.end()) {
+    if (attr_activations->second().type() != ONNX_NAMESPACE::AttributeProto_AttributeType::AttributeProto_AttributeType_STRINGS) {
+      return make_unsupported("attr activations is not of type STRINGS");
+    }
+    const auto attr_activations_size = attr_activations->second().strings_size();
+    if (!(attr_activations_size == GetNumSupportedActivationsSize() || attr_activations_size == 2 * GetNumSupportedActivationsSize())) {
+      return make_unsupported("invalid attr_activations_size: " + std::to_string(attr_activations_size));
+    }
+    std::unordered_set<std::string> supported_activation_function = {"relu"};
+    for (int i = 0; i < attr_activations_size; ++i) {
+      auto attr_activations_value = attr_activations->second().strings(i);
+      std::transform(attr_activations_value.begin(), attr_activations_value.end(), attr_activations_value.begin(), ::tolower);
+      if (supported_activation_function.find(attr_activations_value) == supported_activation_function.end()) {
+        return make_unsupported("attr activations[" + std::to_string(i) + "]: " + attr_activations_value + " is not one of the supported activation_functions");
+      }
+    }
+  }
+  return make_supported();
+}
+
+////////////////////
+////////////////////
+////////////////////
+
+EtGlowArgMaxNodeCapability::EtGlowArgMaxNodeCapability(CapabilityMode mode, const ONNXSymbolsShapes& user_parametric_shapes_replacements)
+    : EtGlowDefaultNodeCapability(mode, user_parametric_shapes_replacements,
+                                  {type_float32, type_int8, type_float16, type_int32, type_int64, type_int32}) {
+  // intentionally empty
+}
+
+EtGlowNodeSupportResult EtGlowArgMaxNodeCapability::Supported(const Node& node, const GraphViewer& graph_viewer) const {
+  ORT_RETURN_IF_NOT_SUPPORTED(IsTypeSupported(node));
+  ORT_RETURN_IF_NOT_SUPPORTED(IsDimensionSupported(node, graph_viewer));
+  ORT_RETURN_IF_NOT_SUPPORTED(IsAttributeSupported(node, graph_viewer));
+  return make_supported();
+}
+
+EtGlowNodeSupportResult EtGlowArgMaxNodeCapability::IsAttributeSupported(const Node& node, const GraphViewer& graph_viewer) const {
+  ORT_UNUSED_PARAMETER(graph_viewer);
+  const NodeAttributes& attributes = node.GetAttributes();
+  auto attr_select_last_index = attributes.find("select_last_index");
+  auto attr_axis = attributes.find("axis");
+  auto attr_keepdims = attributes.find("keepdims");
+  auto node_inputs = node.InputDefs();
+
+  if (attr_select_last_index != attributes.end() && attr_axis != attributes.end()) {
+    const bool is_select_last_index = attr_select_last_index->second().i() == 1;
+    const bool is_axis_negative_or_positive = (attr_axis->second().i() == 1 || attr_axis->second().i() == -1);
+    const bool is_non_random_select = (node_inputs[0]->Shape()->dim_size() < 3);
+    if (is_select_last_index && is_axis_negative_or_positive && is_non_random_select) {
+      return make_unsupported("condition is_select_last_index(" + std::to_string(static_cast<int>(is_non_random_select)) +
+                              ") && is_axis_negative_or_positive(" + std::to_string(static_cast<int>(is_non_random_select)) +
+                              ") && is_non_random_select(" + std::to_string(static_cast<int>(is_non_random_select)) + " failed.");
+    }
+  }
+  return make_supported();
+}
+
+////////////////////
+
+EtGlowNodeSupportResult EtGlowBatchNormalizationNodeCapability::Supported(const Node& node, const GraphViewer& graph_viewer) const {
+  ORT_RETURN_IF_NOT_SUPPORTED(IsTypeSupported(node));
+  ORT_RETURN_IF_NOT_SUPPORTED(IsDimensionSupported(node, graph_viewer));
+  ORT_RETURN_IF_NOT_SUPPORTED(IsAttributeSupported(node, graph_viewer));
+  return make_supported();
+}
+
+EtGlowNodeSupportResult EtGlowBatchNormalizationNodeCapability::IsDimensionSupported(const Node& node, const GraphViewer& graph_viewer) const {
+  ORT_RETURN_IF_NOT_SUPPORTED(EtGlowDefaultNodeCapability::IsDimensionSupported(node, graph_viewer));
+
+  const auto node_inputs = node.InputDefs();
+  constexpr int batch_norm_min_inputs = 5;
+  constexpr int batch_norm_max_outputs = 1;
+  if (node_inputs.size() < batch_norm_min_inputs) {
+    return make_unsupported("should have 5 inputs.");
+  }
+  if (auto node_outputs = node.OutputDefs(); node_outputs.size() > batch_norm_max_outputs) {
+    return make_unsupported("glow frontend doesn't support BatchNormalization optional outputs");
+  }
+  const auto* scale = node_inputs[1];
+  const auto* B = node_inputs[2];
+  const auto* mean = node_inputs[3];
+  const auto* var = node_inputs[4];
+  ORT_RETURN_IF_NO_SHAPE(scale, GetCapabilityMode());
+  ORT_RETURN_IF_NO_SHAPE(B, GetCapabilityMode());
+  ORT_RETURN_IF_NO_SHAPE(mean, GetCapabilityMode());
+  ORT_RETURN_IF_NO_SHAPE(var, GetCapabilityMode());
+  ORT_RETURN_IF_NOT_SUPPORTED(HasNonZeroPositiveValueDims(scale, GetParametricShapeReplacements(), GetCapabilityMode()));
+  ORT_RETURN_IF_NOT_SUPPORTED(HasNonZeroPositiveValueDims(B, GetParametricShapeReplacements(), GetCapabilityMode()));
+  ORT_RETURN_IF_NOT_SUPPORTED(HasNonZeroPositiveValueDims(mean, GetParametricShapeReplacements(), GetCapabilityMode()));
+  ORT_RETURN_IF_NOT_SUPPORTED(HasNonZeroPositiveValueDims(var, GetParametricShapeReplacements(), GetCapabilityMode()));
+  ORT_RETURN_IF_NOT_SUPPORTED(HasSameDims(*scale, *B, GetParametricShapeReplacements()));
+  ORT_RETURN_IF_NOT_SUPPORTED(HasSameDims(*scale, *mean, GetParametricShapeReplacements()));
+  ORT_RETURN_IF_NOT_SUPPORTED(HasSameDims(*scale, *var, GetParametricShapeReplacements()));
+
+  return make_supported();
+}
+
+EtGlowNodeSupportResult EtGlowBatchNormalizationNodeCapability::IsAttributeSupported(const Node& node, const GraphViewer& graph_viewer) const {
+  ORT_UNUSED_PARAMETER(graph_viewer);
+  const NodeAttributes& attributes = node.GetAttributes();
+  if (auto attr_spatial = attributes.find("spatial"); attr_spatial != attributes.end() && attr_spatial->second().i() != 1) {
+    return make_unsupported(node.OpType() + " attr spatial!=1 not supported");
+  }
+  return make_supported();
+}
+
+////////////////////
+
+EtGlowCastNodeCapability::EtGlowCastNodeCapability(CapabilityMode mode, const ONNXSymbolsShapes& user_parametric_shapes_replacements)
+    : EtGlowDefaultNodeCapability(mode, user_parametric_shapes_replacements,
+                                  std::vector<ORT_DataType>{type_bool, type_int32, type_int64, type_bfloat16, type_float16, type_float32},
+                                  std::vector<ORT_DataType>{type_bool, type_int32, type_int64, type_bfloat16, type_float16, type_float32}) {
+  // intentionally empty
+}
+
+EtGlowNodeSupportResult EtGlowCastNodeCapability::Supported(const Node& node, const GraphViewer& graph_viewer) const {
+  ORT_RETURN_IF_NOT_SUPPORTED(IsTypeSupported(node));
+  ORT_RETURN_IF_NOT_SUPPORTED(IsAttributeSupported(node, graph_viewer));
+  return make_supported();
+}
+
+EtGlowNodeSupportResult EtGlowCastNodeCapability::IsAttributeSupported(const Node& node, const GraphViewer& graph_viewer) const {
+  ORT_UNUSED_PARAMETER(graph_viewer);
+  const NodeAttributes& attributes = node.GetAttributes();
+  if (auto attr_to = attributes.find("to"); !(attr_to != attributes.end())) {
+    return make_unsupported("attr 'to' is missing");
+  }
+  if (auto attr_to = attributes.find("saturate"); attr_to != attributes.end()) {
+    return make_unsupported("attr 'saturate' is not supported by glow");
+  }
+  return make_supported();
+}
+
+////////////////////
+
+EtGlowClipNodeCapability::EtGlowClipNodeCapability(CapabilityMode mode, const ONNXSymbolsShapes& user_parametric_shapes_replacements)
+    : EtGlowDefaultNodeCapability(mode, user_parametric_shapes_replacements,
+                                  {type_float32, type_float16, type_int64, type_int32, type_int8}) {
+}
+
+EtGlowNodeSupportResult EtGlowClipNodeCapability::Supported(const Node& node, const GraphViewer& graph_viewer) const {
+  ORT_RETURN_IF_NOT_SUPPORTED(IsTypeSupported(node));
+  ORT_RETURN_IF_NOT_SUPPORTED(IsDimensionSupported(node, graph_viewer));
+  return make_supported();
+}
+
+EtGlowNodeSupportResult EtGlowClipNodeCapability::IsDimensionSupported(const Node& node, const GraphViewer& graph_viewer) const {
+  ORT_RETURN_IF_NOT_SUPPORTED(EtGlowDefaultNodeCapability::IsDimensionSupported(node, graph_viewer));
+
+  auto node_inputs = node.InputDefs();
+  const auto* input = node_inputs[0];
+  ORT_RETURN_IF_NO_SHAPE(input, GetCapabilityMode());
+  const auto* input_shape = input->Shape();
+  for (const auto& dim : input_shape->dim()) {
+    if (dim.has_dim_value() && dim.dim_value() == 0) {
+      return make_unsupported("Glow doesn't support Clip with shape dims with zeros");
+    }
+  }
+  for (const auto& node_input : node_inputs) {
+    if (const auto& node_input_name = node_input->Name(); node_input_name == "min" || node_input_name == "max") {
+      ORT_RETURN_IF_NOT_SUPPORTED(IsScalarConstantSafe(node_input, node_input_name, graph_viewer, GetCapabilityMode()));
+    }
+  }
+  return make_supported();
+}
+
+////////////////////
+
+EtGlowNodeSupportResult EtGlowConcatNodeCapability::Supported(const Node& node, const GraphViewer& graph_viewer) const {
+  ORT_RETURN_IF_NOT_SUPPORTED(IsTypeSupported(node));
+  ORT_RETURN_IF_NOT_SUPPORTED(IsDimensionSupported(node, graph_viewer));
+  ORT_RETURN_IF_NOT_SUPPORTED(IsAttributeSupported(node, graph_viewer));
+  return make_supported();
+}
+
+EtGlowNodeSupportResult EtGlowConcatNodeCapability::IsDimensionSupported(const Node& node, const GraphViewer& graph_viewer) const {
+  ORT_RETURN_IF_NOT_SUPPORTED(EtGlowDefaultNodeCapability::IsDimensionSupported(node, graph_viewer));
+
+  auto check_dimensions = [this](const auto& tensor) {
+    ORT_RETURN_IF_NO_SHAPE(tensor, GetCapabilityMode());
+    const auto input_shape = tensor->Shape();
+    for (const auto& dim : input_shape->dim()) {
+      if (dim.has_dim_value() && dim.dim_value() == 0) {
+        return make_unsupported("Glow doesn't support " + tensor->Name() + " with shape dims containing zeros");
+      }
+    }
+    return make_supported();
+  };
+  for (const auto& node_input : node.InputDefs()) {
+    ORT_RETURN_IF_NOT_SUPPORTED(check_dimensions(node_input));
+  }
+
+  return make_supported();
+}
+
+////////////////////
+
+EtGlowNodeSupportResult EtGlowConstantNodeCapability::Supported(const Node& node, const GraphViewer& graph_viewer) const {
+  ORT_RETURN_IF_NOT_SUPPORTED(IsTypeSupported(node));
+  ORT_RETURN_IF_NOT_SUPPORTED(IsDimensionSupported(node, graph_viewer));
+  ORT_RETURN_IF_NOT_SUPPORTED(IsAttributeSupported(node, graph_viewer));
+  return make_supported();
+}
+
+EtGlowNodeSupportResult EtGlowConstantNodeCapability::IsAttributeSupported(const Node& node, const GraphViewer& graph_viewer) const {
+  ORT_UNUSED_PARAMETER(graph_viewer);
+  const NodeAttributes& attributes = node.GetAttributes();
+  auto attr_value = attributes.find("value");
+  if (!(attr_value != attributes.end())) {
+    return make_unsupported("attr 'value' is missing");
+  }
+  // must have type TENSOR or INTS
+  if (!(attr_value->second().type() == ONNX_NAMESPACE::AttributeProto_AttributeType::AttributeProto_AttributeType_TENSOR ||
+        attr_value->second().type() == ONNX_NAMESPACE::AttributeProto_AttributeType::AttributeProto_AttributeType_INTS)) {
+    return make_unsupported("attr 'value' unsupported type.");
+  }
+  auto check_unsupported_attr = [&attributes](const std::string& attr_name) {
+    if (auto attr_sparse_value = attributes.find(attr_name); !(attr_sparse_value != attributes.end())) {
+      return make_unsupported("attr " + attr_name + "' is not supported");
+    }
+    return make_supported();
+  };
+  ORT_RETURN_IF_NOT_SUPPORTED(check_unsupported_attr("sparse_value"));
+  ORT_RETURN_IF_NOT_SUPPORTED(check_unsupported_attr("value_float"));
+  ORT_RETURN_IF_NOT_SUPPORTED(check_unsupported_attr("value_int"));
+  ORT_RETURN_IF_NOT_SUPPORTED(check_unsupported_attr("value_ints"));
+  ORT_RETURN_IF_NOT_SUPPORTED(check_unsupported_attr("value_string"));
+  ORT_RETURN_IF_NOT_SUPPORTED(check_unsupported_attr("value_strings"));
+  return make_supported();
+}
+
+////////////////////
+
+EtGlowConstantOfShapeNodeCapability::EtGlowConstantOfShapeNodeCapability(CapabilityMode mode, const ONNXSymbolsShapes& user_parametric_shapes_replacements)
+    : EtGlowDefaultNodeCapability(mode, user_parametric_shapes_replacements,
+                                  {type_float32, type_int8, type_float16}) {
+  // intentionally empty
+}
+
+EtGlowNodeSupportResult EtGlowConstantOfShapeNodeCapability::Supported(const Node& node, const GraphViewer& graph_viewer) const {
+  ORT_RETURN_IF_NOT_SUPPORTED(IsTypeSupported(node));
+  ORT_RETURN_IF_NOT_SUPPORTED(IsDimensionSupported(node, graph_viewer));
+  ORT_RETURN_IF_NOT_SUPPORTED(IsAttributeSupported(node, graph_viewer));
+  return make_supported();
+}
+
+EtGlowNodeSupportResult EtGlowConstantOfShapeNodeCapability::IsDimensionSupported(const Node& node, const GraphViewer& graph_viewer) const {
+  ORT_RETURN_IF_NOT_SUPPORTED(EtGlowDefaultNodeCapability::IsDimensionSupported(node, graph_viewer));
+  if (auto node_inputs = node.InputDefs(); node_inputs.size() >= 1) {  // NOLINT(readability-container-size-empty)
+    const auto* input = node_inputs[0];
+    if (!graph_viewer.IsConstantInitializer(input->Name(), true)) {
+      // if input constant cannot be found... not supported
+      return make_unsupported(input->Name() + " is not a constant.");
+    }
+    ORT_RETURN_IF_NO_SHAPE(input, GetCapabilityMode());
+    if (input->Shape()->dim_size() != 1) {
+      return make_unsupported("only 1D input is supported.");
+    }
+  }
+  return make_supported();
+}
+
+EtGlowNodeSupportResult EtGlowConstantOfShapeNodeCapability::IsAttributeSupported(const Node& node, const GraphViewer& graph_viewer) const {
+  if (const auto* output = node.OutputDefs()[0]; graph_viewer.IsConstantInitializer(output->Name(), true)) {
+    // if graph already contains output tensor as a constant, no need to check further
+    return make_supported();
+  }
+  const NodeAttributes& attributes = node.GetAttributes();
+
+  // must have attribute 'value'
+  if (auto attr_value = attributes.find(std::string{"value"}); attr_value != attributes.end()) {
+    // must have type TENSOR or INTS
+    if (!(attr_value->second().type() == ONNX_NAMESPACE::AttributeProto_AttributeType::AttributeProto_AttributeType_TENSOR ||
+          attr_value->second().type() == ONNX_NAMESPACE::AttributeProto_AttributeType::AttributeProto_AttributeType_INTS)) {
+      // unsupported type
+      return make_unsupported("unsupported type");
+    }
+
+    const ONNX_NAMESPACE::TensorProto* tensor_proto = nullptr;
+    graph_viewer.GetInitializedTensor(attr_value->first(), tensor_proto);
+    if (tensor_proto == nullptr || tensor_proto->dims_size() != 1) {
+      return make_unsupported("glow only supports 1D tensor");
+    }
+    if (!(tensor_proto->data_type() == type_float32 ||
+          tensor_proto->data_type() == type_float16 ||
+          tensor_proto->data_type() == type_int64 ||
+          tensor_proto->data_type() == type_int32)) {
+      // type not supported
+      return make_unsupported("type not supported");
+    }
+  }
+  return make_supported();
+}
+
+////////////////////
+
+EtGlowNodeSupportResult EtGlowConvNodeCapability::Supported(const Node& node, const GraphViewer& graph_viewer) const {
+  ORT_RETURN_IF_NOT_SUPPORTED(IsTypeSupported(node));
+  ORT_RETURN_IF_NOT_SUPPORTED(IsDimensionSupported(node, graph_viewer));
+  ORT_RETURN_IF_NOT_SUPPORTED(IsAttributeSupported(node, graph_viewer));
+  return make_supported();
+}
+
+EtGlowNodeSupportResult EtGlowConvNodeCapability::IsDimensionSupported(const Node& node, const GraphViewer& graph_viewer) const {
+  ORT_RETURN_IF_NOT_SUPPORTED(EtGlowDefaultNodeCapability::IsDimensionSupported(node, graph_viewer));
+
+  const auto node_inputs = node.InputDefs();
+  const auto* X = node_inputs[0];
+  ORT_RETURN_IF_NO_SHAPE(X, GetCapabilityMode());
+  ORT_RETURN_IF_NOT_SUPPORTED(HasNonZeroPositiveValueDims(X, GetParametricShapeReplacements(), GetCapabilityMode()));
+  if (const auto X_dim_size = X->Shape()->dim_size(); X_dim_size < 3 || X_dim_size > 5) {
+    return make_unsupported("glow only supports 1D (3dims), 2D (4dims) and 3D (5dims) convolutions. Found: " + std::to_string(X_dim_size));
+  }
+  return make_supported();
+}
+
+EtGlowNodeSupportResult EtGlowConvNodeCapability::IsAttributeSupported(const Node& node, const GraphViewer& graph_viewer) const {
+  ORT_UNUSED_PARAMETER(graph_viewer);
+  const NodeAttributes& attributes = node.GetAttributes();
+  if (auto attr_dilations = attributes.find(std::string{"dilations"}); attr_dilations != attributes.end()) {
+    // if attr dilations is provided
+    if (!(attr_dilations->second().type() == ONNX_NAMESPACE::AttributeProto_AttributeType::AttributeProto_AttributeType_INTS)) {
+      return make_unsupported("attr dilations unsupported type");
+    }
+    const auto& dilations_value = attr_dilations->second().ints();
+    if (dilations_value.size() == 3) {
+      for (int i = 0; i < dilations_value.size(); ++i) {
+        if (const auto dilation = dilations_value[i]; dilation != 1) {
+          return make_unsupported("for 3D Conv dilations glow currently only support the default value of [1, 1, 1]");
+        }
+      }
+    }
+
+    if (const auto* X_shape = node.InputDefs()[0]->Shape(); X_shape != nullptr) {
+      if (const auto num_axes = X_shape->dim_size() - 2; dilations_value.size() != num_axes) {
+        return make_unsupported(node.OpType() + " has invalid dilations size: " + std::to_string(dilations_value.size()) +
+                                " glow expects a " + std::to_string(num_axes) + "D .");
+      }
+    }
+  }
+  return make_supported();
+}
+
+////////////////////
+
+EtGlowNodeSupportResult EtGlowConvTransposeNodeCapability::Supported(const Node& node, const GraphViewer& graph_viewer) const {
+  ORT_RETURN_IF_NOT_SUPPORTED(IsTypeSupported(node));
+  ORT_RETURN_IF_NOT_SUPPORTED(IsDimensionSupported(node, graph_viewer));
+  ORT_RETURN_IF_NOT_SUPPORTED(IsAttributeSupported(node, graph_viewer));
+  return make_supported();
+}
+
+EtGlowNodeSupportResult EtGlowConvTransposeNodeCapability::IsDimensionSupported(const Node& node, const GraphViewer& graph_viewer) const {
+  ORT_RETURN_IF_NOT_SUPPORTED(EtGlowDefaultNodeCapability::IsDimensionSupported(node, graph_viewer));
+  return make_supported();
+}
+
+EtGlowNodeSupportResult EtGlowConvTransposeNodeCapability::IsAttributeSupported(const Node& node, const GraphViewer& graph_viewer) const {
+  ORT_UNUSED_PARAMETER(graph_viewer);
+  const NodeAttributes& attributes = node.GetAttributes();
+  auto attr_dilations = attributes.find("dilations");
+  if (!(attr_dilations != attributes.end())) {
+    return make_unsupported("attr dilations is missing");
+  }
+  if (attr_dilations->second().type() != ONNX_NAMESPACE::AttributeProto_AttributeType::AttributeProto_AttributeType_INTS) {
+    // unsupported type
+    return make_unsupported("attr dilations unsupported type");
+  }
+  if (const auto& attr_dilations_value = attr_dilations->second().ints(); attr_dilations_value.size() != 2) {
+    return make_unsupported("attr dilations must be of size 2. found size: " + std::to_string(attr_dilations_value.size()));
+  }
+  if (auto attr_output_shape = attributes.find("output_shape"); attr_output_shape != attributes.end()) {
+    const auto& attr_output_shape_value = attr_output_shape->second().ints();
+    if (attr_output_shape_value.size() != 2) {
+      return make_unsupported("attr outpu_shape must be of size 2. found size: " + std::to_string(attr_output_shape_value.size()));
+    }
+  }
+  if (auto attr_output_padding = attributes.find("output_padding"); attr_output_padding != attributes.end()) {
+    const auto& attr_attr_output_padding_value = attr_output_padding->second().ints();
+    for (int i = 0; i < attr_attr_output_padding_value.size(); ++i) {
+      const auto& value = attr_attr_output_padding_value.Get(i);
+      if (i == 0 && value != 0) {
+        return make_unsupported("attr output_padding is not supported by glow. First element must be non-zero.");
+      }
+      const auto& previous = attr_attr_output_padding_value.Get(i - 1);
+      if (previous == value) {
+        return make_unsupported("attr output_padding is not supported by glow. Found two equal elements.");
+      }
+    }
+  }
+
+  return make_supported();
+}
+
+////////////////////
+
+EtGlowNodeSupportResult EtGlowCumSumNodeCapability::Supported(const Node& node, const GraphViewer& graph_viewer) const {
+  ORT_RETURN_IF_NOT_SUPPORTED(IsTypeSupported(node));
+  ORT_RETURN_IF_NOT_SUPPORTED(IsDimensionSupported(node, graph_viewer));
+  return make_supported();
+}
+
+EtGlowNodeSupportResult EtGlowCumSumNodeCapability::IsTypeSupported(const Node& node) const {
+  ORT_RETURN_IF_NOT_SUPPORTED(EtGlowDefaultNodeCapability::IsTypeSupported(node));
+  auto node_inputs = node.InputDefs();
+  if (node_inputs.size() < 2) {
+    return make_unsupported("second input 'axis' is required");
+  }
+  if (const auto* axis = node_inputs[1]; axis->TypeAsProto() != nullptr) {
+    const auto node_datatype = static_cast<ORT_DataType>(axis->TypeAsProto()->tensor_type().elem_type());
+    const auto& supported_types = GetSupportedT2Types();
+    if (bool is_supported_type = std::find(supported_types.begin(), supported_types.end(), node_datatype) != supported_types.end(); !is_supported_type) {
+      return make_unsupported("input[1] (" + axis->Name() + ") is of type: " + std::to_string(node_datatype) + " which is not in the list of supported types.");
+    }
+  }
+  return make_supported();
+}
+
+EtGlowNodeSupportResult EtGlowCumSumNodeCapability::IsDimensionSupported(const Node& node, const GraphViewer& graph_viewer) const {
+  ORT_RETURN_IF_NOT_SUPPORTED(EtGlowDefaultNodeCapability::IsDimensionSupported(node, graph_viewer));
+  auto node_inputs = node.InputDefs();
+  const auto* axis = node_inputs[1];
+  ORT_RETURN_IF_NOT_SUPPORTED(IsScalarConstantSafe(axis, "axis", graph_viewer, GetCapabilityMode()));
+  return make_supported();
+}
+
+////////////////////
+
+EtGlowDropoutNodeCapability::EtGlowDropoutNodeCapability(CapabilityMode mode, const ONNXSymbolsShapes& user_parametric_shapes_replacements)
+    : EtGlowDefaultNodeCapability(mode, user_parametric_shapes_replacements,
+                                  {type_float32, type_int8, type_float16, type_int32, type_int64, type_int32}) {
+  // intentionally empty
+}
+
+EtGlowNodeSupportResult EtGlowDropoutNodeCapability::Supported(const Node& node, const GraphViewer& graph_viewer) const {
+  ORT_RETURN_IF_NOT_SUPPORTED(IsTypeSupported(node));
+  ORT_RETURN_IF_NOT_SUPPORTED(IsDimensionSupported(node, graph_viewer));
+  return make_supported();
+}
+
+EtGlowNodeSupportResult EtGlowDropoutNodeCapability::IsDimensionSupported(const Node& node, const GraphViewer& graph_viewer) const {
+  ORT_RETURN_IF_NOT_SUPPORTED(EtGlowDefaultNodeCapability::IsDimensionSupported(node, graph_viewer));
+
+  auto node_outpus = node.OutputDefs();
+  const bool at_most_one_output = node_outpus.size() <= 1;
+  return at_most_one_output ? make_supported() : make_unsupported("Dropout with more than one output is not supported by glow.");
+}
+
+////////////////////
+
+EtGlowEinsumNodeCapability::EtGlowEinsumNodeCapability(CapabilityMode mode, const ONNXSymbolsShapes& user_parametric_shapes_replacements)
+    : EtGlowDefaultNodeCapability(mode, user_parametric_shapes_replacements,
+                                  std::vector<ORT_DataType>{type_int8, type_int16, type_int32, type_int64, type_bfloat16, type_float16, type_float32}) {
+  // intentionally empty
+}
+
+EtGlowNodeSupportResult EtGlowEinsumNodeCapability::Supported(const Node& node, const GraphViewer& graph_viewer) const {
+  ORT_RETURN_IF_NOT_SUPPORTED(IsTypeSupported(node));
+  ORT_RETURN_IF_NOT_SUPPORTED(IsDimensionSupported(node, graph_viewer));
+  return make_unsupported(node.OpType() + " is current implementation in neuralizer is sub-optimal. Declaring as unsupported.");
+}
+
+////////////////////
+
+EtGlowExpandNodeCapability::EtGlowExpandNodeCapability(CapabilityMode mode, const ONNXSymbolsShapes& user_parametric_shapes_replacements)
+    : EtGlowDefaultNodeCapability(mode, user_parametric_shapes_replacements,
+                                  std::vector<ORT_DataType>{type_float32, type_int8, type_float16, type_int32, type_int64, type_int32}) {
+  // intentionally empty
+}
+
+EtGlowNodeSupportResult EtGlowExpandNodeCapability::Supported(const Node& node, const GraphViewer& graph_viewer) const {
+  ORT_RETURN_IF_NOT_SUPPORTED(IsTypeSupported(node));
+  ORT_RETURN_IF_NOT_SUPPORTED(IsDimensionSupported(node, graph_viewer));
+  return make_supported();
+}
+
+EtGlowNodeSupportResult EtGlowExpandNodeCapability::IsDimensionSupported(const Node& node, const GraphViewer& graph_viewer) const {
+  ORT_RETURN_IF_NOT_SUPPORTED(EtGlowDefaultNodeCapability::IsDimensionSupported(node, graph_viewer));
+
+  if (auto node_inputs = node.InputDefs();
+      node_inputs.size() >= 2 && !graph_viewer.IsConstantInitializer(node_inputs[1]->Name(), true)) {
+    return make_unsupported("only second input 'shape' is supported as initialized constant");
+  }
+  return make_supported();
+}
+
+////////////////////
+
+EtGlowNodeSupportResult EtGlowFlattenNodeCapability::Supported(const Node& node, const GraphViewer& graph_viewer) const {
+  ORT_RETURN_IF_NOT_SUPPORTED(IsTypeSupported(node));
+  ORT_RETURN_IF_NOT_SUPPORTED(IsDimensionSupported(node, graph_viewer));
+  ORT_RETURN_IF_NOT_SUPPORTED(IsAttributeSupported(node, graph_viewer));
+  return make_supported();
+}
+
+EtGlowNodeSupportResult EtGlowFlattenNodeCapability::IsAttributeSupported(const Node& node, const GraphViewer& graph_viewer) const {
+  ORT_UNUSED_PARAMETER(graph_viewer);
+  const NodeAttributes& attributes = node.GetAttributes();
+
+  if (auto axis_attr = attributes.find("axis"); axis_attr != attributes.end()) {
+    if (axis_attr->second().type() != ONNX_NAMESPACE::AttributeProto_AttributeType::AttributeProto_AttributeType_INT) {
+      return make_unsupported("attr axis is not of type INT");
+    }
+    const auto axis_attr_value = axis_attr->second().i();
+    if (axis_attr_value < -4 || axis_attr_value > 3) {
+      return make_unsupported("attr axis: " + std::to_string(axis_attr_value) + " is not in the range of glow accepted values ([-4, 3])");
+    }
+  }
+  return make_supported();
+}
+
+////////////////////
+
+EtGlowNodeSupportResult EtGlowGRUNodeCapability::Supported(const Node& node, const GraphViewer& graph_viewer) const {
+  ORT_RETURN_IF_NOT_SUPPORTED(IsTypeSupported(node));
+  ORT_RETURN_IF_NOT_SUPPORTED(IsDimensionSupported(node, graph_viewer));
+  ORT_RETURN_IF_NOT_SUPPORTED(IsAttributeSupported(node, graph_viewer));
+  return make_supported();
+}
+
+EtGlowNodeSupportResult EtGlowGRUNodeCapability::IsAttributeSupported(const Node& node, const GraphViewer& graph_viewer) const {
+  ORT_RETURN_IF_NOT_SUPPORTED(EtGlowOneLayerNodeCapability::IsAttributeSupported(node, graph_viewer));
+
+  const NodeAttributes& attributes = node.GetAttributes();
+  if (auto attr_activation_alpha = attributes.find("activation_alpha"); attr_activation_alpha != attributes.end()) {
+    return make_unsupported("glow does not support 'activation_alpha' attribute");
+  }
+  return make_supported();
+}
+
+////////////////////
+
+EtGlowNodeSupportResult EtGlowGemmNodeCapability::Supported(const Node& node, const GraphViewer& graph_viewer) const {
+  ORT_RETURN_IF_NOT_SUPPORTED(IsTypeSupported(node));
+  ORT_RETURN_IF_NOT_SUPPORTED(IsDimensionSupported(node, graph_viewer));
+  return make_supported();
+}
+
+EtGlowNodeSupportResult EtGlowGemmNodeCapability::IsTypeSupported(const Node& node) const {
+  ORT_RETURN_IF_NOT_SUPPORTED(EtGlowDefaultNodeCapability::IsTypeSupported(node));
+
+  auto node_inputs = node.InputDefs();
+  auto node_outputs = node.OutputDefs();
+  const auto* A = node_inputs[0];
+  const auto* B = node_inputs[1];
+  const auto* Y = node_outputs[0];
+  if (A->TypeAsProto() == nullptr || B->TypeAsProto() == nullptr || Y->TypeAsProto() == nullptr) {
+    return make_unsupported("TypeAsProto not available.");
+  }
+  // A, B & Y elem_type must match
+  auto A_elem_type = A->TypeAsProto()->tensor_type().elem_type();
+  auto B_elem_type = B->TypeAsProto()->tensor_type().elem_type();
+  if (auto Y_elem_type = Y->TypeAsProto()->tensor_type().elem_type();
+      (A_elem_type != B_elem_type) || (A_elem_type != Y_elem_type)) {
+    return make_unsupported("A, B, Y types do not match");
+  }
+
+  if (node_inputs.size() >= 3) {
+    const auto* C = node_inputs[2];
+    auto C_elem_type = C->TypeAsProto()->tensor_type().elem_type();
+
+    if (((A_elem_type == type_float32) || (A_elem_type == type_float16)) && (A_elem_type != C_elem_type)) {
+      return make_unsupported("A & C types are not supported (not-quantization check failed).");
+    } else if ((A_elem_type == type_int8) && !((C_elem_type == type_int8) || (C_elem_type == type_int32))) {  // NOLINT(readability-else-after-return)
+      return make_unsupported("A & C types are not supported (quantization check failed).");
+    }
+  }
+  return make_supported();
+}
+
+EtGlowNodeSupportResult EtGlowGemmNodeCapability::IsDimensionSupported(const Node& node, const GraphViewer& graph_viewer) const {
+  ORT_RETURN_IF_NOT_SUPPORTED(EtGlowDefaultNodeCapability::IsDimensionSupported(node, graph_viewer));
+
+  auto node_inputs = node.InputDefs();
+  auto node_outputs = node.OutputDefs();
+  const auto* A = node_inputs[0];
+  const auto* B = node_inputs[1];
+  const auto* Y = node_outputs[0];
+
+  ORT_RETURN_IF_NO_SHAPE(A, GetCapabilityMode());
+  ORT_RETURN_IF_NO_SHAPE(B, GetCapabilityMode());
+  ORT_RETURN_IF_NO_SHAPE(Y, GetCapabilityMode());
+
+  ORT_RETURN_IF_NOT_SUPPORTED(HasNonZeroPositiveValueDims(A, GetParametricShapeReplacements(), GetCapabilityMode()));
+  ORT_RETURN_IF_NOT_SUPPORTED(HasNonZeroPositiveValueDims(B, GetParametricShapeReplacements(), GetCapabilityMode()));
+  ORT_RETURN_IF_NOT_SUPPORTED(HasNonZeroPositiveValueDims(Y, GetParametricShapeReplacements(), GetCapabilityMode()));
+
+  // check shapes
+  if ((A->Shape()->dim_size() != 2) || (B->Shape()->dim_size() != 2)) {
+    return make_unsupported("A, B must be 2D");
+  }
+  if (node_inputs.size() >= 3) {
+    const auto* C = node_inputs[2];
+    ORT_RETURN_IF_NO_SHAPE(C, GetCapabilityMode());
+    // - C must be 1D or 2D (no scalar support either)
+    if (C->Shape()->dim_size() > 2 || IsScalar(*C)) {
+      return make_unsupported("C must be 1D or 2D");
+    }
+  }
+  // - Y must be 2D
+  if (Y->Shape()->dim_size() != 2) {
+    return make_unsupported("Y must be 2D");
+  }
+
+  const NodeAttributes& attributes = node.GetAttributes();
+  auto transA = attributes.find("transA");
+  auto transB = attributes.find("transB");
+  bool transposeA = transA != attributes.end() && transA->second().i() == 1;
+  bool transposeB = transB != attributes.end() && transB->second().i() == 1;
+
+  auto A_dim_0 = transposeA ? A->Shape()->dim(1).dim_value() : A->Shape()->dim(0).dim_value();
+  auto A_dim_1 = transposeA ? A->Shape()->dim(0).dim_value() : A->Shape()->dim(1).dim_value();
+
+  auto B_dim_0 = transposeB ? B->Shape()->dim(1).dim_value() : B->Shape()->dim(0).dim_value();
+
+  // check dimensions
+  if (auto B_dim_1 = transposeB ? B->Shape()->dim(0).dim_value() : B->Shape()->dim(1).dim_value();
+      (A_dim_0 != Y->Shape()->dim(0).dim_value()) ||
+      (A_dim_1 != B_dim_0) ||
+      (B_dim_1 != Y->Shape()->dim(1).dim_value())) {
+    return make_unsupported("A, B dimensions check fails");
+  }
+  if (node_inputs.size() >= 3) {
+    const auto* C = node_inputs[2];
+    if (C->Shape()->dim_size() == 1) {
+      if (C->Shape()->dim(0).dim_value() != Y->Shape()->dim(1).dim_value()) {
+        return make_unsupported("C, Y dimensions check fails");
+      }
+    } else if (C->Shape()->dim_size() == 2) {
+      if ((C->Shape()->dim(0).dim_value() != Y->Shape()->dim(0).dim_value()) ||
+          (C->Shape()->dim(1).dim_value() != Y->Shape()->dim(1).dim_value())) {
+        return make_unsupported("C, Y dimensions check fails");
+      }
+    }
+  }
+  return make_supported();
+}
+
+////////////////////
+
+EtGlowIfNodeCapability::EtGlowIfNodeCapability(CapabilityMode mode, const ONNXSymbolsShapes& user_parametric_shapes_replacements)
+    : EtGlowDefaultNodeCapability(mode, user_parametric_shapes_replacements, GetSupportedBTypes()) {
+  // intentionally empty
+}
+
+EtGlowNodeSupportResult EtGlowIfNodeCapability::Supported(const Node& node, const GraphViewer& graph_viewer) const {
+  ORT_RETURN_IF_NOT_SUPPORTED(IsTypeSupported(node));
+  ORT_RETURN_IF_NOT_SUPPORTED(IsDimensionSupported(node, graph_viewer));
+  ORT_RETURN_IF_NOT_SUPPORTED(IsAttributeSupported(node, graph_viewer));
+  return make_supported();
+}
+
+EtGlowNodeSupportResult EtGlowIfNodeCapability::IsDimensionSupported(const Node& node, const GraphViewer& graph_viewer) const {
+  ORT_RETURN_IF_NOT_SUPPORTED(EtGlowDefaultNodeCapability::IsDimensionSupported(node, graph_viewer));
+  if (auto node_outputs = node.OutputDefs(); node_outputs.size() > 1) {
+    return make_unsupported("currently we only support a single If output. Detected: " + std::to_string(node_outputs.size()));
+  }
+
+  auto node_inputs = node.InputDefs();
+  const auto* cond = node_inputs[0];
+  ORT_RETURN_IF_NOT_SUPPORTED(IsScalarConstantSafe(cond, "cond", graph_viewer, GetCapabilityMode()));
+  return make_supported();
+}
+
+EtGlowNodeSupportResult EtGlowIfNodeCapability::IsAttributeSupported(const Node& node, const GraphViewer& graph_viewer) const {
+  ORT_UNUSED_PARAMETER(graph_viewer);
+  const NodeAttributes& attributes = node.GetAttributes();
+
+  auto attr_checks = [&attributes](std::string_view attr_name) {
+    auto attr = attributes.find(std::string{attr_name});
+    // must have 'attr_name'
+    if (!(attr != attributes.end())) {
+      return make_unsupported("attr: " + std::string{attr_name} + " not found");
+    }
+    // must have type GRAPH
+    if (attr->second().type() != ONNX_NAMESPACE::AttributeProto_AttributeType::AttributeProto_AttributeType_GRAPH) {
+      // unsupported type
+      return make_unsupported("attr unsupported type");
+    }
+    return make_supported();
+  };
+
+  ORT_RETURN_IF_NOT_SUPPORTED(attr_checks("else_branch"));
+  ORT_RETURN_IF_NOT_SUPPORTED(attr_checks("then_branch"));
+
+  // OBS: ORT limitation, we cannot check Attr GRAPH
+
+  return make_supported();
+}
+
+////////////////////
+
+EtGlowNodeSupportResult EtGlowLSTMNodeCapability::Supported(const Node& node, const GraphViewer& graph_viewer) const {
+  ORT_RETURN_IF_NOT_SUPPORTED(IsTypeSupported(node));
+  ORT_RETURN_IF_NOT_SUPPORTED(IsDimensionSupported(node, graph_viewer));
+  ORT_RETURN_IF_NOT_SUPPORTED(IsAttributeSupported(node, graph_viewer));
+  return make_supported();
+}
+
+EtGlowNodeSupportResult EtGlowLSTMNodeCapability::IsAttributeSupported(const Node& node, const GraphViewer& graph_viewer) const {
+  ORT_RETURN_IF_NOT_SUPPORTED(EtGlowOneLayerNodeCapability::IsAttributeSupported(node, graph_viewer));
+  return make_supported();
+}
+
+////////////////////
+
+EtGlowNodeSupportResult EtGlowLayerNormalizationNodeCapability::Supported(const Node& node, const GraphViewer& graph_viewer) const {
+  ORT_RETURN_IF_NOT_SUPPORTED(IsTypeSupported(node));
+  ORT_RETURN_IF_NOT_SUPPORTED(IsDimensionSupported(node, graph_viewer));
+  ORT_RETURN_IF_NOT_SUPPORTED(IsAttributeSupported(node, graph_viewer));
+  return make_supported();
+}
+
+EtGlowNodeSupportResult EtGlowLayerNormalizationNodeCapability::IsDimensionSupported(const Node& node, const GraphViewer& graph_viewer) const {
+  ORT_RETURN_IF_NOT_SUPPORTED(EtGlowDefaultNodeCapability::IsDimensionSupported(node, graph_viewer));
+
+  auto node_inputs = node.InputDefs();
+
+  if (node_inputs.size() < 2) {
+    return make_unsupported("must have at least input and scale");
+  }
+  const auto* X = node_inputs[0];
+  const auto* scale = node_inputs[1];
+
+  auto X_elem_type = X->TypeAsProto()->tensor_type().elem_type();
+  auto scale_elem_type = scale->TypeAsProto()->tensor_type().elem_type();
+  if (X_elem_type != scale_elem_type) {
+    return make_unsupported("X, scale must have same types");
+  }
+
+  if (node_inputs.size() > 3) {
+    return make_unsupported("invalid number of inputs");
+  }
+  if (node_inputs.size() == 3) {
+    const auto* B = node_inputs[2];  // bias
+
+    auto B_elem_type = B->TypeAsProto()->tensor_type().elem_type();
+    if (B_elem_type != scale_elem_type) {
+      return make_unsupported("B, scale must have same types");
+    }
+  }
+
+  if (auto node_outputs = node.OutputDefs(); node_outputs.size() > 1) {
+    return make_unsupported("only 1 output is supported");
+  }
+
+  return make_supported();
+}
+
+EtGlowNodeSupportResult EtGlowLayerNormalizationNodeCapability::IsAttributeSupported(const Node& node, const GraphViewer& graph_viewer) const {
+  ORT_UNUSED_PARAMETER(graph_viewer);
+  const NodeAttributes& attributes = node.GetAttributes();
+  if (auto attr_epsilon = attributes.find("epsilon"); attr_epsilon != attributes.end() && attr_epsilon->second().type() != ONNX_NAMESPACE::AttributeProto_AttributeType_FLOAT) {
+    return make_unsupported("attr 'epsilon' must be of type FLOAT");
+  }
+  return make_supported();
+}
+
+////////////////////
+
+EtGlowLessNodeCapability::EtGlowLessNodeCapability(CapabilityMode mode, const ONNXSymbolsShapes& user_parametric_shapes_replacements)
+    : EtGlowDefaultNodeCapability(mode, user_parametric_shapes_replacements,
+                                  std::vector<ORT_DataType>{type_float32, type_float16, type_int8, type_int32, type_int64},
+                                  std::vector<ORT_DataType>{type_bool}) {
+  // intentionally blank
+}
+
+EtGlowNodeSupportResult EtGlowLessNodeCapability::Supported(const Node& node, const GraphViewer& graph_viewer) const {
+  ORT_RETURN_IF_NOT_SUPPORTED(IsTypeSupported(node));
+  ORT_RETURN_IF_NOT_SUPPORTED(IsDimensionSupported(node, graph_viewer));
+  return make_supported();
+}
+
+EtGlowNodeSupportResult EtGlowLessNodeCapability::IsTypeSupported(const Node& node) const {
+  auto supportedInputTypes = GetSupportedInputTypes();
+  if (auto node_inputs = node.InputDefs(); !node_inputs.empty() && node_inputs[0]->TypeAsProto() != nullptr) {
+    auto node_datatype = static_cast<ORT_DataType>(node_inputs[0]->TypeAsProto()->tensor_type().elem_type());
+    if (bool is_supported_type = std::find(supportedInputTypes.begin(), supportedInputTypes.end(), node_datatype) != supportedInputTypes.end(); !is_supported_type) {
+      return make_unsupported("input[0] (" + node_inputs[0]->Name() + ") is of type " + std::to_string(node_datatype) + " which is not in the list of supported types");
+    }
+  }
+  auto supportedOutputTypes = GetSupportedOutputTypes();
+  auto node_outputs = node.OutputDefs();
+  if (node_outputs.empty()) {
+    return make_unsupported("must have at least one output");
+  }
+  if (node_outputs[0]->TypeAsProto() == nullptr) {
+    return make_unsupported("cannot check output type, then not supported");
+  }
+  auto node_datatype = static_cast<ORT_DataType>(node_outputs[0]->TypeAsProto()->tensor_type().elem_type());
+  if (bool is_supported_type = std::find(supportedOutputTypes.begin(), supportedOutputTypes.end(), node_datatype) != supportedOutputTypes.end(); !is_supported_type) {
+    return make_unsupported("outputs[0] (" + node_outputs[0]->Name() + ") is of type " + std::to_string(node_datatype) + " which is not in the list of supported types");
+  }
+  return make_supported();
+}
+
+////////////////////
+
+EtGlowNodeSupportResult EtGlowLogSoftmaxNodeCapability::Supported(const Node& node, const GraphViewer& graph_viewer) const {
+  ORT_RETURN_IF_NOT_SUPPORTED(IsTypeSupported(node));
+  ORT_RETURN_IF_NOT_SUPPORTED(IsDimensionSupported(node, graph_viewer));
+  ORT_RETURN_IF_NOT_SUPPORTED(IsAttributeSupported(node, graph_viewer));
+  return make_supported();
+}
+
+EtGlowNodeSupportResult EtGlowLogSoftmaxNodeCapability::IsDimensionSupported(const Node& node, const GraphViewer& graph_viewer) const {
+  ORT_RETURN_IF_NOT_SUPPORTED(EtGlowDefaultNodeCapability::IsDimensionSupported(node, graph_viewer));
+
+  constexpr int log_sofmax_min_inputs = 2;
+  constexpr int log_sofmax_num_inputs_opset13 = 4;
+  auto node_inputs = node.InputDefs();
+  if (node_inputs.size() < log_sofmax_min_inputs) {
+    return make_unsupported(node.Name() + " input dimensions must be >= 2");
+  }
+  if (auto opset = node.SinceVersion(); opset >= 13_opset && node_inputs.size() != log_sofmax_num_inputs_opset13) {  // NOLINT(cppcoreguidelines-avoid-magic-numbers,readability-magic-numbers)
+    return make_unsupported(node.Name() + " opset >= 13 must have 4 inputs");
+  }
+  return make_supported();
+}
+
+EtGlowNodeSupportResult EtGlowLogSoftmaxNodeCapability::IsAttributeSupported(const Node& node, const GraphViewer& graph_viewer) const {
+  ORT_UNUSED_PARAMETER(graph_viewer);
+  const NodeAttributes& attributes = node.GetAttributes();
+
+  if (auto attr_axis = attributes.find("axis"); attr_axis != attributes.end()) {
+    auto opset = node.SinceVersion();
+    if (opset >= 13_opset && attr_axis->second().i() > 3) {  // NOLINT(cppcoreguidelines-avoid-magic-numbers,readability-magic-numbers)
+      return make_unsupported("attr axis (int) value must be <= 3");
+    }
+  }
+
+  return make_supported();
+}
+
+////////////////////
+
+EtGlowNodeSupportResult EtGlowLoopNodeCapability::Supported(const Node& node, const GraphViewer& graph_viewer) const {
+  ORT_RETURN_IF_NOT_SUPPORTED(IsTypeSupported(node));
+  ORT_RETURN_IF_NOT_SUPPORTED(IsDimensionSupported(node, graph_viewer));
+  ORT_RETURN_IF_NOT_SUPPORTED(IsAttributeSupported(node, graph_viewer));
+  return make_supported();
+}
+
+EtGlowNodeSupportResult EtGlowLoopNodeCapability::IsDimensionSupported(const Node& node, const GraphViewer& graph_viewer) const {
+  ORT_RETURN_IF_NOT_SUPPORTED(EtGlowDefaultNodeCapability::IsDimensionSupported(node, graph_viewer));
+
+  auto node_inputs = node.InputDefs();
+  if (node_inputs.size() >= 2) {
+    const auto* M = node_inputs[0];
+    ORT_RETURN_IF_NOT_SUPPORTED(IsScalarConstantSafe(M, "M", graph_viewer, GetCapabilityMode()));
+    if (M->TypeAsProto()->tensor_type().elem_type() != type_int64) {
+      return make_unsupported("M element_type is not int64");
+    }
+
+    const ONNX_NAMESPACE::TensorProto* tensor_proto = nullptr;
+    graph_viewer.GetInitializedTensor(M->Name(), tensor_proto);
+    if (tensor_proto != nullptr) {
+      const auto* val = reinterpret_cast<const float*>(tensor_proto->raw_data().data());
+
+      if (std::isnan(val[0]) || std::isinf(val[0]) || val[0] < 0) {
+        return make_unsupported("trip-count must be positive");
+      }
+    }
+  }
+
+  if (node_inputs.size() > 2) {
+    const auto* cond = node_inputs[1];
+
+    // must be a constant
+    if (!graph_viewer.IsConstantInitializer(cond->Name(), true)) {
+      return make_unsupported(cond->Name() + " is not a constant");
+    }
+    if (cond->TypeAsProto()->tensor_type().elem_type() != type_bool) {
+      return make_unsupported("cond element_type is not bool");
+    }
+  }
+  return make_supported();
+}
+
+EtGlowNodeSupportResult EtGlowLoopNodeCapability::IsAttributeSupported(const Node& node, const GraphViewer& graph_viewer) const {
+  ORT_UNUSED_PARAMETER(graph_viewer);
+  const NodeAttributes& attributes = node.GetAttributes();
+  auto attr_body = attributes.find("body");
+  if (!(attr_body != attributes.end())) {
+    return make_unsupported("attr 'body' is mandatory");
+  }
+  if (attr_body->second().type() != ONNX_NAMESPACE::AttributeProto_AttributeType::AttributeProto_AttributeType_GRAPH) {
+    return make_unsupported("attr 'body' unsupported type");
+  }
+  // OBS: cannot check lots of conditons regarding body 'graph'
+  return make_supported();
+}
+
+////////////////////
+
+EtGlowNodeSupportResult EtGlowMatMulNodeCapability::Supported(const Node& node, const GraphViewer& graph_viewer) const {
+  ORT_RETURN_IF_NOT_SUPPORTED(IsTypeSupported(node));
+  ORT_RETURN_IF_NOT_SUPPORTED(IsAttributeSupported(node, graph_viewer));
+  ORT_RETURN_IF_NOT_SUPPORTED(IsDimensionSupported(node, graph_viewer));
+  return make_supported();
+}
+
+EtGlowNodeSupportResult EtGlowMatMulNodeCapability::IsDimensionSupported(const Node& node, const GraphViewer& graph_viewer) const {
+  ORT_RETURN_IF_NOT_SUPPORTED(EtGlowDefaultNodeCapability::IsDimensionSupported(node, graph_viewer));
+
+  auto node_inputs = node.InputDefs();
+  const auto& A = node_inputs[0];
+  const auto& B = node_inputs[1];
+  ORT_RETURN_IF_NO_SHAPE(A, GetCapabilityMode());
+  ORT_RETURN_IF_NO_SHAPE(B, GetCapabilityMode());
+  const auto* A_shape = A->Shape();
+  const auto* B_shape = B->Shape();
+  if ((A_shape->dim_size() == 2) && (B_shape->dim_size() == 2)) {
+    // constrains for regular 2D matrix multiplication
+    ORT_RETURN_IF_NOT_SUPPORTED(HasNonZeroPositiveValueDimsSafe(A, GetParametricShapeReplacements(), GetCapabilityMode()));
+    ORT_RETURN_IF_NOT_SUPPORTED(HasNonZeroPositiveValueDimsSafe(B, GetParametricShapeReplacements(), GetCapabilityMode()));
+  } else if (A_shape->dim_size() == 2 && B_shape->dim_size() != 2) {
+    return make_unsupported(A->Name() + " and " + B->Name() + " inputs have different dimensions! " + std::to_string(A_shape->dim_size()) + "!=" + std::to_string(B_shape->dim_size()));
+  } else if (A_shape->dim_size() > 2) {
+    // constrains for stack of matrices
+
+    // first dimension contains the batch and glow wants it to be equal
+    int dim_to_check = 0;
+    const auto& A_dim = A_shape->dim(dim_to_check);
+    const auto& B_dim = B_shape->dim(dim_to_check);
+
+    long long int A_value;
+    if (A_dim.has_dim_param()) {
+      const auto& param2value_replacements = GetParametricShapeReplacements();
+      ORT_RETURN_IF_NOT_SUPPORTED(HasValueForParametricDim(A_dim, param2value_replacements, "input: " + A->Name()));
+      A_value = param2value_replacements.get_safe<std::size_t>(A_dim.dim_param()).value();
+    } else {
+      A_value = A_dim.dim_value();
+    }
+
+    long long int B_value;
+    if (B_dim.has_dim_param()) {
+      const auto& param2value_replacements = GetParametricShapeReplacements();
+      ORT_RETURN_IF_NOT_SUPPORTED(HasValueForParametricDim(B_dim, param2value_replacements, "input: " + B->Name()));
+      B_value = param2value_replacements.get_safe<std::size_t>(B_dim.dim_param()).value();
+    } else {
+      B_value = B_dim.dim_value();
+    }
+
+    if (A_value != 0 && B_value != 0 && A_value != B_value) {
+      return make_unsupported(A->Name() + " and " + B->Name() + " inputs have different batch! " + std::to_string(A_value) + "!=" + std::to_string(B_value));
+    }
+  } else if (A_shape->dim_size() < 2) {
+    return make_unsupported("glow requires input[0] (" + A->Name() + ") to at least be 2-dimensional. Got " + std::to_string(A_shape->dim_size()));
+  }
+
+  return make_supported();
+}
+
+////////////////////
+
+EtGlowNodeSupportResult EtGlowMatMulNBitsNodeCapability::Supported(const Node& node, const GraphViewer& graph_viewer) const {
+  ORT_RETURN_IF_NOT_SUPPORTED(IsTypeSupported(node));
+  ORT_RETURN_IF_NOT_SUPPORTED(IsAttributeSupported(node, graph_viewer));
+  ORT_RETURN_IF_NOT_SUPPORTED(IsDimensionSupported(node, graph_viewer));
+  return make_supported();
+}
+
+EtGlowNodeSupportResult EtGlowMatMulNBitsNodeCapability::IsTypeSupported(const Node& node) const {
+  auto node_inputs = node.InputDefs();
+  auto node_outputs = node.OutputDefs();
+  const auto* A = node_inputs[0];
+  const auto* B = node_inputs[1];
+  const auto* scales = node_inputs[2];
+  const auto* Y = node_outputs[0];
+  if (A->TypeAsProto() == nullptr || B->TypeAsProto() == nullptr || scales->TypeAsProto() == nullptr || Y->TypeAsProto() == nullptr) {
+    return make_unsupported("A, B, scales, or Y type info missing");
+  }
+
+  // A, scales & Y elem_type must match and be fp32/16
+  auto A_elem_type = A->TypeAsProto()->tensor_type().elem_type();
+  if ((A_elem_type != type_float32) && (A_elem_type != type_float16)) {
+    return make_unsupported("A, scales, && Y elem_type must match and be fp32/16");
+  }
+  auto scales_elem_type = scales->TypeAsProto()->tensor_type().elem_type();
+  if (auto Y_elem_type = Y->TypeAsProto()->tensor_type().elem_type();
+      (A_elem_type != scales_elem_type) || (A_elem_type != Y_elem_type)) {
+    return make_unsupported("A, scales, && Y elem_type must match and be fp32/16");
+  }
+
+  auto B_elem_type = B->TypeAsProto()->tensor_type().elem_type();
+  if (B_elem_type != type_uint8) {
+    return make_unsupported("B elem_type must be uint8");
+  }
+
+  if (node_inputs.size() >= 4) {
+    const auto* zero_points = node_inputs[3];
+    if (zero_points->TypeAsProto() == nullptr) {
+      return GetCapabilityMode() == CapabilityMode::STRICT ? make_unsupported("zero_points type info missing")
+                                                           : make_supported();
+    }
+    if (auto zero_points_elem_type = zero_points->TypeAsProto()->tensor_type().elem_type();
+        B_elem_type != zero_points_elem_type) {
+      return make_unsupported("B_elem_type & zero_points_elem_type must match");
+    }
+  }
+
+  if (node_inputs.size() >= 5) {  // NOLINT(cppcoreguidelines-avoid-magic-numbers,readability-magic-numbers)
+    return make_unsupported("optional g_idx input not supported by glow");
+  }
+
+  return make_supported();
+}
+
+EtGlowNodeSupportResult EtGlowMatMulNBitsNodeCapability::IsAttributeSupported(const Node& node, const GraphViewer& graph_viewer) const {
+  ORT_UNUSED_PARAMETER(graph_viewer);
+  const NodeAttributes& attributes = node.GetAttributes();
+
+  if (auto attr_K = attributes.find(std::string{"K"});
+      !(attr_K != attributes.end()) || (attr_K->second().type() != ONNX_NAMESPACE::AttributeProto_AttributeType::AttributeProto_AttributeType_INT)) {
+    return make_unsupported("must have attribute 'K' (INT)");
+  }
+
+  if (auto attr_N = attributes.find(std::string{"N"});
+      !(attr_N != attributes.end()) || (attr_N->second().type() != ONNX_NAMESPACE::AttributeProto_AttributeType::AttributeProto_AttributeType_INT)) {
+    return make_unsupported("must have attribute 'N' (INT)");
+  }
+
+  if (auto attr_bits = attributes.find(std::string{"bits"}); !(attr_bits != attributes.end()) || (attr_bits->second().type() != ONNX_NAMESPACE::AttributeProto_AttributeType::AttributeProto_AttributeType_INT)) {
+    return make_unsupported("must have attribute 'bits' (INT), can be either 8 or 4");
+  } else if (auto nbits = attr_bits->second().i(); (nbits != 8) && (nbits != 4)) {  // NOLINT(cppcoreguidelines-avoid-magic-numbers,readability-magic-numbers)
+    return make_unsupported("must have attribute 'bits' (INT), can be either 8 or 4");
+  }
+
+  // must have attribute 'block_size' (INT), must be a power of 2
+  if (auto attr_block_size = attributes.find(std::string{"block_size"});
+      !(attr_block_size != attributes.end()) || (attr_block_size->second().type() != ONNX_NAMESPACE::AttributeProto_AttributeType::AttributeProto_AttributeType_INT)) {
+    return make_unsupported("must have attribute 'block_size' (INT), must be a power of 2");
+  } else if (auto block = attr_block_size->second().i(); (block <= 0) || ((block & (block - 1)) != 0)) {  // NOLINT(readability-else-after-return)
+    return make_unsupported("must have attribute 'block_size' (INT), must be a power of 2");
+  }
+
+  // optional attribute 'accuracy_level' (INT) not supported, unless it is 0 (=unset)
+  if (auto attr_accuracy = attributes.find(std::string{"accuracy_level"});
+      (attr_accuracy != attributes.end()) && ((attr_accuracy->second().type() != ONNX_NAMESPACE::AttributeProto_AttributeType::AttributeProto_AttributeType_INT) || (attr_accuracy->second().i() != 0))) {
+    return make_unsupported("optional attribute 'accuracy_level' (INT) not supported, unless it is 0 (=unset)");
+  }
+
+  return make_supported();
+}
+
+EtGlowNodeSupportResult EtGlowMatMulNBitsNodeCapability::IsDimensionSupported(const Node& node, const GraphViewer& graph_viewer) const {
+  ORT_RETURN_IF_NOT_SUPPORTED(EtGlowDefaultNodeCapability::IsDimensionSupported(node, graph_viewer));
+
+  auto node_inputs = node.InputDefs();
+  auto node_outputs = node.OutputDefs();
+  const auto* A = node_inputs[0];
+  const auto* B = node_inputs[1];
+  const auto* scales = node_inputs[2];
+  const auto* Y = node_outputs[0];
+
+  if (!graph_viewer.IsConstantInitializer(B->Name(), true) || !graph_viewer.IsConstantInitializer(scales->Name(), true)) {
+    return make_unsupported("Neuralizer requires B and scales to be constant");
+  }
+
+  ORT_RETURN_IF_NO_SHAPE(A, GetCapabilityMode());
+  ORT_RETURN_IF_NO_SHAPE(B, GetCapabilityMode());
+  ORT_RETURN_IF_NO_SHAPE(scales, GetCapabilityMode());
+  ORT_RETURN_IF_NO_SHAPE(Y, GetCapabilityMode());
+
+  // check shapes
+  // - A must be (at least) 2D
+  // - B must be 3D
+  // - scales must be 1D
+  auto act_dims = A->Shape()->dim_size();
+  if ((act_dims < 2) || (B->Shape()->dim_size() != 3) || (scales->Shape()->dim_size() != 1)) {
+    return make_unsupported("A must be (at least) 2D, B must be 3D, scales must be 1D");
+  }
+
+  if (node_inputs.size() >= 4) {
+    const auto* zero_points = node_inputs[3];
+    if (!graph_viewer.IsConstantInitializer(zero_points->Name(), true)) {
+      return make_unsupported("Neuralizer requires zero_points to be constant");
+    }
+
+    ORT_RETURN_IF_NO_SHAPE(zero_points, GetCapabilityMode());
+    if (zero_points->Shape()->dim_size() != 1) {
+      return make_unsupported("zero_points must be 1D");
+    }
+  }
+
+  if (Y->Shape()->dim_size() != act_dims) {
+    return make_unsupported("Y must have the same number of dimensions as A");
+  }
+
+  const NodeAttributes& attributes = node.GetAttributes();
+  auto Acols = attributes.find("K")->second().i();
+  auto Bcols = attributes.find("N")->second().i();
+  auto block = attributes.find("block_size")->second().i();
+  auto nbits = attributes.find("bits")->second().i();
+
+  // check dimensions
+  for (int i = 0; i < act_dims - 1; ++i) {
+    if (A->Shape()->dim(i).dim_value() != Y->Shape()->dim(i).dim_value()) {
+      return make_unsupported("A and Y must have the same number of rows (including batching)");
+    }
+  }
+  if (A->Shape()->dim(act_dims - 1).dim_value() != Acols) {
+    return make_unsupported("Acols must be correct");
+  }
+  auto nblocks = (Acols + block - 1) / block;
+  if ((B->Shape()->dim(0).dim_value() != Bcols) || (B->Shape()->dim(1).dim_value() != nblocks) || (B->Shape()->dim(2).dim_value() != ((block * nbits) / 8))) {  // NOLINT(cppcoreguidelines-avoid-magic-numbers,readability-magic-numbers)
+    return make_unsupported("B must have dimensions (Bcols, nblocks, blockbytes)");
+  }
+  if (scales->Shape()->dim(0).dim_value() != (Bcols * nblocks)) {
+    return make_unsupported("scales must have Bcols*nblocks elements");
+  }
+
+  if (node_inputs.size() >= 4) {
+    const auto* zero_points = node_inputs[3];
+    if (zero_points->Shape()->dim(0).dim_value() != ((Bcols * nblocks * nbits + 7) / 8)) {  // NOLINT(cppcoreguidelines-avoid-magic-numbers,readability-magic-numbers)
+      return make_unsupported("zero_points must have Bcols*nblocks elements");
+    }
+  }
+
+  return make_supported();
+}
+
+////////////////////
+
+EtGlowNodeSupportResult EtGlowNonMaxSuppressionNodeCapability::Supported(const Node& node, const GraphViewer& graph_viewer) const {
+  ORT_RETURN_IF_NOT_SUPPORTED(IsTypeSupported(node));
+  ORT_RETURN_IF_NOT_SUPPORTED(IsDimensionSupported(node, graph_viewer));
+  return make_supported();
+}
+
+EtGlowNodeSupportResult EtGlowNonMaxSuppressionNodeCapability::IsDimensionSupported(const Node& node, const GraphViewer& graph_viewer) const {
+  ORT_RETURN_IF_NOT_SUPPORTED(EtGlowDefaultNodeCapability::IsDimensionSupported(node, graph_viewer));
+
+  auto node_inputs = node.InputDefs();
+  if (node_inputs.size() >= 3) {
+    const auto* max_output_boxes_per_class = node_inputs[2];
+    ORT_RETURN_IF_NOT_SUPPORTED(IsScalarConstantSafe(max_output_boxes_per_class, "max_output_boxes_per_class", graph_viewer, GetCapabilityMode()));
+    auto max_output_boxes_per_class_elem_type = max_output_boxes_per_class->TypeAsProto()->tensor_type().elem_type();
+    if (!(max_output_boxes_per_class_elem_type == type_int64 || max_output_boxes_per_class_elem_type == type_int32)) {
+      return make_unsupported("max_output_boxes_per_class must be int64 or int32");
+    }
+  }
+  if (node_inputs.size() >= 4) {
+    const auto* iou_threshold = node_inputs[3];
+    ORT_RETURN_IF_NOT_SUPPORTED(IsScalarConstantSafe(iou_threshold, "iou_threshold", graph_viewer, GetCapabilityMode()));
+  }
+  if (node_inputs.size() >= 5) {                   // NOLINT(cppcoreguidelines-avoid-magic-numbers,readability-magic-numbers)
+    const auto* score_threshold = node_inputs[4];  // NOLINT(cppcoreguidelines-avoid-magic-numbers,readability-magic-numbers)
+    ORT_RETURN_IF_NOT_SUPPORTED(IsScalarConstantSafe(score_threshold, "score_threshold", graph_viewer, GetCapabilityMode()));
+  }
+  return make_supported();
+}
+
+////////////////////
+
+EtGlowNonZeroNodeCapability::EtGlowNonZeroNodeCapability(CapabilityMode mode, const ONNXSymbolsShapes& user_parametric_shapes_replacements)
+    : EtGlowDefaultNodeCapability(mode, user_parametric_shapes_replacements,
+                                  std::vector<ORT_DataType>{type_float32, type_int64, type_int32}) {
+  // intentionally empty
+}
+
+EtGlowNodeSupportResult EtGlowNonZeroNodeCapability::Supported(const Node& node, const GraphViewer& graph_viewer) const {
+  ORT_RETURN_IF_NOT_SUPPORTED(IsTypeSupported(node));
+  ORT_RETURN_IF_NOT_SUPPORTED(IsDimensionSupported(node, graph_viewer));
+  return make_supported();
+}
+
+EtGlowNodeSupportResult EtGlowNonZeroNodeCapability::IsDimensionSupported(const Node& node, const GraphViewer& graph_viewer) const {
+  ORT_RETURN_IF_NOT_SUPPORTED(EtGlowDefaultNodeCapability::IsDimensionSupported(node, graph_viewer));
+
+  auto node_inputs = node.InputDefs();
+  if (const auto* X = node_inputs[0]; !graph_viewer.IsConstantInitializer(X->Name(), true)) {
+    return make_unsupported(X->Name() + " must be a constant");
+  }
+  return make_supported();
+}
+
+////////////////////
+
+EtGlowNodeSupportResult EtGlowPadNodeCapability::Supported(const Node& node, const GraphViewer& graph_viewer) const {
+  ORT_RETURN_IF_NOT_SUPPORTED(IsTypeSupported(node));
+  ORT_RETURN_IF_NOT_SUPPORTED(IsDimensionSupported(node, graph_viewer));
+  ORT_RETURN_IF_NOT_SUPPORTED(IsAttributeSupported(node, graph_viewer));
+  return make_supported();
+}
+
+EtGlowNodeSupportResult EtGlowPadNodeCapability::IsDimensionSupported(const Node& node, const GraphViewer& graph_viewer) const {
+  ORT_RETURN_IF_NOT_SUPPORTED(EtGlowDefaultNodeCapability::IsDimensionSupported(node, graph_viewer));
+
+  auto node_inputs = node.InputDefs();
+  const auto* input = node_inputs[0];
+  ORT_RETURN_IF_NOT_SUPPORTED(HasNonZeroPositiveValueDimsSafe(input, GetParametricShapeReplacements(), GetCapabilityMode()));
+
+  if (auto opset = node.SinceVersion(); opset >= 11_opset) {  // NOLINT(cppcoreguidelines-avoid-magic-numbers,readability-magic-numbers)
+    if (node_inputs.size() < 2) {
+      return make_unsupported("input 'pads' is mandatory");
+    }
+    const auto* pads = node_inputs[1];
+    if (!graph_viewer.IsConstantInitializer(pads->Name(), true)) {
+      return make_unsupported("input 'pads' must be a constant");
+    }
+    if (pads->TypeAsProto()->tensor_type().elem_type() != type_int64) {
+      return make_unsupported("pads element_type is not int64");
+    }
+    ORT_RETURN_IF_NOT_SUPPORTED(HasNonZeroPositiveValueDimsSafe(pads, GetParametricShapeReplacements(), GetCapabilityMode()));
+    if (HasShape(pads) && HasShape(input)) {
+      const auto pads_dim_size = pads->Shape()->dim_size();
+      const auto& input_shape = *input->Shape();
+      const auto input_dim_size = input_shape.dim_size();
+      ORT_RETURN_IF_NOT_SUPPORTED(HasValidPadsShape(pads_dim_size, input_dim_size));
+
+      const ONNX_NAMESPACE::TensorProto* tensor_proto = nullptr;
+      graph_viewer.GetInitializedTensor(pads->Name(), tensor_proto);
+      if (pads_dim_size != tensor_proto->dims_size()) {
+        return make_unsupported("pads dims size do not match with raw TensorProto");
+      }
+      if (tensor_proto != nullptr) {
+        const auto* pad_values = reinterpret_cast<const int64_t*>(tensor_proto->raw_data().data());
+        ORT_RETURN_IF_NOT_SUPPORTED(HasValidPadsValues(pads_dim_size, pad_values, *input));
+      }
+    }
+
+    if (node_inputs.size() >= 3) {
+      const auto* constant_value = node_inputs[2];
+      if (!graph_viewer.IsConstantInitializer(constant_value->Name(), true)) {
+        return make_unsupported("constant_value must be a constant");
+      }
+      auto constant_value_elem_type = static_cast<ORT_DataType>(constant_value->TypeAsProto()->tensor_type().elem_type());
+      const auto supported_elem_types = {type_int64, type_int32, type_float16, type_float32};
+      if (bool is_supported_type = std::find(supported_elem_types.begin(), supported_elem_types.end(), constant_value_elem_type) != supported_elem_types.end(); !is_supported_type) {
+        return make_unsupported("constant_value elem_type: " + std::to_string(constant_value_elem_type) + " is not supported by glow.");
+      }
+    }
+  }
+
+  return make_supported();
+}
+
+EtGlowNodeSupportResult EtGlowPadNodeCapability::IsAttributeSupported(const Node& node, const GraphViewer& graph_viewer) const {
+  ORT_UNUSED_PARAMETER(graph_viewer);
+  const NodeAttributes& attributes = node.GetAttributes();
+  if (auto attr_mode = attributes.find("mode"); attr_mode != attributes.end()) {
+    auto attr_mode_value = attr_mode->second().s();
+    if (!(attr_mode_value == "constant" || attr_mode_value == "reflect" || attr_mode_value == "edge")) {
+      return make_unsupported("unsupported 'pad' mode: " + attr_mode_value + " in (glow frontend)");
+    }
+    if (attr_mode_value != "constant") {
+      return make_unsupported("unsupported 'pad' mode: " + attr_mode_value + " in (glow graph)");
+    }
+  }
+  if (auto opset = node.SinceVersion(); opset < 11_opset) {  // NOLINT(cppcoreguidelines-avoid-magic-numbers,readability-magic-numbers)
+    auto attr_pads = attributes.find("pads");
+    if (!(attr_pads != attributes.end()) || (attr_pads->second().type() != ONNX_NAMESPACE::AttributeProto_AttributeType::AttributeProto_AttributeType_INTS)) {
+      return make_unsupported("if old opset, 'pads' (INTS) attribute must be present");
+    }
+    const auto& attr_pads_values = attr_pads->second().ints();
+    const auto& input = *node.InputDefs()[0];
+    const auto pads_dim_size = attr_pads_values.size();
+    const auto input_dim_size = input.Shape()->dim_size();
+    ORT_RETURN_IF_NOT_SUPPORTED(HasValidPadsShape(pads_dim_size, input_dim_size));
+    ORT_RETURN_IF_NOT_SUPPORTED(HasValidPadsValues(pads_dim_size, attr_pads_values, input));
+
+    ////
+    if (auto attr_value = attributes.find("value"); !(attr_value != attributes.end())) {
+      return make_unsupported("if old opset, value attribute must be present");
+    }
+  }
+  return make_supported();
+}
+
+EtGlowNodeSupportResult EtGlowPadNodeCapability::HasValidPadsShape(int pads_dim_size, int input_dim_size) const {
+  if (pads_dim_size != 2 * input_dim_size) {
+    return make_unsupported(
+        "pads array must contain 2 values per dimensions. "
+        "Found input_dim_size: " +
+        std::to_string(pads_dim_size) +
+        " input_dim_size: " + std::to_string(input_dim_size));
+  }
+  return make_supported();
+}
+
+////////////////////
+
+EtGlowPowNodeCapability::EtGlowPowNodeCapability(CapabilityMode mode, const ONNXSymbolsShapes& user_parametric_shapes_replacements)
+    : EtGlowDefaultNodeCapability(mode, user_parametric_shapes_replacements,
+                                  std::vector<ORT_DataType>{type_float32, type_float16},
+                                  std::vector<ORT_DataType>{type_float32, type_float16}) {
+  // intentionally empty
+}
+
+EtGlowNodeSupportResult EtGlowPowNodeCapability::Supported(const Node& node, const GraphViewer& graph_viewer) const {
+  ORT_RETURN_IF_NOT_SUPPORTED(IsTypeSupported(node));
+  ORT_RETURN_IF_NOT_SUPPORTED(IsDimensionSupported(node, graph_viewer));
+  return make_supported();
+}
+
+EtGlowNodeSupportResult EtGlowPowNodeCapability::IsTypeSupported(const Node& node) const {
+  ORT_RETURN_IF_NOT_SUPPORTED(EtGlowDefaultNodeCapability::IsTypeSupported(node));
+
+  auto node_inputs = node.InputDefs();
+  if (const auto* Y = node_inputs[1]; Y->TypeAsProto() != nullptr) {
+    const auto node_datatype = static_cast<ORT_DataType>(Y->TypeAsProto()->tensor_type().elem_type());
+    const auto& supported_types = GetSupportedInputTypes();
+    if (bool is_supported_type = std::find(supported_types.begin(), supported_types.end(), node_datatype) != supported_types.end(); !is_supported_type) {
+      return make_unsupported("input[1] (" + Y->Name() + ") is of type: " + std::to_string(node_datatype) + " which is not in the list of supported types.");
+    }
+  }
+  return make_supported();
+}
+
+////////////////////
+
+EtGlowQuantizeLinearNodeCapability::EtGlowQuantizeLinearNodeCapability(CapabilityMode mode, const ONNXSymbolsShapes& user_parametric_shapes_replacements)
+    : EtGlowDefaultNodeCapability(mode, user_parametric_shapes_replacements,
+                                  std::vector<ORT_DataType>{type_float32, type_int32},
+                                  std::vector<ORT_DataType>{type_int8, type_uint8}) {
+  // intentionally empty
+}
+
+EtGlowNodeSupportResult EtGlowQuantizeLinearNodeCapability::Supported(const Node& node, const GraphViewer& graph_viewer) const {
+  ORT_RETURN_IF_NOT_SUPPORTED(IsTypeSupported(node));
+  ORT_RETURN_IF_NOT_SUPPORTED(IsDimensionSupported(node, graph_viewer));
+  return make_supported();
+}
+
+EtGlowNodeSupportResult EtGlowQuantizeLinearNodeCapability::IsTypeSupported(const Node& node) const {
+  auto node_inputs = node.InputDefs();
+  if (!node_inputs.empty() && node_inputs[0]->TypeAsProto() != nullptr) {
+    const auto* X = node_inputs[0];
+    const auto node_datatype = static_cast<ORT_DataType>(X->TypeAsProto()->tensor_type().elem_type());
+    const auto& supported_types = GetSupportedT1Types();
+    if (bool is_supported_in_type = std::find(supported_types.begin(), supported_types.end(), node_datatype) != supported_types.end(); !is_supported_in_type) {
+      return make_unsupported("input[0] (" + X->Name() + ") is of type: " + std::to_string(node_datatype) + " which is not in the list of supported types.");
+    }
+  }
+  if (node_inputs.size() >= 3) {
+    if (const auto* y_zero_point = node_inputs[2]; y_zero_point->TypeAsProto() != nullptr) {
+      const auto node_datatype = static_cast<ORT_DataType>(y_zero_point->TypeAsProto()->tensor_type().elem_type());
+      const auto& supported_types = GetSupportedT2Types();
+      if (bool is_supported_type = std::find(supported_types.begin(), supported_types.end(), node_datatype) != supported_types.end(); !is_supported_type) {
+        return make_unsupported("input[2] (" + y_zero_point->Name() + ") is of type: " + std::to_string(node_datatype) + " which is not in the list of supported types.");
+      }
+    }
+  }
+  return make_supported();
+}
+
+EtGlowNodeSupportResult EtGlowQuantizeLinearNodeCapability::IsDimensionSupported(const Node& node, const GraphViewer& graph_viewer) const {
+  ORT_RETURN_IF_NOT_SUPPORTED(EtGlowDefaultNodeCapability::IsDimensionSupported(node, graph_viewer));
+  auto node_inputs = node.InputDefs();
+  ORT_RETURN_IF_NOT_SUPPORTED(IsScalarConstantSafe(node_inputs[1], "y_scale", graph_viewer, GetCapabilityMode()));
+  if (node_inputs.size() >= 3) {
+    ORT_RETURN_IF_NOT_SUPPORTED(IsScalarConstantSafe(node_inputs[2], "y_zero_point", graph_viewer, GetCapabilityMode()));
+  }
+  return make_supported();
+}
+
+////////////////////
+
+EtGlowNodeSupportResult EtGlowPReluNodeCapability::Supported(const Node& node, const GraphViewer& graph_viewer) const {
+  ORT_RETURN_IF_NOT_SUPPORTED(IsTypeSupported(node));
+  ORT_RETURN_IF_NOT_SUPPORTED(IsDimensionSupported(node, graph_viewer));
+  return make_supported();
+}
+
+EtGlowNodeSupportResult EtGlowPReluNodeCapability::IsDimensionSupported(const Node& node, const GraphViewer& graph_viewer) const {
+  ORT_RETURN_IF_NOT_SUPPORTED(EtGlowDefaultNodeCapability::IsDimensionSupported(node, graph_viewer));
+  return make_supported();
+}
+
+////////////////////
+
+EtGlowNodeSupportResult EtGlowRNNNodeCapability::Supported(const Node& node, const GraphViewer& graph_viewer) const {
+  ORT_RETURN_IF_NOT_SUPPORTED(IsTypeSupported(node));
+  ORT_RETURN_IF_NOT_SUPPORTED(IsDimensionSupported(node, graph_viewer));
+  ORT_RETURN_IF_NOT_SUPPORTED(IsAttributeSupported(node, graph_viewer));
+  return make_supported();
+}
+
+EtGlowNodeSupportResult EtGlowRNNNodeCapability::IsAttributeSupported(const Node& node, const GraphViewer& graph_viewer) const {
+  ORT_RETURN_IF_NOT_SUPPORTED(EtGlowOneLayerNodeCapability::IsAttributeSupported(node, graph_viewer));
+  return make_supported();
+}
+
+////////////////////
+
+EtGlowRangeNodeCapability::EtGlowRangeNodeCapability(CapabilityMode mode, const ONNXSymbolsShapes& user_parametric_shapes_replacements)
+    : EtGlowDefaultNodeCapability(mode, user_parametric_shapes_replacements,
+                                  std::vector<ORT_DataType>{type_int64, type_int32, type_float32}) {
+  // intentionally empty
+}
+
+EtGlowNodeSupportResult EtGlowRangeNodeCapability::Supported(const Node& node, const GraphViewer& graph_viewer) const {
+  ORT_RETURN_IF_NOT_SUPPORTED(IsTypeSupported(node));
+  ORT_RETURN_IF_NOT_SUPPORTED(IsDimensionSupported(node, graph_viewer));
+  return make_supported();
+}
+
+EtGlowNodeSupportResult EtGlowRangeNodeCapability::IsDimensionSupported(const Node& node, const GraphViewer& graph_viewer) const {
+  ORT_RETURN_IF_NOT_SUPPORTED(EtGlowDefaultNodeCapability::IsDimensionSupported(node, graph_viewer));
+
+  auto node_inputs = node.InputDefs();
+  ORT_RETURN_IF_NOT_SUPPORTED(IsScalarConstantSafe(node_inputs[0], "start", graph_viewer, GetCapabilityMode()));
+  ORT_RETURN_IF_NOT_SUPPORTED(IsScalarConstantSafe(node_inputs[1], "limit", graph_viewer, GetCapabilityMode()));
+  ORT_RETURN_IF_NOT_SUPPORTED(IsScalarConstantSafe(node_inputs[2], "delta", graph_viewer, GetCapabilityMode()));
+
+  return make_supported();
+}
+
+////////////////////
+
+EtGlowReduceNodeCapability::EtGlowReduceNodeCapability(CapabilityMode mode, const ONNXSymbolsShapes& user_parametric_shapes_replacements)
+    : EtGlowReduceNodeCapability(mode, user_parametric_shapes_replacements, GetSupportedTTypes()) {
+  // intentionally empty
+}
+
+EtGlowReduceNodeCapability::EtGlowReduceNodeCapability(CapabilityMode mode, const ONNXSymbolsShapes& user_parametric_shapes_replacements, const std::vector<ORT_DataType>& supported_types)
+    : EtGlowDefaultNodeCapability(mode, user_parametric_shapes_replacements,
+                                  supported_types /* in */,
+                                  supported_types /* out */) {
+  // intentionally empty
+}
+
+EtGlowNodeSupportResult EtGlowReduceNodeCapability::Supported(const Node& node, const GraphViewer& graph_viewer) const {
+  ORT_RETURN_IF_NOT_SUPPORTED(IsTypeSupported(node));
+  ORT_RETURN_IF_NOT_SUPPORTED(IsDimensionSupported(node, graph_viewer));
+  ORT_RETURN_IF_NOT_SUPPORTED(IsAttributeSupported(node, graph_viewer));
+  return make_supported();
+}
+
+EtGlowNodeSupportResult EtGlowReduceNodeCapability::IsDimensionSupported(const Node& node, const GraphViewer& graph_viewer) const {
+  ORT_RETURN_IF_NOT_SUPPORTED(EtGlowDefaultNodeCapability::IsDimensionSupported(node, graph_viewer));
+
+  auto node_inputs = node.InputDefs();
+  if (node_inputs.size() > 1) {
+    return make_unsupported("'axes' as input tensor is not supported. glow only handles 'axes' as attribute or deduced from first input dims.");
+  }
+
+  const auto* data = node_inputs[0];
+  ORT_RETURN_IF_NOT_SUPPORTED(HasNonZeroPositiveValueDimsSafe(data, GetParametricShapeReplacements(), GetCapabilityMode()));
+
+  const NodeAttributes& attributes = node.GetAttributes();
+  if (auto attr_axes = attributes.find("axes"); !(attr_axes != attributes.end()) && node_inputs.size() < 2) {
+    // if 'axes' is not provided as a tensor nor as an attribute
+    // -> glow tries to deduce an axes from the 'data' input.
+
+    ORT_RETURN_IF_NO_SHAPE(data, GetCapabilityMode());
+    if (const bool is1Dtensor = IsScalar(data) && !HasEmptyShape(data->Shape()); !is1Dtensor) {
+      return make_unsupported("axes from deduced 'data' shape must be a non-zero 1-d tensor");
+    }
+  }
+
+  return make_supported();
+}
+
+EtGlowNodeSupportResult EtGlowReduceNodeCapability::IsAttributeSupported(const Node& node, const GraphViewer& graph_viewer) const {
+  ORT_UNUSED_PARAMETER(graph_viewer);
+  const NodeAttributes& attributes = node.GetAttributes();
+  if (auto attr_axes = attributes.find("axes"); attr_axes != attributes.end()) {
+    const auto& attr_axes_values = attr_axes->second().ints();
+    if (attr_axes_values.size() == 0) {
+      return make_unsupported("glow requires 'axes' to not be empty");
+    }
+
+    std::vector<int64_t> axes_dim_values;
+    axes_dim_values.reserve(attr_axes_values.size());
+    for (int i = 0; i < attr_axes_values.size(); ++i) {
+      axes_dim_values.emplace_back(attr_axes_values.Get(i));
+    }
+    std::sort(axes_dim_values.begin(), axes_dim_values.end());
+    if (auto it = std::unique(axes_dim_values.begin(), axes_dim_values.end()); it != axes_dim_values.end()) {
+      return make_unsupported("glow requires axes dim values to be unique");
+    }
+    // for ReduceSum or those that lower to BatchReduceAdd, axis from deduced 'data' shape must be 1-d
+    if (axes_dim_values.size() != 1) {
+      return make_unsupported("glow expects axes to be 1-dimensional");
+    }
+  }
+  if (auto attr_noop_with_empty_axes = attributes.find("noop_with_empty_axes"); attr_noop_with_empty_axes != attributes.end()) {
+    const auto& attr_noop_with_empty_axes_value = attr_noop_with_empty_axes->second().i();
+    const int64_t attr_noop_with_empty_axes_default_value = 0;
+    if (attr_noop_with_empty_axes_value != attr_noop_with_empty_axes_default_value) {
+      return make_unsupported("Detected 'noop_with_empty_axes' with value: " + std::to_string(attr_noop_with_empty_axes_value) + " but glow does not handle noop_with_empty_axes attribute.");
+    }
+  }
+  return make_supported();
+}
+
+////////////////////
+
+EtGlowReduceL2NodeCapability::EtGlowReduceL2NodeCapability(CapabilityMode mode, const ONNXSymbolsShapes& user_parametric_shapes_replacements)
+    : EtGlowReduceNodeCapability(mode, user_parametric_shapes_replacements,
+                                 std::vector<ORT_DataType>{type_float32}) {}
+
+EtGlowNodeSupportResult EtGlowReduceL2NodeCapability::Supported(const Node& node, const GraphViewer& graph_viewer) const {
+  ORT_RETURN_IF_NOT_SUPPORTED(IsTypeSupported(node));
+  ORT_RETURN_IF_NOT_SUPPORTED(IsDimensionSupported(node, graph_viewer));
+  ORT_RETURN_IF_NOT_SUPPORTED(IsAttributeSupported(node, graph_viewer));
+  return make_supported();
+}
+
+EtGlowNodeSupportResult EtGlowReduceL2NodeCapability::IsAttributeSupported(const Node& node, const GraphViewer& graph_viewer) const {
+  ORT_UNUSED_PARAMETER(graph_viewer);
+  const NodeAttributes& attributes = node.GetAttributes();
+  if (auto attr_noop_with_empty_axes = attributes.find("noop_with_empty_axes"); attr_noop_with_empty_axes != attributes.end()) {
+    return make_unsupported("attr noop_with_empty_axes is not supported");
+  }
+  return make_supported();
+}
+
+////////////////////
+
+EtGlowNodeSupportResult EtGlowReshapeNodeCapability::Supported(const Node& node, const GraphViewer& graph_viewer) const {
+  ORT_RETURN_IF_NOT_SUPPORTED(IsTypeSupported(node));
+  ORT_RETURN_IF_NOT_SUPPORTED(IsDimensionSupported(node, graph_viewer));
+  ORT_RETURN_IF_NOT_SUPPORTED(IsAttributeSupported(node, graph_viewer));
+  return make_supported();
+}
+
+EtGlowNodeSupportResult EtGlowReshapeNodeCapability::IsDimensionSupported(const Node& node, const GraphViewer& graph_viewer) const {
+  ORT_RETURN_IF_NOT_SUPPORTED(EtGlowDefaultNodeCapability::IsDimensionSupported(node, graph_viewer));
+
+  if (auto node_inputs = node.InputDefs(); node_inputs.size() > 1) {
+    // if Reshape has 'shape' as second input
+    const auto* shape = node_inputs[1];
+    if (!graph_viewer.IsConstantInitializer(shape->Name(), true)) {
+      // in theory, it has to be constant
+      return make_unsupported(shape->Name() + " must be a constant");
+    }
+    auto constant_value_elem_type = shape->TypeAsProto()->tensor_type().elem_type();
+    if (constant_value_elem_type != type_int64) {
+      return make_unsupported("shape - must be a tensor(int64)");
+    }
+  }
+
+  return make_supported();
+}
+
+EtGlowNodeSupportResult EtGlowReshapeNodeCapability::IsAttributeSupported(const Node& node, const GraphViewer& graph_viewer) const {
+  ORT_UNUSED_PARAMETER(graph_viewer);
+  const NodeAttributes& attributes = node.GetAttributes();
+  if (auto attr_shape = attributes.find("allowzero"); attr_shape != attributes.end()) {
+    return make_unsupported("glow doesn't support Reshape with attr 'allowzero'");
+  }
+  return make_supported();
+}
+
+////////////////////
+
+EtGlowNodeSupportResult EtGlowResizeNodeCapability::Supported(const Node& node, const GraphViewer& graph_viewer) const {
+  ORT_RETURN_IF_NOT_SUPPORTED(IsTypeSupported(node));
+  ORT_RETURN_IF_NOT_SUPPORTED(IsDimensionSupported(node, graph_viewer));
+  ORT_RETURN_IF_NOT_SUPPORTED(IsAttributeSupported(node, graph_viewer));
+  return make_supported();
+}
+
+EtGlowNodeSupportResult EtGlowResizeNodeCapability::IsDimensionSupported(const Node& node, const GraphViewer& graph_viewer) const {
+  ORT_RETURN_IF_NOT_SUPPORTED(EtGlowDefaultNodeCapability::IsDimensionSupported(node, graph_viewer));
+
+  auto node_inputs = node.InputDefs();
+  const size_t scales_idx = node.SinceVersion() >= 11 ? 2 : 1;
+  if (node_inputs.size() < scales_idx) {
+    return make_unsupported("required input 'scales' is missing");
+  }
+  // if Reshape has 'shape' as second input
+  const auto* scales = node_inputs[scales_idx];
+  if (!graph_viewer.IsConstantInitializer(scales->Name(), true)) {
+    return make_unsupported(scales->Name() + " must be a constant");
+  }
+  if (auto constant_value_elem_type = scales->TypeAsProto()->tensor_type().elem_type(); constant_value_elem_type != type_float32) {
+    return make_unsupported("shape - must be a tensor(float32)");
+  }
+  return make_supported();
+}
+
+EtGlowNodeSupportResult EtGlowResizeNodeCapability::IsAttributeSupported(const Node& node, const GraphViewer& graph_viewer) const {
+  ORT_UNUSED_PARAMETER(graph_viewer);
+  const NodeAttributes& attributes = node.GetAttributes();
+  auto attr_mode = attributes.find("mode");
+  if (!(attr_mode != attributes.end())) {
+    return make_unsupported("attr 'mode' is required");
+  }
+  if (attr_mode->second().type() != ONNX_NAMESPACE::AttributeProto_AttributeType::AttributeProto_AttributeType_STRING) {
+    return make_unsupported("glow expects 'attr_mode' to be STRING");
+  }
+  if (auto attr_exclude_outside = attributes.find("exclude_outside"); attr_exclude_outside != attributes.end()) {
+    // if attr_exclude_outside is present
+    if (attr_exclude_outside->second().type() != ONNX_NAMESPACE::AttributeProto_AttributeType::AttributeProto_AttributeType_INT) {
+      return make_unsupported("glow expects 'exclude_outside' to be INT");
+    }
+    if (attr_exclude_outside->second().i() != 0) {
+      return make_unsupported("glow does not support exclude_outside!=0");
+    }
+  }
+  if (auto attr_extrapolation_value = attributes.find("extrapolation_value"); attr_extrapolation_value != attributes.end()) {
+    // if attr_exclude_outside is present
+    if (attr_extrapolation_value->second().type() != ONNX_NAMESPACE::AttributeProto_AttributeType::AttributeProto_AttributeType_FLOAT) {
+      return make_unsupported("glow expects 'exclude_outside' to be FLOAT");
+    }
+    if (attr_extrapolation_value->second().f() != 0.0) {
+      return make_unsupported("glow does not support extrapolation_value!=0.0");
+    }
+  }
+  if (auto attr_nearest_mode = attributes.find("nearest_mode"); attr_nearest_mode != attributes.end()) {
+    // if attr_exclude_outside is present
+    if (attr_nearest_mode->second().type() != ONNX_NAMESPACE::AttributeProto_AttributeType::AttributeProto_AttributeType_STRING) {
+      return make_unsupported("glow expects 'exclude_outside' to be STRING");
+    }
+    const auto attr_nearest_value = attr_nearest_mode->second().s();
+    if (attr_mode->second().s() == "nearest" && attr_nearest_value != "floor") {
+      return make_unsupported(node.OpType() + " if 'mode=nearest', only 'nearest_mode=floor' is supported. Found: " + attr_nearest_value);
+    }
+  }
+  if (auto attr_coordinate_transformation_mode = attributes.find("coordinate_transformation_mode"); attr_coordinate_transformation_mode != attributes.end()) {
+    if (attr_coordinate_transformation_mode->second().type() != ONNX_NAMESPACE::AttributeProto_AttributeType::AttributeProto_AttributeType_STRING) {
+      return make_unsupported("glow expects 'coordinate_transformation_mode' to be STRING");
+    }
+    const auto attr_coordinate_transformation_mode_value = attr_coordinate_transformation_mode->second().s();
+    if (attr_coordinate_transformation_mode_value != "asymmetric") {
+      return make_unsupported(node.OpType() + " only 'coordinate_transformation_mode_value=asymmetric' is supported. Found: " + attr_coordinate_transformation_mode_value);
+    }
+  }
+  return make_supported();
+}
+
+////////////////////
+
+EtGlowNodeSupportResult EtGlowScatterNDNodeCapability::Supported(const Node& node, const GraphViewer& graph_viewer) const {
+  ORT_RETURN_IF_NOT_SUPPORTED(IsTypeSupported(node));
+  ORT_RETURN_IF_NOT_SUPPORTED(IsDimensionSupported(node, graph_viewer));
+  ORT_RETURN_IF_NOT_SUPPORTED(IsAttributeSupported(node, graph_viewer));
+  return make_supported();
+}
+
+EtGlowNodeSupportResult EtGlowScatterNDNodeCapability::IsAttributeSupported(const Node& node, const GraphViewer& graph_viewer) const {
+  ORT_UNUSED_PARAMETER(graph_viewer);
+  const NodeAttributes& attributes = node.GetAttributes();
+
+  if (auto attr_reduction = attributes.find("reduction"); attr_reduction != attributes.end()) {
+    if (attr_reduction->second().type() != ONNX_NAMESPACE::AttributeProto_AttributeType::AttributeProto_AttributeType_STRING) {
+      return make_unsupported("attr reductions should be a STRING");
+    }
+    if (auto attr_reduction_value = attr_reduction->second().s(); attr_reduction_value != "none") {
+      return make_unsupported(node.OpType() + " operator has attr reduction: " + attr_reduction_value + " but glow only supports 'none' (does not support reductions).");
+    }
+  }
+  return make_supported();
+}
+
+////////////////////
+
+EtGlowNodeSupportResult EtGlowShapeNodeCapability::Supported(const Node& node, const GraphViewer& graph_viewer) const {
+  ORT_RETURN_IF_NOT_SUPPORTED(IsTypeSupported(node));
+  ORT_RETURN_IF_NOT_SUPPORTED(IsDimensionSupported(node, graph_viewer));
+  return make_supported();
+}
+
+EtGlowNodeSupportResult EtGlowShapeNodeCapability::IsDimensionSupported(const Node& node, const GraphViewer& graph_viewer) const {
+  ORT_RETURN_IF_NOT_SUPPORTED(EtGlowDefaultNodeCapability::IsDimensionSupported(node, graph_viewer));
+
+  auto node_inputs = node.InputDefs();
+  if (const auto* data = node_inputs[0]; HasShape(data)) {
+    for (const auto& dim : data->Shape()->dim()) {
+      ORT_RETURN_IF_NOT_SUPPORTED(HasValueForParametricDim(dim, GetParametricShapeReplacements(), data->Name()));
+    }
+  }
+  return make_supported();
+}
+
+////////////////////
+
+EtGlowNodeSupportResult EtGlowSimplifiedLayerNormalizationNodeCapability::Supported(const Node& node, const GraphViewer& graph_viewer) const {
+  ORT_RETURN_IF_NOT_SUPPORTED(IsTypeSupported(node));
+  ORT_RETURN_IF_NOT_SUPPORTED(IsDimensionSupported(node, graph_viewer));
+  ORT_RETURN_IF_NOT_SUPPORTED(IsAttributeSupported(node, graph_viewer));
+  return make_supported();
+}
+
+EtGlowNodeSupportResult EtGlowSimplifiedLayerNormalizationNodeCapability::IsDimensionSupported(const Node& node, const GraphViewer& graph_viewer) const {
+  ORT_RETURN_IF_NOT_SUPPORTED(EtGlowDefaultNodeCapability::IsDimensionSupported(node, graph_viewer));
+
+  auto node_inputs = node.InputDefs();
+
+  if (node_inputs.size() < 2) {
+    return make_unsupported("must have at least input and scale");
+  }
+  const auto* x = node_inputs[0];
+  const auto* scale = node_inputs[1];
+
+  auto x_elem_type = x->TypeAsProto()->tensor_type().elem_type();
+  if (auto scale_elem_type = scale->TypeAsProto()->tensor_type().elem_type(); x_elem_type != scale_elem_type) {
+    return make_unsupported("x and scale must have same types");
+  }
+
+  if (node_inputs.size() > 2) {
+    return make_unsupported("invalid number of inputs");
+  }
+
+  if (auto node_outputs = node.OutputDefs(); node_outputs.size() > 1) {
+    return make_unsupported("only 1 output is supported");
+  }
+
+  return make_supported();
+}
+
+EtGlowNodeSupportResult EtGlowSimplifiedLayerNormalizationNodeCapability::IsAttributeSupported(const Node& node, const GraphViewer& graph_viewer) const {
+  ORT_UNUSED_PARAMETER(graph_viewer);
+  const NodeAttributes& attributes = node.GetAttributes();
+  if (auto epsilon = attributes.find("epsilon"); epsilon != attributes.end() && epsilon->second().type() != ONNX_NAMESPACE::AttributeProto_AttributeType_FLOAT) {
+    return make_unsupported("attr 'epsilon' must be of type FLOAT");
+  }
+  return make_supported();
+}
+
+////////////////////
+
+EtGlowNodeSupportResult EtGlowSliceNodeCapability::Supported(const Node& node, const GraphViewer& graph_viewer) const {
+  ORT_RETURN_IF_NOT_SUPPORTED(IsTypeSupported(node));
+  ORT_RETURN_IF_NOT_SUPPORTED(IsDimensionSupported(node, graph_viewer));
+  ORT_RETURN_IF_NOT_SUPPORTED(IsAttributeSupported(node, graph_viewer));
+  return make_supported();
+}
+
+template <typename T>
+EtGlowNodeSupportResult HasValidStartEndRange(const T& start_values, const T& end_values, const int num_values, int64_t lower_bound, int64_t upper_bound) {
+  for (int i = 0; i < num_values; ++i) {
+    const auto start = start_values[i];
+    const auto end = end_values[i];
+    if (start > end) {
+      return make_unsupported("invalid start/end values at idx: " + std::to_string(i) +
+                              " start: " + std::to_string(start) + " > " +
+                              " end:" + std::to_string(end) + " !");
+    } else if (start == end) {
+      return make_unsupported("neuralizer does not accept start & ends being equal. (start: " + std::to_string(start) + " end: " + std::to_string(end) + ")");
+    } else if (start > upper_bound) {
+      return make_unsupported("start is out of bounds (start: " + std::to_string(start) + " end: " + std::to_string(end) + ")");
+    } else if (end > upper_bound || end < lower_bound) {
+      return make_unsupported("end is out of bounds (start: " + std::to_string(start) + " end: " + std::to_string(end) + ")");
+    }
+  }
+  return make_supported();
+}
+
+EtGlowNodeSupportResult EtGlowSliceNodeCapability::IsDimensionSupported(const Node& node, const GraphViewer& graph_viewer) const {
+  ORT_RETURN_IF_NOT_SUPPORTED(EtGlowDefaultNodeCapability::IsDimensionSupported(node, graph_viewer));
+
+  const auto node_inputs = node.InputDefs();
+  if (auto opset = node.SinceVersion(); opset >= 10) {
+    if (node_inputs.size() < 3) {
+      return make_unsupported("for opset >= 10, data, starts, ends tensors are mandatory");
+    }
+    const auto* data = node_inputs[0];
+    const auto* starts = node_inputs[1];
+    const auto* ends = node_inputs[2];
+    ORT_RETURN_IF_NOT_SUPPORTED(HasNonZeroPositiveValueDimsSafe(data, GetParametricShapeReplacements(), GetCapabilityMode()));
+    ORT_RETURN_IF_NOT_SUPPORTED(HasNonZeroPositiveValueDimsSafe(starts, GetParametricShapeReplacements(), GetCapabilityMode()));
+    ORT_RETURN_IF_NOT_SUPPORTED(HasNonZeroPositiveValueDimsSafe(ends, GetParametricShapeReplacements(), GetCapabilityMode()));
+
+    auto is_1D_tensor = [this](const NodeArg* node_arg) {
+      ORT_RETURN_IF_NO_SHAPE(node_arg, GetCapabilityMode());
+      if (const onnx::TensorShapeProto& shape_proto = *node_arg->Shape(); shape_proto.dim_size() != 1) {
+        return make_unsupported(node_arg->Name() + " is not a 1D tensor");
+      }
+      return make_supported();
+    };
+    ORT_RETURN_IF_NOT_SUPPORTED(is_1D_tensor(starts));
+    ORT_RETURN_IF_NOT_SUPPORTED(is_1D_tensor(ends));
+
+    if (!graph_viewer.IsConstantInitializer(starts->Name(), true)) {
+      return make_unsupported("glow only supports 'starts' as a constant");
+    }
+    if (!graph_viewer.IsConstantInitializer(ends->Name(), true) && !SkipShapeChecks(GetCapabilityMode())) {
+      return make_unsupported("glow only supports 'ends' as a constant");
+    }
+
+    const auto& supported_types = GetSupportedTindTypes();
+    auto HasSuportedTindType = [&supported_types](ORT_DataType elem_type, const std::string& name) {
+      if (bool is_supported_type = std::find(supported_types.begin(), supported_types.end(), elem_type) != supported_types.end(); !is_supported_type) {
+        return make_unsupported(name + " is of type: " + std::to_string(elem_type) + " which is not in the list of supported types.");
+      }
+      return make_supported();
+    };
+    ORT_RETURN_IF_NOT_SUPPORTED(HasSuportedTindType(static_cast<ORT_DataType>(starts->TypeAsProto()->tensor_type().elem_type()), starts->Name()));
+    ORT_RETURN_IF_NOT_SUPPORTED(HasSuportedTindType(static_cast<ORT_DataType>(ends->TypeAsProto()->tensor_type().elem_type()), ends->Name()));
+
+    const ONNX_NAMESPACE::TensorProto* tensor_proto_starts = nullptr;
+    graph_viewer.GetInitializedTensor(starts->Name(), tensor_proto_starts);
+
+    const ONNX_NAMESPACE::TensorProto* tensor_proto_ends = nullptr;
+    graph_viewer.GetInitializedTensor(ends->Name(), tensor_proto_ends);
+
+    if (tensor_proto_starts != nullptr && tensor_proto_ends != nullptr && HasShape(data)) {
+      const auto* starts_values = reinterpret_cast<const int64_t*>(tensor_proto_starts->raw_data().data());
+      const auto* ends_values = reinterpret_cast<const int64_t*>(tensor_proto_ends->raw_data().data());
+      if (tensor_proto_ends->dims_size() != tensor_proto_starts->dims_size()) {
+        return make_unsupported("attr starts and ends have different sizes!");
+      }
+      const auto input_dims = data->Shape()->dim_size();
+      ORT_RETURN_IF_NOT_SUPPORTED(HasValidStartEndRange(starts_values, ends_values, tensor_proto_starts->dims_size(), 0, input_dims));
+    }
+
+    if (node_inputs.size() >= 4) {
+      const auto* steps = node_inputs[3];
+      ORT_RETURN_IF_NOT_SUPPORTED(is_1D_tensor(steps));
+      if (!graph_viewer.IsConstantInitializer(steps->Name(), true)) {
+        return make_unsupported("'steps' is not constant, we cannot check glow requirements");
+      }
+      ORT_RETURN_IF_NOT_SUPPORTED(HasSuportedTindType(static_cast<ORT_DataType>(steps->TypeAsProto()->tensor_type().elem_type()), steps->Name()));
+
+      const ONNX_NAMESPACE::TensorProto* tensor_proto_steps = nullptr;
+      graph_viewer.GetInitializedTensor(steps->Name(), tensor_proto_steps);
+      if (tensor_proto_steps != nullptr && HasShape(steps)) {
+        const auto* steps_values = reinterpret_cast<const int64_t*>(tensor_proto_steps->raw_data().data());
+        for (int i = 0; i < steps->Shape()->dim_size(); ++i) {
+          const auto steps_value = steps_values[i];
+          if (steps_value != 1) {
+            return make_unsupported("glow does not support anything other than 'steps==1'. Found: " + std::to_string(steps_value) + " on dim: " + std::to_string(i));
+          }
+        }
+      }
+    }
+  }
+  return make_supported();
+}
+
+EtGlowNodeSupportResult EtGlowSliceNodeCapability::IsAttributeSupported(const Node& node, const GraphViewer& graph_viewer) const {
+  ORT_UNUSED_PARAMETER(graph_viewer);
+  const NodeAttributes& attributes = node.GetAttributes();
+  if (auto opset = node.SinceVersion(); opset < 10) {
+    auto attr_starts = attributes.find("starts");
+    auto attr_ends = attributes.find("ends");
+    auto AttrIsRequired = [&attributes](const auto& attr_it, const std::string& attr_name) {
+      if (!(attr_it != attributes.end())) {
+        return make_unsupported("for opset < 10 attr " + attr_name + " is required");
+      }
+      return make_supported();
+    };
+    ORT_RETURN_IF_NOT_SUPPORTED(AttrIsRequired(attr_starts, "starts"));
+    ORT_RETURN_IF_NOT_SUPPORTED(AttrIsRequired(attr_ends, "ends"));
+    auto AttrTypeIsINTS = [](auto& attr_it, const std::string& attr_name) {
+      if (!(attr_it->second().type() == ONNX_NAMESPACE::AttributeProto_AttributeType::AttributeProto_AttributeType_INTS)) {
+        return make_unsupported("attr " + attr_name + " is required");
+      }
+      return make_supported();
+    };
+    ORT_RETURN_IF_NOT_SUPPORTED(AttrTypeIsINTS(attr_starts, "start"));
+    ORT_RETURN_IF_NOT_SUPPORTED(AttrTypeIsINTS(attr_ends, "ends"));
+
+    const auto& attr_start_values = attr_starts->second().ints();
+    const auto& attr_ends_values = attr_ends->second().ints();
+    if (attr_start_values.size() != attr_ends_values.size()) {
+      return make_unsupported("attr starts and ends have different sizes!");
+    }
+    const auto input_dims = node.InputDefs()[0]->Shape()->dim_size();
+    ORT_RETURN_IF_NOT_SUPPORTED(HasValidStartEndRange(attr_start_values, attr_ends_values, attr_start_values.size(), 0, input_dims));
+  }
+  return make_supported();
+}
+
+////////////////////
+
+EtGlowNodeSupportResult EtGlowSoftmaxNodeCapability::Supported(const Node& node, const GraphViewer& graph_viewer) const {
+  ORT_RETURN_IF_NOT_SUPPORTED(IsTypeSupported(node));
+  ORT_RETURN_IF_NOT_SUPPORTED(IsDimensionSupported(node, graph_viewer));
+  return make_supported();
+}
+
+EtGlowNodeSupportResult EtGlowSoftmaxNodeCapability::IsDimensionSupported(const Node& node, const GraphViewer& graph_viewer) const {
+  ORT_RETURN_IF_NOT_SUPPORTED(EtGlowDefaultNodeCapability::IsDimensionSupported(node, graph_viewer));
+
+  if (const auto* input0_shape = node.InputDefs()[0]->Shape(); input0_shape != nullptr) {
+    const auto& input_dim_size = input0_shape->dim_size();
+    constexpr int min_dims_before_13 = 2_dim;  // NOLINT(cppcoreguidelines-avoid-magic-numbers,readability-magic-numbers)
+    if (auto opset = node.SinceVersion(); opset < 13_opset) {
+      if (input_dim_size < min_dims_before_13) {  // NOLINT(cppcoreguidelines-avoid-magic-numbers,readability-magic-numbers)
+        return make_unsupported("for opset previous to 13 input shape dims must be >= 2");
+      }
+    } else {
+      if (input_dim_size != 4_dim) {  // NOLINT(cppcoreguidelines-avoid-magic-numbers,readability-magic-numbers)
+        return make_unsupported("for opset 13 input shape must have 4 dims");
+      }
+    }
+  }
+
+  return make_supported();
+}
+
+////////////////////
+
+EtGlowSplitNodeCapability::EtGlowSplitNodeCapability(CapabilityMode mode, const ONNXSymbolsShapes& user_parametric_shapes_replacements)
+    : EtGlowDefaultNodeCapability(mode, user_parametric_shapes_replacements,
+                                  std::vector<ORT_DataType>{type_float32, type_float16,
+                                                            type_int64, type_int32, type_int8}) {}
+
+EtGlowNodeSupportResult EtGlowSplitNodeCapability::Supported(const Node& node, const GraphViewer& graph_viewer) const {
+  ORT_RETURN_IF_NOT_SUPPORTED(IsTypeSupported(node));
+  ORT_RETURN_IF_NOT_SUPPORTED(IsDimensionSupported(node, graph_viewer));
+  ORT_RETURN_IF_NOT_SUPPORTED(IsAttributeSupported(node, graph_viewer));
+  return make_supported();
+}
+
+EtGlowNodeSupportResult EtGlowSplitNodeCapability::IsTypeSupported(const Node& node) const {
+  ORT_RETURN_IF_NOT_SUPPORTED(EtGlowDefaultNodeCapability::IsTypeSupported(node));
+
+  if (auto node_inputs = node.InputDefs(); node_inputs.size() >= 2) {
+    const auto* split = node_inputs[1];
+    if (split->TypeAsProto() == nullptr) {
+      LOGS_DEFAULT(WARNING) << "cannot verify 'split' element_type, assume it's tensor(int64).";
+      return make_supported();
+    }
+    if (auto split_elem_type = static_cast<ORT_DataType>(split->TypeAsProto()->tensor_type().elem_type());
+        split_elem_type != type_int64) {
+      return make_unsupported("'split' element_type " + std::to_string(split_elem_type) + " is not int64");
+    }
+  }
+  return make_supported();
+}
+
+EtGlowNodeSupportResult EtGlowSplitNodeCapability::IsDimensionSupported(const Node& node, const GraphViewer& graph_viewer) const {
+  ORT_RETURN_IF_NOT_SUPPORTED(EtGlowDefaultNodeCapability::IsDimensionSupported(node, graph_viewer));
+
+  auto node_inputs = node.InputDefs();
+  const auto* input = node_inputs[0];
+  ORT_RETURN_IF_NOT_SUPPORTED(HasNonZeroPositiveValueDimsSafe(input, GetParametricShapeReplacements(), GetCapabilityMode()));
+
+  if (node_inputs.size() >= 2) {
+    const auto* split = node_inputs[1];
+    if (!graph_viewer.IsConstantInitializer(split->Name(), true)) {
+      // Glow then requires 'split' tensor to be a constant
+      return make_unsupported("glow requires input " + split->Name() + " 'split' to be a constant.");
+    }
+  }
+  return make_supported();
+}
+
+EtGlowNodeSupportResult EtGlowSplitNodeCapability::IsAttributeSupported(const Node& node, const GraphViewer& graph_viewer) const {
+  ORT_UNUSED_PARAMETER(graph_viewer);
+  const NodeAttributes& attributes = node.GetAttributes();
+  if (auto split_axes = attributes.find("split"); split_axes != attributes.end() &&
+                                                  split_axes->second().type() != ONNX_NAMESPACE::AttributeProto_AttributeType::AttributeProto_AttributeType_INTS) {
+    return make_unsupported("splits attr does not contain the expected type (INTS)");
+  }
+  if (auto attr_num_outputs = attributes.find("num_outputs"); attr_num_outputs != attributes.end()) {
+    return make_unsupported("attr num_outputs is not supported");
+  }
+  return make_supported();
+}
+
+////////////////////
+
+EtGlowNodeSupportResult EtGlowSqueezeNodeCapability::Supported(const Node& node, const GraphViewer& graph_viewer) const {
+  ORT_RETURN_IF_NOT_SUPPORTED(IsTypeSupported(node));
+  ORT_RETURN_IF_NOT_SUPPORTED(IsDimensionSupported(node, graph_viewer));
+  ORT_RETURN_IF_NOT_SUPPORTED(IsAttributeSupported(node, graph_viewer));
+  return make_supported();
+}
+
+EtGlowNodeSupportResult EtGlowSqueezeNodeCapability::IsTypeSupported(const Node& node) const {
+  ORT_RETURN_IF_NOT_SUPPORTED(EtGlowDefaultNodeCapability::IsTypeSupported(node));
+
+  // if opset >= 13 'axes' should be a int64_t but glow expects it to be a int.
+  // Add this restriction here while we sort this limitation in glow
+  if (auto node_inputs = node.InputDefs(); node_inputs.size() > 1) {
+    const auto* axes = node_inputs[1];
+    if (axes->TypeAsProto() == nullptr) {
+      return make_unsupported("cannot verify 'axes' element_type, assume we do not support.");
+    }
+    if (auto axes_elem_type = static_cast<ORT_DataType>(axes->TypeAsProto()->tensor_type().elem_type());
+        !(axes_elem_type == type_int32 || axes_elem_type == type_int64)) {
+      return make_unsupported("'axes' element_type " + std::to_string(axes_elem_type) + " is not int32 or int64");
+    }
+  }
+  return make_supported();
+}
+
+EtGlowNodeSupportResult EtGlowSqueezeNodeCapability::IsDimensionSupported(const Node& node, const GraphViewer& graph_viewer) const {
+  ORT_RETURN_IF_NOT_SUPPORTED(EtGlowDefaultNodeCapability::IsDimensionSupported(node, graph_viewer));
+
+  if (auto opset = node.SinceVersion(); opset >= 13) {
+    auto node_inputs = node.InputDefs();
+    if (node_inputs.size() < 2) {
+      return make_unsupported("if opset >= 13 'axes' is provided as an input. Even if spec says it's an optional input. Glow requires it. But it's missing.");
+    }
+    const auto* axes = node_inputs[1];
+    if (!graph_viewer.IsConstantInitializer(axes->Name(), true)) {
+      // if opset >= 13 'axes' is provided as an input. Glow then requires 'axes' tensor to be a constant
+      return make_unsupported("glow requires input " + axes->Name() + " 'axes' to be a constant.");
+    }
+    if (auto axes_elem_type = static_cast<ORT_DataType>(axes->TypeAsProto()->tensor_type().elem_type());
+        !(axes_elem_type == type_int32 || axes_elem_type == type_int64)) {
+      return make_unsupported("'axes' element_type " + std::to_string(axes_elem_type) + " is not int32 or int64");
+    }
+  }
+  return make_supported();
+}
+
+EtGlowNodeSupportResult EtGlowSqueezeNodeCapability::IsAttributeSupported(const Node& node, const GraphViewer& graph_viewer) const {
+  ORT_UNUSED_PARAMETER(graph_viewer);
+  const NodeAttributes& attributes = node.GetAttributes();
+  if (auto opset = node.SinceVersion(); opset < 13) {
+    if (auto attr_axes = attributes.find("axes"); !(attr_axes != attributes.end())) {
+      return make_unsupported("if opset < 13 'axes' is required as an attributte. glow requires attributte 'axes'");
+    }
+  }
+  return make_supported();
+}
+
+////////////////////
+
+EtGlowNodeSupportResult EtGlowSumNodeCapability::Supported(const Node& node, const GraphViewer& graph_viewer) const {
+  ORT_RETURN_IF_NOT_SUPPORTED(IsTypeSupported(node));
+  ORT_RETURN_IF_NOT_SUPPORTED(IsDimensionSupported(node, graph_viewer));
+  return make_supported();
+}
+
+EtGlowNodeSupportResult EtGlowSumNodeCapability::IsDimensionSupported(const Node& node, const GraphViewer& graph_viewer) const {
+  ORT_RETURN_IF_NOT_SUPPORTED(EtGlowDefaultNodeCapability::IsDimensionSupported(node, graph_viewer));
+
+  if (auto node_inputs = node.InputDefs(); node_inputs.size() > 1) {
+    // glow does not seem to support multidimensional broadcasting for 'Sum'
+    // if inputs do not share same shape --> unsupported
+    const auto& input_0 = node_inputs[0];
+    ORT_RETURN_IF_NO_SHAPE(input_0, GetCapabilityMode());
+    auto dim_size = input_0->Shape()->dim_size();
+    for (size_t i = 1; i < node_inputs.size(); ++i) {
+      const auto& input_i = node_inputs[i];
+      ORT_RETURN_IF_NO_SHAPE(input_i, GetCapabilityMode());
+      if (input_i->Shape()->dim_size() != dim_size) {
+        return make_unsupported("Not all inputs have same shape. Glow doesn't support multidimensional broadcasting for 'Sum'.");
+      }
+    }
+  }
+  return make_supported();
+}
+
+////////////////////
+
+EtGlowTileNodeCapability::EtGlowTileNodeCapability(CapabilityMode mode, const ONNXSymbolsShapes& user_parametric_shapes_replacements)
+    : EtGlowDefaultNodeCapability(mode, user_parametric_shapes_replacements,
+                                  std::vector<ORT_DataType>{type_float32, type_float16, type_bfloat16,
+                                                            type_int64, type_int32, /* not supported type_int16,*/ type_int8,
+                                                            type_bool}) {}
+
+EtGlowNodeSupportResult EtGlowTileNodeCapability::Supported(const Node& node, const GraphViewer& graph_viewer) const {
+  ORT_RETURN_IF_NOT_SUPPORTED(IsTypeSupported(node));
+  ORT_RETURN_IF_NOT_SUPPORTED(IsDimensionSupported(node, graph_viewer));
+  return make_supported();
+}
+
+EtGlowNodeSupportResult EtGlowTileNodeCapability::IsTypeSupported(const Node& node) const {
+  ORT_RETURN_IF_NOT_SUPPORTED(EtGlowDefaultNodeCapability::IsTypeSupported(node));
+
+  auto node_inputs = node.InputDefs();
+  if (node_inputs.size() < 2) {
+    return make_unsupported(node.OpType() + " required 'repeats' input is missing");
+  }
+
+  const auto* repeats = node_inputs[1];
+  if (repeats->TypeAsProto() == nullptr) {
+    return make_unsupported("cannot verify 'repeats' element_type, assume we do not support.");
+  }
+  if (auto repeats_elem_type = repeats->TypeAsProto()->tensor_type().elem_type(); repeats_elem_type != type_int64) {
+    return make_unsupported("'repeats' element_type is not int64");
+  }
+
+  return make_supported();
+}
+
+EtGlowNodeSupportResult EtGlowTileNodeCapability::IsDimensionSupported(const Node& node, const GraphViewer& graph_viewer) const {
+  ORT_RETURN_IF_NOT_SUPPORTED(EtGlowDefaultNodeCapability::IsDimensionSupported(node, graph_viewer));
+  if (auto opset = node.SinceVersion(); opset < 6) {
+    return make_unsupported(node.OpType() + " operator is only supported in glow for opset >= 6");
+  }
+
+  auto node_inputs = node.InputDefs();
+
+  const auto& input = node_inputs[0];
+  const auto* input_shape = input->Shape();
+  if (input_shape == nullptr) {
+    return make_unsupported(node.OpType() + " input " + input->Name() + " 'input' shape is nullptr");
+  }
+
+  const auto& repeats = node_inputs[1];
+  if (!graph_viewer.IsConstantInitializer(repeats->Name(), true)) {
+    return make_unsupported(node.OpType() + " input " + repeats->Name() + " 'repeats' must be a constant");
+  }
+
+  const auto* repeats_shape = repeats->Shape();
+  if (repeats_shape == nullptr) {
+    return make_unsupported(node.OpType() + " input " + repeats->Name() + " 'repeats' shape is nullptr");
+  }
+  if (repeats_shape->dim_size() != 1) {
+    return make_unsupported(node.OpType() + " input " + repeats->Name() + " 'repeats' should have a single dim tensor");
+  }
+  if (repeats_shape->dim(0).dim_value() != input_shape->dim_size()) {
+    return make_unsupported(node.OpType() + " input " + repeats->Name() + " 'repeats' should have one value for each dimension of input!");
+  }
+
+  return make_supported();
+}
+
+////////////////////
+
+EtGlowTopKNodeCapability::EtGlowTopKNodeCapability(CapabilityMode mode, const ONNXSymbolsShapes& user_parametric_shapes_replacements)
+    : EtGlowDefaultNodeCapability(mode, user_parametric_shapes_replacements) {
+  // Intentionally left empty
+}
+
+EtGlowNodeSupportResult EtGlowTopKNodeCapability::Supported(const Node& node, const GraphViewer& graph_viewer) const {
+  ORT_RETURN_IF_NOT_SUPPORTED(IsTypeSupported(node));
+  ORT_RETURN_IF_NOT_SUPPORTED(IsDimensionSupported(node, graph_viewer));
+  ORT_RETURN_IF_NOT_SUPPORTED(IsAttributeSupported(node, graph_viewer));
+  return make_unsupported("unconditionally disabled. Not yet supported by dnn-library");
+}
+
+EtGlowNodeSupportResult EtGlowTopKNodeCapability::IsTypeSupported(const Node& node) const {
+  auto node_inputs = node.InputDefs();
+
+  auto check_tensor_types = [this](const auto* tensor, const std::string& node_alias, const std::vector<ORT_DataType>& supported_types) {
+    if (tensor->TypeAsProto() == nullptr) {
+      return SkipTypeChecks(GetCapabilityMode()) ? make_supported() : make_unsupported("cannot verify 'K' element_type, assume we do not support.");
+    }
+    const auto datatype = static_cast<ORT_DataType>(tensor->TypeAsProto()->tensor_type().elem_type());
+    if (bool is_supported_in_type = std::find(supported_types.begin(), supported_types.end(), datatype) != supported_types.end(); !is_supported_in_type) {
+      return make_unsupported(node_alias + " (" + tensor->Name() + ") is of type: " + std::to_string(datatype) + " which is not in the list of supported types: " + std::to_string(supported_types));
+    }
+    return make_supported();
+  };
+
+  const auto* X = node_inputs[0];
+  ORT_RETURN_IF_NOT_SUPPORTED(check_tensor_types(X, "input[0]", GetSupportedTTypes()));
+
+  if (node_inputs.size() >= 2) {
+    const auto* K = node_inputs[1];
+    ORT_RETURN_IF_NOT_SUPPORTED(check_tensor_types(K, "input[1]", {type_int64}));
+  }
+
+  auto node_outputs = node.OutputDefs();
+  const auto* values = node_outputs[0];
+  ORT_RETURN_IF_NOT_SUPPORTED(check_tensor_types(values, "output[0]", GetSupportedTTypes()));
+
+  const auto* indices = node_outputs[1];
+  ORT_RETURN_IF_NOT_SUPPORTED(check_tensor_types(indices, "output[1]", GetSupportedITypes()));
+
+  return make_supported();
+}
+
+EtGlowNodeSupportResult EtGlowTopKNodeCapability::IsDimensionSupported(const Node& node, const GraphViewer& graph_viewer) const {
+  ORT_RETURN_IF_NOT_SUPPORTED(EtGlowDefaultNodeCapability::IsDimensionSupported(node, graph_viewer));
+  if (const auto node_inputs = node.InputDefs(); node_inputs.size() >= 2) {
+    const auto* K = node_inputs[1];
+    ORT_RETURN_IF_NOT_SUPPORTED(IsScalarConstantSafe(K, "K", graph_viewer, GetCapabilityMode()));
+  }
+  return make_supported();
+}
+
+EtGlowNodeSupportResult EtGlowTopKNodeCapability::IsAttributeSupported(const Node& node, const GraphViewer& graph_viewer) const {
+  ORT_UNUSED_PARAMETER(graph_viewer);
+  const NodeAttributes& attributes = node.GetAttributes();
+  if (auto attr_largest = attributes.find("largest"); attr_largest != attributes.end()) {
+    const int64_t attr_largest_default_value = 1;
+    if (auto attr_largest_value = attr_largest->second().i(); attr_largest_value != attr_largest_default_value) {
+      return make_unsupported("attr 'largest' is not supported by glow");
+    }
+  }
+  if (auto attr_sorted = attributes.find("sorted"); attr_sorted != attributes.end()) {
+    const int64_t attr_sorted_default_value = 1;
+    if (auto attr_sorted_value = attr_sorted->second().i(); attr_sorted_value != attr_sorted_default_value) {
+      return make_unsupported("attr 'sorted' is not supported by glow");
+    }
+  }
+  return make_supported();
+}
+
+////////////////////
+
+EtGlowNodeSupportResult EtGlowTriluNodeCapability::Supported(const Node& node, const GraphViewer& graph_viewer) const {
+  ORT_RETURN_IF_NOT_SUPPORTED(IsTypeSupported(node));
+  ORT_RETURN_IF_NOT_SUPPORTED(IsDimensionSupported(node, graph_viewer));
+  ORT_RETURN_IF_NOT_SUPPORTED(IsAttributeSupported(node, graph_viewer));
+  return make_unsupported("unconditionally disabled. Not yet supported by dnn-library");
+}
+
+EtGlowNodeSupportResult EtGlowTriluNodeCapability::IsDimensionSupported(const Node& node, const GraphViewer& graph_viewer) const {
+  ORT_RETURN_IF_NOT_SUPPORTED(EtGlowDefaultNodeCapability::IsDimensionSupported(node, graph_viewer));
+  const auto node_inputs = node.InputDefs();
+  const auto* input = node_inputs[0];
+  if (!HasShape(input) && !SkipShapeChecks(GetCapabilityMode())) {
+    return make_unsupported(input->Name() + " does not have shape information. Assume not supported");
+  }
+  if (HasShape(input) && input->Shape()->dim_size() < 2) {
+    return make_unsupported("input " + input->Name() + " must be 2D or higher");
+  }
+  ORT_RETURN_IF_NOT_SUPPORTED(HasNonZeroPositiveValueDimsSafe(input, GetParametricShapeReplacements(), GetCapabilityMode()));
+  if (node_inputs.size() > 1) {
+    const auto* k = node_inputs[1];
+    if (!HasShape(k) && !SkipShapeChecks(GetCapabilityMode())) {
+      return make_unsupported(k->Name() + " does not have shape information. Assume not supported");
+    }
+    if (HasShape(k) && !IsScalar(k)) {
+      return make_unsupported(k->Name() + " must be a scalar");
+    }
+    ORT_RETURN_IF_NOT_SUPPORTED(HasNonZeroPositiveValueDimsSafe(k, GetParametricShapeReplacements(), GetCapabilityMode()));
+  }
+  return make_supported();
+}
+
+EtGlowNodeSupportResult EtGlowTriluNodeCapability::IsAttributeSupported(const Node& node, const GraphViewer& graph_viewer) const {
+  ORT_UNUSED_PARAMETER(graph_viewer);
+  const NodeAttributes& attributes = node.GetAttributes();
+  auto attr_upper = attributes.find("upper");
+  if (!(attr_upper != attributes.end())) {
+    return make_unsupported("missing attr upper");
+  }
+  if (attr_upper->second().type() != ONNX_NAMESPACE::AttributeProto_AttributeType::AttributeProto_AttributeType_INT) {
+    return make_unsupported("upper attr does not contain the expected type (INT)");
+  }
+  return make_supported();
+}
+
+////////////////////
+
+EtGlowNodeSupportResult EtGlowUpsampleNodeCapability::Supported(const Node& node, const GraphViewer& graph_viewer) const {
+  ORT_RETURN_IF_NOT_SUPPORTED(IsTypeSupported(node));
+  ORT_RETURN_IF_NOT_SUPPORTED(IsDimensionSupported(node, graph_viewer));
+  ORT_RETURN_IF_NOT_SUPPORTED(IsAttributeSupported(node, graph_viewer));
+  return make_supported();
+}
+
+EtGlowNodeSupportResult EtGlowUpsampleNodeCapability::IsDimensionSupported(const Node& node, const GraphViewer& graph_viewer) const {
+  ORT_RETURN_IF_NOT_SUPPORTED(EtGlowDefaultNodeCapability::IsDimensionSupported(node, graph_viewer));
+  constexpr int opset_9 = 9;
+  constexpr int opset_7 = 7;
+  if (auto opset = node.SinceVersion(); opset < opset_7 || opset > opset_9) {
+    return make_unsupported(node.OpType() + " operator is only supported in glow for opset between 7 and 9");
+  }
+
+  auto node_inputs = node.InputDefs();
+
+  if (auto opset = node.SinceVersion(); opset == opset_7 && node_inputs.size() < 2) {
+    return make_unsupported("opset 7 requires 2 inputs ('X' and 'scales') but 'scales' is missing.");
+  }
+  if (auto opset = node.SinceVersion(); opset > opset_7) {
+    const auto* scales = node_inputs[1];
+    ORT_RETURN_IF_NOT_SUPPORTED(IsScalarConstantSafe(scales, "scales", graph_viewer, GetCapabilityMode()));
+    if (auto scales_dim_size = scales->Shape()->dim_size(); scales_dim_size < 4 || scales_dim_size > 5) {  // NOLINT(cppcoreguidelines-avoid-magic-numbers,readability-magic-numbers)
+      return make_unsupported("found (scales) with dim_size: " + std::to_string(scales_dim_size) + " but glow only accepts [4, 5] ");
+    }
+    for (const auto& dim : scales->Shape()->dim()) {
+      if (!dim.has_dim_value()) {
+        return make_unsupported("glow only supports non-parametric shapes");
+      }
+      auto dim_value = dim.dim_value();
+      if (dim_value < 1) {
+        return make_unsupported("scales can only be greater or equal to 1, but found" + std::to_string(dim_value));
+      }
+    }
+  }
+
+  return make_supported();
+}
+
+EtGlowNodeSupportResult EtGlowUpsampleNodeCapability::IsAttributeSupported(const Node& node, const GraphViewer& graph_viewer) const {
+  ORT_UNUSED_PARAMETER(graph_viewer);
+  const NodeAttributes& attributes = node.GetAttributes();
+  auto attr_mode = attributes.find("mode");
+  if (!(attr_mode != attributes.end())) {
+    return make_unsupported("missing attr mode");
+  }
+  if (auto attr_mode_value = attr_mode->second().s(); attr_mode_value != "nearest") {
+    return make_unsupported(node.OpType() + " operator has attr mode: " + attr_mode_value + " glow only supports nearest.");
+  }
+
+  constexpr int opset_7 = 7;
+  auto attr_scales = attributes.find("scales");
+  if (auto opset = node.SinceVersion(); opset == opset_7 && !(attr_scales != attributes.end())) {
+    return make_unsupported("(opset 7) requires attr scales, but is missing");
+  }
+  if (!(attr_scales != attributes.end())) {
+    if (attr_scales->second().type() != ONNX_NAMESPACE::AttributeProto_AttributeType::AttributeProto_AttributeType_FLOATS) {
+      return make_unsupported("scales attr does not contain the expected type (FLOATS)");
+    }
+    auto scales_dim_size = attr_scales->second().floats_size();
+    if (scales_dim_size < 4 || scales_dim_size > 5) {  // NOLINT(cppcoreguidelines-avoid-magic-numbers,readability-magic-numbers)
+      return make_unsupported("found (scales) with dim_size: " + std::to_string(scales_dim_size) + " but glow only accepts [4, 5] ");
+    }
+    for (int i = 0; i < scales_dim_size; ++i) {
+      const auto dim_value = attr_scales->second().floats(i);
+      if (dim_value < 1) {
+        return make_unsupported("scales can only be greater or equal to 1, but found" + std::to_string(dim_value));
+      }
+    }
+  }
+  return make_supported();
+}
+
+////////////////////
+
+EtGlowWhereNodeCapability::EtGlowWhereNodeCapability(CapabilityMode mode, const ONNXSymbolsShapes& user_parametric_shapes_replacements)
+    : EtGlowDefaultNodeCapability(mode, user_parametric_shapes_replacements,
+                                  std::vector<ORT_DataType>{type_bool} /* condition can only be bool */) {}
+
+EtGlowNodeSupportResult EtGlowWhereNodeCapability::Supported(const Node& node, const GraphViewer& graph_viewer) const {
+  ORT_RETURN_IF_NOT_SUPPORTED(IsTypeSupported(node));
+  ORT_RETURN_IF_NOT_SUPPORTED(IsDimensionSupported(node, graph_viewer));
+  return make_supported();
+}
+
+EtGlowNodeSupportResult EtGlowWhereNodeCapability::IsTypeSupported(const Node& node) const {
+  ORT_RETURN_IF_NOT_SUPPORTED(EtGlowDefaultNodeCapability::IsTypeSupported(node));
+  const auto node_inputs = node.InputDefs();
+  if (node_inputs.size() < 3) {
+    return make_unsupported(node.OpType() + " must have at least 3 inputs");
+  }
+  auto check_supported_types = [this](const auto input) {
+    if (input->TypeAsProto() == nullptr) {
+      return make_unsupported(input->Name() + " is missing type information");
+    }
+    const auto node_datatype = static_cast<ORT_DataType>(input->TypeAsProto()->tensor_type().elem_type());
+    const auto& supported_types = GetSupportedTTypes();
+    if (bool is_supported_type = std::find(supported_types.begin(), supported_types.end(), node_datatype) != supported_types.end(); !is_supported_type) {
+      return make_unsupported("input (" + input->Name() + ") is of type: " + std::to_string(node_datatype) + " which is not in the list of supported types.");
+    }
+    return make_supported();
+  };
+
+  const auto& X = node_inputs[1];
+  const auto& Y = node_inputs[2];
+  ORT_RETURN_IF_NOT_SUPPORTED(check_supported_types(X));
+  ORT_RETURN_IF_NOT_SUPPORTED(check_supported_types(Y));
+
+  return EtGlowDefaultNodeCapability::IsTypeSupported(node);
+}
+
+EtGlowNodeSupportResult EtGlowWhereNodeCapability::IsDimensionSupported(const Node& node, const GraphViewer& graph_viewer) const {
+  ORT_RETURN_IF_NOT_SUPPORTED(EtGlowDefaultNodeCapability::IsDimensionSupported(node, graph_viewer));
+  const auto node_inputs = node.InputDefs();
+
+  const auto& X = node_inputs[1];
+  const auto& Y = node_inputs[2];
+  ORT_RETURN_IF_NO_SHAPE(X, GetCapabilityMode());
+  ORT_RETURN_IF_NO_SHAPE(Y, GetCapabilityMode());
+
+  ORT_RETURN_IF_NOT_SUPPORTED(HasNonZeroPositiveValueDims(X, GetParametricShapeReplacements(), GetCapabilityMode()));
+  ORT_RETURN_IF_NOT_SUPPORTED(HasNonZeroPositiveValueDims(Y, GetParametricShapeReplacements(), GetCapabilityMode()));
+
+  return make_supported();
+}
+
+}  // end namespace onnxruntime
diff --git a/onnxruntime/core/providers/etglow/etglow_node_capability.h b/onnxruntime/core/providers/etglow/etglow_node_capability.h
new file mode 100644
index 0000000000..3669d5f753
--- /dev/null
+++ b/onnxruntime/core/providers/etglow/etglow_node_capability.h
@@ -0,0 +1,815 @@
+#pragma once
+
+#include "core/providers/shared_library/provider_api.h"
+#include "etglow_execution_provider_info.h"
+#include <unordered_set>
+
+namespace onnxruntime {
+
+// borrowed from dnnl_node_capability
+
+// redefinition of `enum TensorProto_DataType : int` to help code readability by using shorter names.
+enum ORT_DataType : int {
+  type_undefined = ONNX_NAMESPACE::TensorProto_DataType_UNDEFINED,
+  type_float32 = ONNX_NAMESPACE::TensorProto_DataType_FLOAT,
+  type_uint8 = ONNX_NAMESPACE::TensorProto_DataType_UINT8,
+  type_int8 = ONNX_NAMESPACE::TensorProto_DataType_INT8,
+  type_uint16 = ONNX_NAMESPACE::TensorProto_DataType_UINT16,
+  type_int16 = ONNX_NAMESPACE::TensorProto_DataType_INT16,
+  type_int32 = ONNX_NAMESPACE::TensorProto_DataType_INT32,
+  type_int64 = ONNX_NAMESPACE::TensorProto_DataType_INT64,
+  type_string = ONNX_NAMESPACE::TensorProto_DataType_STRING,
+  type_bool = ONNX_NAMESPACE::TensorProto_DataType_BOOL,
+  type_float16 = ONNX_NAMESPACE::TensorProto_DataType_FLOAT16,
+  type_double = ONNX_NAMESPACE::TensorProto_DataType_DOUBLE,
+  type_uint32 = ONNX_NAMESPACE::TensorProto_DataType_UINT32,
+  type_uint64 = ONNX_NAMESPACE::TensorProto_DataType_UINT64,
+  type_complex64 = ONNX_NAMESPACE::TensorProto_DataType_COMPLEX64,
+  type_complex128 = ONNX_NAMESPACE::TensorProto_DataType_COMPLEX128,
+  type_bfloat16 = ONNX_NAMESPACE::TensorProto_DataType_BFLOAT16,
+  type_float8e3m3fn = ONNX_NAMESPACE::TensorProto_DataType_FLOAT8E4M3FN,
+  type_float8e4m3fnuz = ONNX_NAMESPACE::TensorProto_DataType_FLOAT8E4M3FNUZ,
+  type_float8e5m2 = ONNX_NAMESPACE::TensorProto_DataType_FLOAT8E5M2,
+  type_float8e5m2fnuz = ONNX_NAMESPACE::TensorProto_DataType_FLOAT8E5M2FNUZ,
+  type_uint4 = ONNX_NAMESPACE::TensorProto_DataType_UINT4,
+  type_int4 = ONNX_NAMESPACE::TensorProto_DataType_INT4
+};
+
+enum class CapabilityMode {
+  DEFAULT,  // Default mode of operation. It's a trade-of between no-capability checking and the maximum strictness.
+            // Should maximize number of supported networks
+
+  STRICT  // Level where all graph nodes are checked to ensure glow preconditions suffice. Can lead to false negatives.
+};
+
+}  // end namespace onnxruntime
+
+namespace std {
+std::string to_string(onnxruntime::ORT_DataType t);
+std::string to_string(const std::vector<onnxruntime::ORT_DataType>& t);
+std::string to_string(onnxruntime::CapabilityMode mode);
+
+onnxruntime::CapabilityMode from_string(const std::string& str);
+}  // namespace std
+
+namespace onnxruntime {
+
+// user defined literals to specify constants
+constexpr int operator"" _dim(unsigned long long dims) {  // NOLINT(google-runtime-int)
+  return static_cast<int>(dims);
+}
+
+constexpr int operator"" _opset(unsigned long long opset) {  // NOLINT(google-runtime-int)
+  return static_cast<int>(opset);
+}
+
+// helper class to declare a node is supported
+struct supported_t {
+  constexpr explicit supported_t() = default;
+};
+
+// The result of checking support for a Operator node.
+// Acts like a boolean, and if unsupported, contains the reason why
+class EtGlowNodeSupportResult {
+ public:
+  explicit EtGlowNodeSupportResult(supported_t t) { ORT_UNUSED_PARAMETER(t); };
+  explicit EtGlowNodeSupportResult(std::string reason) : opt_reason_{std::move(reason)} {};
+
+  bool supported() const { return !opt_reason_.has_value(); }
+  const std::string& reason() const { return opt_reason_.value(); }
+
+ private:
+  std::optional<std::string> opt_reason_;
+};
+EtGlowNodeSupportResult make_supported();
+EtGlowNodeSupportResult make_unsupported(std::string&& reason);
+
+EtGlowNodeSupportResult HasValueForParametricDim(const onnx::TensorShapeProto_Dimension& dim, const ONNXSymbolsShapes& user_parametric_shapes_replacements, const std::string& name);
+EtGlowNodeSupportResult HasNonZeroPositiveValueDimsSafe(const NodeArg* node_arg, const ONNXSymbolsShapes& user_parametric_shapes_replacements, CapabilityMode mode);
+
+// similar to ORT_RETURN_IF
+#define ORT_RETURN_IF_NOT_SUPPORTED(condition, ...)     \
+  do {                                                  \
+    if (auto result = condition; !result.supported()) { \
+      return result;                                    \
+    }                                                   \
+  } while (false)
+
+class EtGlowNodeCapability {
+ public:
+  EtGlowNodeCapability() = default;
+  virtual ~EtGlowNodeCapability() = default;
+
+  EtGlowNodeCapability(EtGlowNodeCapability&& other) noexcept = default;
+  EtGlowNodeCapability& operator=(EtGlowNodeCapability&& other) = default;
+
+  virtual EtGlowNodeSupportResult Supported(const Node& node, const GraphViewer& graph_viewer) const = 0;
+
+ private:
+  ORT_DISALLOW_COPY_AND_ASSIGNMENT(EtGlowNodeCapability);
+};
+
+// Dummy implementation to explicitly declare that a node is not supported
+class EtGlowNotSupportedNodeCapability : public EtGlowNodeCapability {
+ public:
+  EtGlowNotSupportedNodeCapability() = default;
+  explicit EtGlowNotSupportedNodeCapability(std::string user_provided_message);
+
+  EtGlowNodeSupportResult Supported(const Node& node, const GraphViewer& graph_viewer) const override;
+
+ private:
+  std::optional<std::string> unsupported_message_;
+};
+
+// This class decides if we are capable of running the node based on the input data type
+class EtGlowDefaultNodeCapability : public EtGlowNodeCapability {
+ public:
+  // Assumes that we support given input types:
+  // - type_float32, type_uint16, type_int16, type_int32, type_bool, type_float16, type_double, type_uint32, type_uint64
+  explicit EtGlowDefaultNodeCapability(CapabilityMode mode);
+  // Assumes that we support supported nodes from constructor (1)
+  // This constructor provides a list of user parametric shapes replacements
+  EtGlowDefaultNodeCapability(CapabilityMode mode, const ONNXSymbolsShapes& user_parametric_shapes_replacements);
+  // This constructor accepts a list of supported data types (for inputs only).
+  // This constructor provides a list of user parametric shapes replacements
+  EtGlowDefaultNodeCapability(CapabilityMode mode, const ONNXSymbolsShapes& user_parametric_shapes_replacements, std::vector<ORT_DataType> inputTypes);
+  // This constructor accepts two lists of supported data types (for inputs & outputs).
+  EtGlowDefaultNodeCapability(CapabilityMode mode, const ONNXSymbolsShapes& user_parametric_shapes_replacements, std::vector<ORT_DataType> inputTypes, std::vector<ORT_DataType> outputTypes);
+
+  EtGlowNodeSupportResult Supported(const Node& node, const GraphViewer& graph_viewer) const override;
+
+ protected:
+  virtual EtGlowNodeSupportResult IsTypeSupported(const Node& node) const;
+  virtual EtGlowNodeSupportResult IsDimensionSupported(const Node& node, const GraphViewer& graph_viewer) const;
+  virtual EtGlowNodeSupportResult IsAttributeSupported(const Node& node, const GraphViewer& graph_viewer) const;
+
+  CapabilityMode GetCapabilityMode() const { return mode_; }
+  const ONNXSymbolsShapes& GetParametricShapeReplacements() const { return user_parametric_shapes_replacements_; }
+  std::vector<ORT_DataType> GetSupportedInputTypes() const { return inputTypes_; }
+  std::vector<ORT_DataType> GetSupportedOutputTypes() const { return outputTypes_; }
+
+ private:
+  CapabilityMode mode_{};
+  ONNXSymbolsShapes user_parametric_shapes_replacements_{};
+  // Restriction from glow/Base/Type.h::55
+  int max_tensor_dimensions_ = 6_dim;  // NOLINT(cppcoreguidelines-avoid-magic-numbers,readability-magic-numbers)
+  std::vector<ORT_DataType> inputTypes_{type_float32, type_float16, type_int64, type_int32, type_int16, type_int8, type_bool};
+  std::vector<ORT_DataType> outputTypes_{};
+};
+
+///// Classes per operator family /////
+
+// This class decides if Pooling (AveragePool, MaxPool..) are supported or not
+class EtGlowPoolNodeCapability : public EtGlowDefaultNodeCapability {
+ public:
+  EtGlowPoolNodeCapability(CapabilityMode mode, const ONNXSymbolsShapes& user_parametric_shapes_replacements);
+
+  EtGlowNodeSupportResult Supported(const Node& node, const GraphViewer& graph_viewer) const override;
+
+ private:
+  EtGlowNodeSupportResult IsDimensionSupported(const Node& node, const GraphViewer& graph_viewer) const override;
+  EtGlowNodeSupportResult IsAttributeSupported(const Node& node, const GraphViewer& graph_viewer) const override;
+};
+
+// This class decides if ReduceMax, ReduceMean, ReduceProd, ReduceSum, ReduceSumSquare is supported or not
+class EtGlowReduceNodeCapability : public EtGlowDefaultNodeCapability {
+ public:
+  EtGlowReduceNodeCapability(CapabilityMode mode, const ONNXSymbolsShapes& user_parametric_shapes_replacements);
+  EtGlowReduceNodeCapability(CapabilityMode mode, const ONNXSymbolsShapes& user_parametric_shapes_replacements, const std::vector<ORT_DataType>& supported_types);
+
+  EtGlowNodeSupportResult Supported(const Node& node, const GraphViewer& graph_viewer) const override;
+
+ protected:
+  EtGlowNodeSupportResult IsDimensionSupported(const Node& node, const GraphViewer& graph_viewer) const override;
+  EtGlowNodeSupportResult IsAttributeSupported(const Node& node, const GraphViewer& graph_viewer) const override;
+
+  std::vector<ORT_DataType> GetSupportedTTypes() const { return {type_float32, type_float16, type_int64, type_int32, type_int16, type_int8}; }
+};
+
+// Implements common checks for 'OneLayer' ops (GRU, LSTM, RNN,..)
+class EtGlowOneLayerNodeCapability : public EtGlowDefaultNodeCapability {
+ public:
+  using EtGlowDefaultNodeCapability::EtGlowDefaultNodeCapability;
+
+ protected:
+  EtGlowNodeSupportResult IsAttributeSupported(const Node& node, const GraphViewer& graph_viewer) const override;
+
+  virtual int GetNumSupportedActivationsSize() const = 0;
+};
+
+///// Classes per operator /////
+
+// This class decides if Add is supported or not
+class EtGlowAddNodeCapability : public EtGlowDefaultNodeCapability {
+ public:
+  EtGlowAddNodeCapability(CapabilityMode mode, const ONNXSymbolsShapes& user_parametric_shapes_replacements);
+
+  EtGlowNodeSupportResult Supported(const Node& node, const GraphViewer& graph_viewer) const override;
+
+ private:
+  EtGlowNodeSupportResult IsDimensionSupported(const Node& node, const GraphViewer& graph_viewer) const override;
+};
+
+// This class decides if ArgMax is supported or not
+// We don't support select_last_index:1 with axis:1/-1 with non-random index (input shape dim lower than 3)
+class EtGlowArgMaxNodeCapability : public EtGlowDefaultNodeCapability {
+ public:
+  EtGlowArgMaxNodeCapability(CapabilityMode mode, const ONNXSymbolsShapes& user_parametric_shapes_replacements);
+
+  EtGlowNodeSupportResult Supported(const Node& node, const GraphViewer& graph_viewer) const override;
+
+ private:
+  EtGlowNodeSupportResult IsAttributeSupported(const Node& node, const GraphViewer& graph_viewer) const override;
+};
+
+class EtGlowBatchNormalizationNodeCapability : public EtGlowDefaultNodeCapability {
+ public:
+  using EtGlowDefaultNodeCapability::EtGlowDefaultNodeCapability;
+
+  EtGlowNodeSupportResult Supported(const Node& node, const GraphViewer& graph_viewer) const override;
+
+ private:
+  EtGlowNodeSupportResult IsDimensionSupported(const Node& node, const GraphViewer& graph_viewer) const override;
+  EtGlowNodeSupportResult IsAttributeSupported(const Node& node, const GraphViewer& graph_viewer) const override;
+};
+
+// This class decides if Clip is supported or not
+class EtGlowCastNodeCapability : public EtGlowDefaultNodeCapability {
+ public:
+  EtGlowCastNodeCapability(CapabilityMode mode, const ONNXSymbolsShapes& user_parametric_shapes_replacements);
+
+  EtGlowNodeSupportResult Supported(const Node& node, const GraphViewer& graph_viewer) const override;
+
+ private:
+  EtGlowNodeSupportResult IsAttributeSupported(const Node& node, const GraphViewer& graph_viewer) const override;
+
+  std::vector<ORT_DataType> GetSupportedGroup1T2Types() const { return {type_bool, type_int32, type_int64, type_bfloat16, type_float16, type_float32}; };
+};
+
+// This class decides if Clip is supported or not
+class EtGlowClipNodeCapability : public EtGlowDefaultNodeCapability {
+ public:
+  EtGlowClipNodeCapability(CapabilityMode mode, const ONNXSymbolsShapes& user_parametric_shapes_replacements);
+
+  EtGlowNodeSupportResult Supported(const Node& node, const GraphViewer& graph_viewer) const override;
+
+ private:
+  EtGlowNodeSupportResult IsDimensionSupported(const Node& node, const GraphViewer& graph_viewer) const override;
+};
+
+// This class decides if Concat is supported or not
+class EtGlowConcatNodeCapability : public EtGlowDefaultNodeCapability {
+ public:
+  using EtGlowDefaultNodeCapability::EtGlowDefaultNodeCapability;
+
+  EtGlowNodeSupportResult Supported(const Node& node, const GraphViewer& graph_viewer) const override;
+
+ private:
+  EtGlowNodeSupportResult IsDimensionSupported(const Node& node, const GraphViewer& graph_viewer) const override;
+};
+
+// This class decides if Constant is supported or not
+class EtGlowConstantNodeCapability : public EtGlowDefaultNodeCapability {
+ public:
+  using EtGlowDefaultNodeCapability::EtGlowDefaultNodeCapability;
+
+  EtGlowNodeSupportResult Supported(const Node& node, const GraphViewer& graph_viewer) const override;
+
+ private:
+  EtGlowNodeSupportResult IsAttributeSupported(const Node& node, const GraphViewer& graph_viewer) const override;
+};
+
+// This class decides if ConstantOfShape is supported or not
+class EtGlowConstantOfShapeNodeCapability : public EtGlowDefaultNodeCapability {
+ public:
+  EtGlowConstantOfShapeNodeCapability(CapabilityMode mode, const ONNXSymbolsShapes& user_parametric_shapes_replacements);
+
+  EtGlowNodeSupportResult Supported(const Node& node, const GraphViewer& graph_viewer) const override;
+
+ private:
+  EtGlowNodeSupportResult IsDimensionSupported(const Node& node, const GraphViewer& graph_viewer) const override;
+  EtGlowNodeSupportResult IsAttributeSupported(const Node& node, const GraphViewer& graph_viewer) const override;
+};
+
+// This class decides if Conv is supported or not
+class EtGlowConvNodeCapability : public EtGlowDefaultNodeCapability {
+ public:
+  using EtGlowDefaultNodeCapability::EtGlowDefaultNodeCapability;
+
+  EtGlowNodeSupportResult Supported(const Node& node, const GraphViewer& graph_viewer) const override;
+
+ private:
+  EtGlowNodeSupportResult IsDimensionSupported(const Node& node, const GraphViewer& graph_viewer) const override;
+  EtGlowNodeSupportResult IsAttributeSupported(const Node& node, const GraphViewer& graph_viewer) const override;
+};
+
+// This class decides if ConvTranspose is supported or not
+class EtGlowConvTransposeNodeCapability : public EtGlowDefaultNodeCapability {
+ public:
+  using EtGlowDefaultNodeCapability::EtGlowDefaultNodeCapability;
+
+  EtGlowNodeSupportResult Supported(const Node& node, const GraphViewer& graph_viewer) const override;
+
+ private:
+  EtGlowNodeSupportResult IsDimensionSupported(const Node& node, const GraphViewer& graph_viewer) const override;
+  EtGlowNodeSupportResult IsAttributeSupported(const Node& node, const GraphViewer& graph_viewer) const override;
+};
+
+// This class decides if CumSum is supported
+class EtGlowCumSumNodeCapability : public EtGlowDefaultNodeCapability {
+ public:
+  using EtGlowDefaultNodeCapability::EtGlowDefaultNodeCapability;
+
+  EtGlowNodeSupportResult Supported(const Node& node, const GraphViewer& graph_viewer) const override;
+
+ private:
+  EtGlowNodeSupportResult IsTypeSupported(const Node& node) const override;
+  EtGlowNodeSupportResult IsDimensionSupported(const Node& node, const GraphViewer& graph_viewer) const override;
+
+  std::vector<ORT_DataType> GetSupportedT2Types() const { return T2Types_; }
+
+  std::vector<ORT_DataType> T2Types_{type_int64, type_int32};
+};
+
+// This class decides if Dropout is supported or not
+class EtGlowDropoutNodeCapability : public EtGlowDefaultNodeCapability {
+ public:
+  EtGlowDropoutNodeCapability(CapabilityMode mode, const ONNXSymbolsShapes& user_parametric_shapes_replacements);
+
+  EtGlowNodeSupportResult Supported(const Node& node, const GraphViewer& graph_viewer) const override;
+
+ private:
+  EtGlowNodeSupportResult IsDimensionSupported(const Node& node, const GraphViewer& graph_viewer) const override;
+};
+
+// This class decides if Dropout is supported or not
+class EtGlowEinsumNodeCapability : public EtGlowDefaultNodeCapability {
+ public:
+  EtGlowEinsumNodeCapability(CapabilityMode mode, const ONNXSymbolsShapes& user_parametric_shapes_replacements);
+
+  EtGlowNodeSupportResult Supported(const Node& node, const GraphViewer& graph_viewer) const override;
+};
+
+// This class decides if Expand is supported or not
+class EtGlowExpandNodeCapability : public EtGlowDefaultNodeCapability {
+ public:
+  EtGlowExpandNodeCapability(CapabilityMode mode, const ONNXSymbolsShapes& user_parametric_shapes_replacements);
+
+  EtGlowNodeSupportResult Supported(const Node& node, const GraphViewer& graph_viewer) const override;
+
+ private:
+  EtGlowNodeSupportResult IsDimensionSupported(const Node& node, const GraphViewer& graph_viewer) const override;
+};
+
+// This class decides if Flatten is supported or not
+class EtGlowFlattenNodeCapability : public EtGlowDefaultNodeCapability {
+ public:
+  using EtGlowDefaultNodeCapability::EtGlowDefaultNodeCapability;
+
+  EtGlowNodeSupportResult Supported(const Node& node, const GraphViewer& graph_viewer) const override;
+
+ private:
+  EtGlowNodeSupportResult IsAttributeSupported(const Node& node, const GraphViewer& graph_viewer) const override;
+};
+
+// This class decides if GRU is supported or not
+class EtGlowGRUNodeCapability : public EtGlowOneLayerNodeCapability {
+ public:
+  using EtGlowOneLayerNodeCapability::EtGlowOneLayerNodeCapability;
+
+  EtGlowNodeSupportResult Supported(const Node& node, const GraphViewer& graph_viewer) const override;
+
+ private:
+  EtGlowNodeSupportResult IsAttributeSupported(const Node& node, const GraphViewer& graph_viewer) const override;
+
+  int GetNumSupportedActivationsSize() const final { return 2; }
+};
+
+// This class decides if Gemm is supported or not
+class EtGlowGemmNodeCapability : public EtGlowDefaultNodeCapability {
+ public:
+  using EtGlowDefaultNodeCapability::EtGlowDefaultNodeCapability;
+
+  EtGlowNodeSupportResult Supported(const Node& node, const GraphViewer& graph_viewer) const override;
+
+ private:
+  EtGlowNodeSupportResult IsTypeSupported(const Node& node) const override;
+  EtGlowNodeSupportResult IsDimensionSupported(const Node& node, const GraphViewer& graph_viewer) const override;
+};
+
+// This class decides if If is supported or not
+class EtGlowIfNodeCapability : public EtGlowDefaultNodeCapability {
+ public:
+  EtGlowIfNodeCapability(CapabilityMode mode, const ONNXSymbolsShapes& user_parametric_shapes_replacements);
+
+  EtGlowNodeSupportResult Supported(const Node& node, const GraphViewer& graph_viewer) const override;
+
+ private:
+  EtGlowNodeSupportResult IsDimensionSupported(const Node& node, const GraphViewer& graph_viewer) const override;
+  EtGlowNodeSupportResult IsAttributeSupported(const Node& node, const GraphViewer& graph_viewer) const override;
+
+  std::vector<ORT_DataType> GetSupportedBTypes() const { return {type_bool}; }
+};
+
+// This class decides if LSTM op is supported or not
+class EtGlowLSTMNodeCapability : public EtGlowOneLayerNodeCapability {
+ public:
+  using EtGlowOneLayerNodeCapability::EtGlowOneLayerNodeCapability;
+
+  EtGlowNodeSupportResult Supported(const Node& node, const GraphViewer& graph_viewer) const override;
+
+ private:
+  EtGlowNodeSupportResult IsAttributeSupported(const Node& node, const GraphViewer& graph_viewer) const override;
+
+  int GetNumSupportedActivationsSize() const final { return 3; }
+};
+
+class EtGlowLayerNormalizationNodeCapability : public EtGlowDefaultNodeCapability {
+ public:
+  using EtGlowDefaultNodeCapability::EtGlowDefaultNodeCapability;
+
+  EtGlowNodeSupportResult Supported(const Node& node, const GraphViewer& graph_viewer) const override;
+
+ private:
+  EtGlowNodeSupportResult IsDimensionSupported(const Node& node, const GraphViewer& graph_viewer) const override;
+  EtGlowNodeSupportResult IsAttributeSupported(const Node& node, const GraphViewer& graph_viewer) const override;
+};
+
+class EtGlowLessNodeCapability : public EtGlowDefaultNodeCapability {
+ public:
+  EtGlowLessNodeCapability(CapabilityMode mode, const ONNXSymbolsShapes& user_parametric_shapes_replacements);
+
+  EtGlowNodeSupportResult Supported(const Node& node, const GraphViewer& graph_viewer) const override;
+
+ private:
+  EtGlowNodeSupportResult IsTypeSupported(const Node& node) const override;
+};
+
+// This class decides if LogSoftmax is supported or not
+class EtGlowLogSoftmaxNodeCapability : public EtGlowDefaultNodeCapability {
+ public:
+  using EtGlowDefaultNodeCapability::EtGlowDefaultNodeCapability;
+
+  EtGlowNodeSupportResult Supported(const Node& node, const GraphViewer& graph_viewer) const override;
+
+ private:
+  EtGlowNodeSupportResult IsDimensionSupported(const Node& node, const GraphViewer& graph_viewer) const override;
+  EtGlowNodeSupportResult IsAttributeSupported(const Node& node, const GraphViewer& graph_viewer) const override;
+};
+
+class EtGlowLoopNodeCapability : public EtGlowDefaultNodeCapability {
+ public:
+  using EtGlowDefaultNodeCapability::EtGlowDefaultNodeCapability;
+
+  EtGlowNodeSupportResult Supported(const Node& node, const GraphViewer& graph_viewer) const override;
+
+ private:
+  EtGlowNodeSupportResult IsDimensionSupported(const Node& node, const GraphViewer& graph_viewer) const override;
+  EtGlowNodeSupportResult IsAttributeSupported(const Node& node, const GraphViewer& graph_viewer) const override;
+};
+
+// This class decides if MatMul op is supported or not
+class EtGlowMatMulNodeCapability : public EtGlowDefaultNodeCapability {
+ public:
+  using EtGlowDefaultNodeCapability::EtGlowDefaultNodeCapability;
+
+  EtGlowNodeSupportResult Supported(const Node& node, const GraphViewer& graph_viewer) const override;
+
+ private:
+  EtGlowNodeSupportResult IsDimensionSupported(const Node& node, const GraphViewer& graph_viewer) const override;
+};
+
+// This class decides if MatMulNBits is supported or not
+class EtGlowMatMulNBitsNodeCapability : public EtGlowDefaultNodeCapability {
+ public:
+  using EtGlowDefaultNodeCapability::EtGlowDefaultNodeCapability;
+
+  EtGlowNodeSupportResult Supported(const Node& node, const GraphViewer& graph_viewer) const override;
+
+ private:
+  EtGlowNodeSupportResult IsTypeSupported(const Node& node) const override;
+  EtGlowNodeSupportResult IsDimensionSupported(const Node& node, const GraphViewer& graph_viewer) const override;
+  EtGlowNodeSupportResult IsAttributeSupported(const Node& node, const GraphViewer& graph_viewer) const override;
+};
+
+class EtGlowNonMaxSuppressionNodeCapability : public EtGlowDefaultNodeCapability {
+ public:
+  using EtGlowDefaultNodeCapability::EtGlowDefaultNodeCapability;
+
+  EtGlowNodeSupportResult Supported(const Node& node, const GraphViewer& graph_viewer) const override;
+
+ private:
+  EtGlowNodeSupportResult IsDimensionSupported(const Node& node, const GraphViewer& graph_viewer) const override;
+};
+
+class EtGlowNonZeroNodeCapability : public EtGlowDefaultNodeCapability {
+ public:
+  EtGlowNonZeroNodeCapability(CapabilityMode mode, const ONNXSymbolsShapes& user_parametric_shapes_replacements);
+
+  EtGlowNodeSupportResult Supported(const Node& node, const GraphViewer& graph_viewer) const override;
+
+ private:
+  EtGlowNodeSupportResult IsDimensionSupported(const Node& node, const GraphViewer& graph_viewer) const override;
+};
+
+// This class decides if Pad is supported or not
+class EtGlowPadNodeCapability : public EtGlowDefaultNodeCapability {
+ public:
+  using EtGlowDefaultNodeCapability::EtGlowDefaultNodeCapability;
+
+  EtGlowNodeSupportResult Supported(const Node& node, const GraphViewer& graph_viewer) const override;
+
+ private:
+  EtGlowNodeSupportResult IsDimensionSupported(const Node& node, const GraphViewer& graph_viewer) const override;
+  EtGlowNodeSupportResult IsAttributeSupported(const Node& node, const GraphViewer& graph_viewer) const override;
+
+  EtGlowNodeSupportResult HasValidPadsShape(int pads_dim_size, int input_dim_size) const;
+  template <typename PadTensorT>
+  EtGlowNodeSupportResult HasValidPadsValues(int pads_dim_size, const PadTensorT& pad_dim_values, const NodeArg& input) const {
+    const auto& input_shape = *input.Shape();
+    const auto& param2value_replacements = GetParametricShapeReplacements();
+    std::vector<int> input_dim_values;
+    input_dim_values.reserve(input_shape.dim_size());
+    for (const auto& input_dim : input_shape.dim()) {
+      ORT_RETURN_IF_NOT_SUPPORTED(HasValueForParametricDim(input_dim, param2value_replacements, input.Name()));
+      input_dim_values.emplace_back(input_dim.has_dim_value() ? input_dim.dim_value() : param2value_replacements.get_safe<std::size_t>(input_dim.dim_param()).value_or(0));
+    }
+    const auto input_dim_size = input_dim_values.size();
+    if (pads_dim_size != 2 * static_cast<int>(input_dim_size)) {
+      return make_unsupported("must be 2 pad values per input. Found pads_dim_size: " + std::to_string(pads_dim_size) + " input_dim_size: " + std::to_string(input_dim_size));
+    }
+    for (size_t i = 0; i < input_dim_size; ++i) {
+      auto input_dim_value = input_dim_values[0];
+      auto pad_dim_value = pad_dim_values[i];
+      auto pad_dim_value_next = pad_dim_values[i + input_dim_size];
+      auto new_dim_value = input_dim_value + pad_dim_value + pad_dim_value_next;
+      if (new_dim_value <= 0) {
+        return make_unsupported("pads can't remove all elements of a dimension");
+      }
+    }
+    return make_supported();
+  }
+};
+
+// This class decides if Pow is supported or not
+class EtGlowPowNodeCapability : public EtGlowDefaultNodeCapability {
+ public:
+  EtGlowPowNodeCapability(CapabilityMode mode, const ONNXSymbolsShapes& user_parametric_shapes_replacements);
+
+  EtGlowNodeSupportResult Supported(const Node& node, const GraphViewer& graph_viewer) const override;
+
+ private:
+  EtGlowNodeSupportResult IsTypeSupported(const Node& node) const override;
+};
+
+// This class decides if QuantizeLinear is supported or not
+class EtGlowQuantizeLinearNodeCapability : public EtGlowDefaultNodeCapability {
+ public:
+  EtGlowQuantizeLinearNodeCapability(CapabilityMode mode, const ONNXSymbolsShapes& use_parametric_shapes_replacements);
+
+  EtGlowNodeSupportResult Supported(const Node& node, const GraphViewer& graph_viewer) const override;
+
+ private:
+  EtGlowNodeSupportResult IsTypeSupported(const Node& node) const override;
+  EtGlowNodeSupportResult IsDimensionSupported(const Node& node, const GraphViewer& graph_viewer) const override;
+
+  std::vector<ORT_DataType> GetSupportedT1Types() const { return GetSupportedInputTypes(); }
+  std::vector<ORT_DataType> GetSupportedT2Types() const { return GetSupportedOutputTypes(); }
+};
+
+// This class decides if PRelu is supported or not
+class EtGlowPReluNodeCapability : public EtGlowDefaultNodeCapability {
+ public:
+  using EtGlowDefaultNodeCapability::EtGlowDefaultNodeCapability;
+
+  EtGlowNodeSupportResult Supported(const Node& node, const GraphViewer& graph_viewer) const override;
+
+ private:
+  EtGlowNodeSupportResult IsDimensionSupported(const Node& node, const GraphViewer& graph_viewer) const override;
+};
+
+// This class decides if RNN is supported or not
+class EtGlowRNNNodeCapability : public EtGlowOneLayerNodeCapability {
+ public:
+  using EtGlowOneLayerNodeCapability::EtGlowOneLayerNodeCapability;
+
+  EtGlowNodeSupportResult Supported(const Node& node, const GraphViewer& graph_viewer) const override;
+
+ private:
+  EtGlowNodeSupportResult IsAttributeSupported(const Node& node, const GraphViewer& graph_viewer) const override;
+
+  int GetNumSupportedActivationsSize() const final { return 3; };
+};
+
+// This class decides if Range is supported or not
+class EtGlowRangeNodeCapability : public EtGlowDefaultNodeCapability {
+ public:
+  EtGlowRangeNodeCapability(CapabilityMode mode, const ONNXSymbolsShapes& user_parametric_shapes_replacements);
+
+  EtGlowNodeSupportResult Supported(const Node& node, const GraphViewer& graph_viewer) const override;
+
+ private:
+  EtGlowNodeSupportResult IsDimensionSupported(const Node& node, const GraphViewer& graph_viewer) const override;
+};
+
+// This class decides if ReduceL2 is supported or not
+class EtGlowReduceL2NodeCapability : public EtGlowReduceNodeCapability {
+ public:
+  EtGlowReduceL2NodeCapability(CapabilityMode mode, const ONNXSymbolsShapes& user_parametric_shapes_replacements);
+
+  EtGlowNodeSupportResult Supported(const Node& node, const GraphViewer& graph_viewer) const override;
+
+ private:
+  EtGlowNodeSupportResult IsAttributeSupported(const Node& node, const GraphViewer& graph_viewer) const override;
+};
+
+// This class decides if Reshape is supported or not
+class EtGlowReshapeNodeCapability : public EtGlowDefaultNodeCapability {
+ public:
+  using EtGlowDefaultNodeCapability::EtGlowDefaultNodeCapability;
+
+  EtGlowNodeSupportResult Supported(const Node& node, const GraphViewer& graph_viewer) const override;
+
+ private:
+  EtGlowNodeSupportResult IsDimensionSupported(const Node& node, const GraphViewer& graph_viewer) const override;
+  EtGlowNodeSupportResult IsAttributeSupported(const Node& node, const GraphViewer& graph_viewer) const override;
+};
+
+// This class decides if Resize is supported or not
+class EtGlowResizeNodeCapability : public EtGlowDefaultNodeCapability {
+ public:
+  using EtGlowDefaultNodeCapability::EtGlowDefaultNodeCapability;
+
+  EtGlowNodeSupportResult Supported(const Node& node, const GraphViewer& graph_viewer) const override;
+
+ private:
+  EtGlowNodeSupportResult IsDimensionSupported(const Node& node, const GraphViewer& graph_viewer) const override;
+  EtGlowNodeSupportResult IsAttributeSupported(const Node& node, const GraphViewer& graph_viewer) const override;
+};
+
+// This class decides if ScatterND is supported or not
+class EtGlowScatterNDNodeCapability : public EtGlowDefaultNodeCapability {
+ public:
+  using EtGlowDefaultNodeCapability::EtGlowDefaultNodeCapability;
+
+  EtGlowNodeSupportResult Supported(const Node& node, const GraphViewer& graph_viewer) const override;
+
+ private:
+  EtGlowNodeSupportResult IsAttributeSupported(const Node& node, const GraphViewer& graph_viewer) const override;
+};
+
+// This class decides if Shape is supported or not
+class EtGlowShapeNodeCapability : public EtGlowDefaultNodeCapability {
+ public:
+  using EtGlowDefaultNodeCapability::EtGlowDefaultNodeCapability;
+
+  EtGlowNodeSupportResult Supported(const Node& node, const GraphViewer& graph_viewer) const override;
+
+ private:
+  EtGlowNodeSupportResult IsDimensionSupported(const Node& node, const GraphViewer& graph_viewer) const override;
+};
+
+class EtGlowSimplifiedLayerNormalizationNodeCapability : public EtGlowDefaultNodeCapability {
+ public:
+  using EtGlowDefaultNodeCapability::EtGlowDefaultNodeCapability;
+
+  EtGlowNodeSupportResult Supported(const Node& node, const GraphViewer& graph_viewer) const override;
+
+ private:
+  EtGlowNodeSupportResult IsDimensionSupported(const Node& node, const GraphViewer& graph_viewer) const override;
+  EtGlowNodeSupportResult IsAttributeSupported(const Node& node, const GraphViewer& graph_viewer) const override;
+};
+
+// This class decides if Slice is supported or not
+class EtGlowSliceNodeCapability : public EtGlowDefaultNodeCapability {
+ public:
+  using EtGlowDefaultNodeCapability::EtGlowDefaultNodeCapability;
+
+  EtGlowNodeSupportResult Supported(const Node& node, const GraphViewer& graph_viewer) const override;
+
+ private:
+  EtGlowNodeSupportResult IsDimensionSupported(const Node& node, const GraphViewer& graph_viewer) const override;
+  EtGlowNodeSupportResult IsAttributeSupported(const Node& node, const GraphViewer& graph_viewer) const override;
+
+  std::vector<ORT_DataType> GetSupportedTindTypes() const { return TindTypes_; }
+
+  std::vector<ORT_DataType> TindTypes_{type_int64, type_int32};
+};
+
+// This class decides if Softmax is supported or not
+class EtGlowSoftmaxNodeCapability : public EtGlowDefaultNodeCapability {
+ public:
+  using EtGlowDefaultNodeCapability::EtGlowDefaultNodeCapability;
+
+  EtGlowNodeSupportResult Supported(const Node& node, const GraphViewer& graph_viewer) const override;
+
+ private:
+  EtGlowNodeSupportResult IsDimensionSupported(const Node& node, const GraphViewer& graph_viewer) const override;
+};
+
+// This class decides if Split is supported or not
+class EtGlowSplitNodeCapability : public EtGlowDefaultNodeCapability {
+ public:
+  explicit EtGlowSplitNodeCapability(CapabilityMode mode, const ONNXSymbolsShapes& user_parametric_shapes_replacements);
+
+  EtGlowNodeSupportResult Supported(const Node& node, const GraphViewer& graph_viewer) const override;
+
+ private:
+  EtGlowNodeSupportResult IsTypeSupported(const Node& node) const override;
+  EtGlowNodeSupportResult IsDimensionSupported(const Node& node, const GraphViewer& graph_viewer) const override;
+  EtGlowNodeSupportResult IsAttributeSupported(const Node& node, const GraphViewer& graph_viewer) const override;
+};
+
+// This class decides if Squeeze / Unsqueeze is supported or not
+class EtGlowSqueezeNodeCapability : public EtGlowDefaultNodeCapability {
+ public:
+  using EtGlowDefaultNodeCapability::EtGlowDefaultNodeCapability;
+
+  EtGlowNodeSupportResult Supported(const Node& node, const GraphViewer& graph_viewer) const override;
+
+ private:
+  EtGlowNodeSupportResult IsTypeSupported(const Node& node) const override;
+  EtGlowNodeSupportResult IsDimensionSupported(const Node& node, const GraphViewer& graph_viewer) const override;
+  EtGlowNodeSupportResult IsAttributeSupported(const Node& node, const GraphViewer& graph_viewer) const override;
+};
+
+// This class decides if Sum is supported or not
+class EtGlowSumNodeCapability : public EtGlowDefaultNodeCapability {
+ public:
+  using EtGlowDefaultNodeCapability::EtGlowDefaultNodeCapability;
+
+  EtGlowNodeSupportResult Supported(const Node& node, const GraphViewer& graph_viewer) const override;
+
+ private:
+  EtGlowNodeSupportResult IsDimensionSupported(const Node& node, const GraphViewer& graph_viewer) const override;
+};
+
+// This class decides if Tile is supported or not
+class EtGlowTileNodeCapability : public EtGlowDefaultNodeCapability {
+ public:
+  EtGlowTileNodeCapability(CapabilityMode mode, const ONNXSymbolsShapes& user_parametric_shapes_replacements);
+
+  EtGlowNodeSupportResult Supported(const Node& node, const GraphViewer& graph_viewer) const override;
+
+ private:
+  EtGlowNodeSupportResult IsTypeSupported(const Node& node) const override;
+  EtGlowNodeSupportResult IsDimensionSupported(const Node& node, const GraphViewer& graph_viewer) const override;
+};
+
+// This class decides if TopK is supported or not
+class EtGlowTopKNodeCapability : public EtGlowDefaultNodeCapability {
+ public:
+  EtGlowTopKNodeCapability(CapabilityMode mode, const ONNXSymbolsShapes& user_parametric_shapes_replacements);
+
+  EtGlowNodeSupportResult Supported(const Node& node, const GraphViewer& graph_viewer) const override;
+
+ private:
+  EtGlowNodeSupportResult IsTypeSupported(const Node& node) const override;
+  EtGlowNodeSupportResult IsDimensionSupported(const Node& node, const GraphViewer& graph_viewer) const override;
+  EtGlowNodeSupportResult IsAttributeSupported(const Node& node, const GraphViewer& graph_viewer) const override;
+
+  std::vector<ORT_DataType> GetSupportedTTypes() const { return {type_float32, type_float16, type_int16, type_int8}; }
+  std::vector<ORT_DataType> GetSupportedITypes() const { return {type_int64, type_int32}; }
+};
+
+// This class decides if Trilu is supported or not
+class EtGlowTriluNodeCapability : public EtGlowDefaultNodeCapability {
+ public:
+  using EtGlowDefaultNodeCapability::EtGlowDefaultNodeCapability;
+
+  EtGlowNodeSupportResult Supported(const Node& node, const GraphViewer& graph_viewer) const override;
+
+ private:
+  EtGlowNodeSupportResult IsDimensionSupported(const Node& node, const GraphViewer& graph_viewer) const override;
+  EtGlowNodeSupportResult IsAttributeSupported(const Node& node, const GraphViewer& graph_viewer) const override;
+};
+
+// This class decides if Upsample is supported or not
+class EtGlowUpsampleNodeCapability : public EtGlowDefaultNodeCapability {
+ public:
+  using EtGlowDefaultNodeCapability::EtGlowDefaultNodeCapability;
+
+  EtGlowNodeSupportResult Supported(const Node& node, const GraphViewer& graph_viewer) const override;
+
+ private:
+  EtGlowNodeSupportResult IsDimensionSupported(const Node& node, const GraphViewer& graph_viewer) const override;
+  EtGlowNodeSupportResult IsAttributeSupported(const Node& node, const GraphViewer& graph_viewer) const override;
+};
+
+// This class decides if Where is supported or not
+class EtGlowWhereNodeCapability : public EtGlowDefaultNodeCapability {
+ public:
+  EtGlowWhereNodeCapability(CapabilityMode mode, const ONNXSymbolsShapes& user_parametric_shapes_replacements);
+
+  EtGlowNodeSupportResult Supported(const Node& node, const GraphViewer& graph_viewer) const override;
+
+ private:
+  EtGlowNodeSupportResult IsTypeSupported(const Node& node) const override;
+  EtGlowNodeSupportResult IsDimensionSupported(const Node& node, const GraphViewer& graph_viewer) const override;
+
+  std::vector<ORT_DataType> GetSupportedTTypes() const { return TTypes_; }
+
+  std::vector<ORT_DataType> TTypes_{type_float32, type_float16, type_int64, type_int32, type_int16, type_int8, type_bool};
+};
+
+}  // end namespace onnxruntime
diff --git a/onnxruntime/core/providers/etglow/etglow_op_manager.cc b/onnxruntime/core/providers/etglow/etglow_op_manager.cc
new file mode 100644
index 0000000000..adcf7f45b3
--- /dev/null
+++ b/onnxruntime/core/providers/etglow/etglow_op_manager.cc
@@ -0,0 +1,268 @@
+// Copyright (c) Microsoft Corporation. All rights reserved.
+// Licensed under the MIT License.
+
+#include "etglow_op_manager.h"
+#include "etglow_node_capability.h"
+
+namespace onnxruntime {
+
+EtGlowOpManager::EtGlowOpManager(const ONNXSymbolsShapes& supp_param_shapes)
+    : user_parametric_shapes_replacements_{supp_param_shapes} {
+  RegisterSupportedOps();
+}
+
+void EtGlowOpManager::RegisterSupportedOps() {
+  // Naming convention for EtGlowOpManager supported kernels
+#define ONNX_OPERATOR_SUPPORT_CLASS_NAME(provider, name) \
+  provider##name##NodeCapability
+#define ONNX_OPERATOR_SUPPORT_FAMILY_NAME(provider, family) \
+  provider##family##NodeCapability
+#define ONNX_OPERATOR_KEY_NAME(domain, name) \
+  std::string{(domain)} + std::string { (#name) }
+
+#define ONNX_OPERATOR_REGISTER_EX(key_name, class_name, ...) \
+  etglow_ops_map_.try_emplace(key_name, std::make_unique<class_name>(__VA_ARGS__))
+
+#define ONNX_OPERATOR_NOT_SUPPORTED(domain, name, reason) \
+  ONNX_OPERATOR_REGISTER_EX(ONNX_OPERATOR_KEY_NAME(domain, name), EtGlowNotSupportedNodeCapability, reason)
+
+#define ONNX_OPERATOR_SUPPORTED_FAMILY(domain, name, family, ...) \
+  ONNX_OPERATOR_REGISTER_EX(ONNX_OPERATOR_KEY_NAME(domain, name), ONNX_OPERATOR_SUPPORT_FAMILY_NAME(EtGlow, family), mode_, user_parametric_shapes_replacements_ __VA_OPT__(, ) __VA_ARGS__)
+#define ONNX_OPERATOR_SUPPORTED_CLASS(domain, name, ...) \
+  ONNX_OPERATOR_REGISTER_EX(ONNX_OPERATOR_KEY_NAME(domain, name), ONNX_OPERATOR_SUPPORT_CLASS_NAME(EtGlow, name), mode_, user_parametric_shapes_replacements_ __VA_OPT__(, ) __VA_ARGS__)
+#define ONNX_OPERATOR_SUPPORTED_DEFAULT(domain, name, ...) \
+  ONNX_OPERATOR_REGISTER_EX(ONNX_OPERATOR_KEY_NAME(domain, name), EtGlowDefaultNodeCapability, mode_, user_parametric_shapes_replacements_ __VA_OPT__(, ) __VA_ARGS__)
+
+  static const std::string k_not_supported_by_ort_numerical_stability = "not supported by ORT due lack of numerical stability guarantees";
+  static const std::string k_not_supported_in_glow_backend = "not in ETSOC GlowOpSupported list";
+  static const std::string k_not_supported_in_glow_fronted = "not supported in glow frontend";
+  static const std::string k_not_correctly_implemented = "current glow implementation does not follow ONNX standard";
+  static const std::string k_unclassified = "not yet classified";
+
+  // clang-format off
+
+  //********* list of supported nodes *********//
+  ONNX_OPERATOR_SUPPORTED_CLASS(  kOnnxDomain,  Add);
+  ONNX_OPERATOR_SUPPORTED_DEFAULT(kOnnxDomain,  And,                std::vector<ORT_DataType>{type_bool}, std::vector<ORT_DataType>{type_bool});
+  ONNX_OPERATOR_SUPPORTED_CLASS(  kOnnxDomain,  ArgMax);
+  ONNX_OPERATOR_SUPPORTED_FAMILY( kOnnxDomain,  AveragePool, Pool);
+  ONNX_OPERATOR_SUPPORTED_CLASS(  kOnnxDomain,  BatchNormalization);
+  ONNX_OPERATOR_SUPPORTED_CLASS(  kOnnxDomain,  Cast);
+  ONNX_OPERATOR_SUPPORTED_CLASS(  kOnnxDomain,  Clip);
+  ONNX_OPERATOR_SUPPORTED_CLASS(  kOnnxDomain,  Concat);
+  ONNX_OPERATOR_SUPPORTED_CLASS(  kOnnxDomain,  Constant);
+  ONNX_OPERATOR_SUPPORTED_CLASS(  kOnnxDomain,  ConstantOfShape);
+  ONNX_OPERATOR_SUPPORTED_CLASS(  kOnnxDomain,  Conv);
+  ONNX_OPERATOR_SUPPORTED_CLASS(  kOnnxDomain,  ConvTranspose);
+  ONNX_OPERATOR_SUPPORTED_DEFAULT(kOnnxDomain,  Cos);
+  ONNX_OPERATOR_SUPPORTED_CLASS(  kOnnxDomain,  CumSum);
+  ONNX_OPERATOR_SUPPORTED_DEFAULT(kOnnxDomain,  DepthToSpace);
+  ONNX_OPERATOR_SUPPORTED_DEFAULT(kOnnxDomain,  Div, std::vector<ORT_DataType>{type_int8, type_int32, type_int64, type_bfloat16, type_float16, type_float32});
+  ONNX_OPERATOR_SUPPORTED_CLASS(  kOnnxDomain,  Dropout);
+  ONNX_OPERATOR_SUPPORTED_CLASS(  kOnnxDomain,  Einsum);
+  ONNX_OPERATOR_SUPPORTED_DEFAULT(kOnnxDomain,  Equal,  std::vector<ORT_DataType>{type_int16, type_int32, type_int64, type_bfloat16, type_float16, type_float32}, std::vector<ORT_DataType>{type_bool}); //OBS: only validated that 'string' is not supported and 'int32' is supported
+  ONNX_OPERATOR_SUPPORTED_DEFAULT(kOnnxDomain,  Erf,    std::vector<ORT_DataType>{type_int8, type_float16, type_float32});
+  ONNX_OPERATOR_SUPPORTED_DEFAULT(kOnnxDomain,  Exp,    std::vector<ORT_DataType>{type_float16, type_float32});
+  ONNX_OPERATOR_SUPPORTED_CLASS(  kOnnxDomain,  Expand);
+  ONNX_OPERATOR_SUPPORTED_CLASS(  kOnnxDomain,  Flatten);
+  ONNX_OPERATOR_SUPPORTED_CLASS(  kOnnxDomain,  GRU);
+  ONNX_OPERATOR_SUPPORTED_DEFAULT(kOnnxDomain,  Gather, std::vector<ORT_DataType>{type_float32, type_int32, type_int64});
+  ONNX_OPERATOR_SUPPORTED_CLASS(  kOnnxDomain,  Gemm);
+  ONNX_OPERATOR_SUPPORTED_DEFAULT(kOnnxDomain,  GlobalAveragePool);
+  ONNX_OPERATOR_SUPPORTED_DEFAULT(kOnnxDomain,  Greater,        std::vector<ORT_DataType>{type_float16, type_float32, type_int8, type_int32, type_int64}, std::vector<ORT_DataType>{type_bool});
+  ONNX_OPERATOR_SUPPORTED_DEFAULT(kOnnxDomain,  GreaterOrEqual, std::vector<ORT_DataType>{type_float16, type_float32, type_int8, type_int32, type_int64}, std::vector<ORT_DataType>{type_bool});
+  ONNX_OPERATOR_SUPPORTED_DEFAULT(kOnnxDomain,  HardSigmoid);
+  ONNX_OPERATOR_SUPPORTED_DEFAULT(kOnnxDomain,  Identity, std::vector<ORT_DataType>{type_uint16, type_uint32, type_uint64, type_int8, type_int16, type_int32, type_int64, type_bfloat16, type_float16, type_float32, type_double, type_bool}); // type_string, type_uint8 not handled well by glow
+  ONNX_OPERATOR_SUPPORTED_CLASS(  kOnnxDomain,  If);
+  ONNX_OPERATOR_SUPPORTED_DEFAULT(kOnnxDomain,  InstanceNormalization);
+  ONNX_OPERATOR_SUPPORTED_DEFAULT(kOnnxDomain,  LRN);
+  ONNX_OPERATOR_SUPPORTED_CLASS(  kOnnxDomain,  LSTM);
+  ONNX_OPERATOR_SUPPORTED_CLASS(  kOnnxDomain,  LayerNormalization);
+  ONNX_OPERATOR_SUPPORTED_DEFAULT(kOnnxDomain,  LeakyRelu);
+  ONNX_OPERATOR_SUPPORTED_CLASS(  kOnnxDomain,  Less);
+  ONNX_OPERATOR_SUPPORTED_DEFAULT(kOnnxDomain,  Log);
+  ONNX_OPERATOR_SUPPORTED_CLASS(  kOnnxDomain,  LogSoftmax);
+  ONNX_OPERATOR_SUPPORTED_CLASS(  kOnnxDomain,  Loop);
+  ONNX_OPERATOR_SUPPORTED_CLASS(  kOnnxDomain,  MatMul);
+  ONNX_OPERATOR_SUPPORTED_DEFAULT(kOnnxDomain,  Max, std::vector<ORT_DataType>{type_float32, type_float16, type_bfloat16, type_int8, type_int32, type_int64});
+  ONNX_OPERATOR_SUPPORTED_FAMILY(  kOnnxDomain, MaxPool, Pool);
+  ONNX_OPERATOR_SUPPORTED_DEFAULT(kOnnxDomain,  Mean);
+  ONNX_OPERATOR_SUPPORTED_DEFAULT(kOnnxDomain,  Min);
+  ONNX_OPERATOR_SUPPORTED_DEFAULT(kOnnxDomain,  Mul);
+  ONNX_OPERATOR_SUPPORTED_DEFAULT(kOnnxDomain,  Neg, std::vector<ORT_DataType>{type_float32, type_float16});
+  ONNX_OPERATOR_SUPPORTED_CLASS(  kOnnxDomain,  NonMaxSuppression);
+  ONNX_OPERATOR_SUPPORTED_CLASS(  kOnnxDomain,  NonZero);
+  ONNX_OPERATOR_SUPPORTED_DEFAULT(kOnnxDomain,  Not);
+  ONNX_OPERATOR_SUPPORTED_DEFAULT(kOnnxDomain,  Or);
+  ONNX_OPERATOR_SUPPORTED_CLASS(  kOnnxDomain,  PRelu);
+  ONNX_OPERATOR_SUPPORTED_CLASS(  kOnnxDomain,  Pad);
+  ONNX_OPERATOR_SUPPORTED_CLASS(  kOnnxDomain,  Pow);
+  ONNX_OPERATOR_SUPPORTED_CLASS(  kOnnxDomain,  QuantizeLinear);
+  ONNX_OPERATOR_SUPPORTED_CLASS(  kOnnxDomain,  RNN);
+  ONNX_OPERATOR_SUPPORTED_CLASS(  kOnnxDomain,  Range);
+  ONNX_OPERATOR_SUPPORTED_DEFAULT(kOnnxDomain,  Reciprocal);
+  ONNX_OPERATOR_SUPPORTED_CLASS(  kOnnxDomain,  ReduceL2);
+  ONNX_OPERATOR_SUPPORTED_FAMILY( kOnnxDomain,  ReduceMin,       Reduce); // implemented as a BatchedReduceMin
+  ONNX_OPERATOR_SUPPORTED_FAMILY( kOnnxDomain,  ReduceSum,       Reduce); // implemented as a BatchedReduceAdd
+  ONNX_OPERATOR_SUPPORTED_FAMILY( kOnnxDomain,  ReduceMean,      Reduce); // lowered from BatchedReduceMean      to BatchedReduceAdd+BroadcastScalar
+  ONNX_OPERATOR_SUPPORTED_FAMILY( kOnnxDomain,  ReduceSumSquare, Reduce); // lowered from BatchedReduceSumSquare to Mul+BatchedReduceAdd
+  ONNX_OPERATOR_SUPPORTED_DEFAULT(kOnnxDomain,  Relu, std::vector<ORT_DataType>{type_bfloat16, type_float32, type_float16, type_int64, type_int32, type_int16});
+  ONNX_OPERATOR_SUPPORTED_CLASS(  kOnnxDomain,  Reshape);
+  ONNX_OPERATOR_SUPPORTED_CLASS(  kOnnxDomain,  Resize);
+  ONNX_OPERATOR_SUPPORTED_CLASS(  kOnnxDomain,  ScatterND);
+  ONNX_OPERATOR_SUPPORTED_CLASS(  kOnnxDomain,  Shape);
+  ONNX_OPERATOR_SUPPORTED_DEFAULT(kOnnxDomain,  Sigmoid);
+  ONNX_OPERATOR_SUPPORTED_CLASS(  kOnnxDomain,  SimplifiedLayerNormalization);
+  ONNX_OPERATOR_SUPPORTED_DEFAULT(kOnnxDomain,  Sin);
+  ONNX_OPERATOR_SUPPORTED_CLASS(  kOnnxDomain,  Slice);
+  ONNX_OPERATOR_SUPPORTED_CLASS(  kOnnxDomain,  Softmax);
+  ONNX_OPERATOR_SUPPORTED_DEFAULT(kOnnxDomain,  SpaceToDepth);
+  ONNX_OPERATOR_SUPPORTED_CLASS(  kOnnxDomain,  Split);
+  ONNX_OPERATOR_SUPPORTED_DEFAULT(kOnnxDomain,  Sqrt);
+  ONNX_OPERATOR_SUPPORTED_CLASS(  kOnnxDomain,  Squeeze);
+  ONNX_OPERATOR_SUPPORTED_DEFAULT(kOnnxDomain,  Sub);
+  ONNX_OPERATOR_SUPPORTED_CLASS(  kOnnxDomain,  Sum);
+  ONNX_OPERATOR_SUPPORTED_DEFAULT(kOnnxDomain,  Tanh);
+  ONNX_OPERATOR_SUPPORTED_CLASS(  kOnnxDomain,  Tile);
+  ONNX_OPERATOR_SUPPORTED_CLASS(  kOnnxDomain,  TopK);
+  ONNX_OPERATOR_SUPPORTED_DEFAULT(kOnnxDomain,  Transpose);
+  ONNX_OPERATOR_SUPPORTED_CLASS(  kOnnxDomain,  Trilu);
+  ONNX_OPERATOR_SUPPORTED_FAMILY(  kOnnxDomain, Unsqueeze, Squeeze);
+  ONNX_OPERATOR_SUPPORTED_CLASS(  kOnnxDomain,  Upsample);
+  ONNX_OPERATOR_SUPPORTED_CLASS(  kOnnxDomain,  Where);
+  ONNX_OPERATOR_SUPPORTED_DEFAULT(kOnnxDomain,  Xor);
+  //************************************//
+
+  //********* list of contrib supported nodes *********//
+  ONNX_OPERATOR_SUPPORTED_CLASS(  kMSDomain,    MatMulNBits);
+  //************************************//
+
+  // list of explicitly unsupported nodes
+  ONNX_OPERATOR_NOT_SUPPORTED(kOnnxDomain, Abs,                       k_not_supported_in_glow_backend);
+  ONNX_OPERATOR_NOT_SUPPORTED(kOnnxDomain, Acos,                      k_not_supported_in_glow_backend);
+  ONNX_OPERATOR_NOT_SUPPORTED(kOnnxDomain, Acosh,                     k_not_supported_in_glow_fronted);
+  ONNX_OPERATOR_NOT_SUPPORTED(kOnnxDomain, AffineGrid,                k_not_supported_in_glow_fronted);
+  ONNX_OPERATOR_NOT_SUPPORTED(kOnnxDomain, ArgMin,                    k_not_supported_in_glow_backend);
+  ONNX_OPERATOR_NOT_SUPPORTED(kOnnxDomain, Asin,                      k_not_supported_in_glow_backend);
+  ONNX_OPERATOR_NOT_SUPPORTED(kOnnxDomain, Asinh,                     k_not_supported_in_glow_fronted);
+  ONNX_OPERATOR_NOT_SUPPORTED(kOnnxDomain, Atan,                      k_not_supported_in_glow_backend);
+  ONNX_OPERATOR_NOT_SUPPORTED(kOnnxDomain, Atanh,                     k_not_supported_in_glow_fronted);
+  ONNX_OPERATOR_NOT_SUPPORTED(kOnnxDomain, Bernoulli,                 k_not_supported_by_ort_numerical_stability);
+  ONNX_OPERATOR_NOT_SUPPORTED(kOnnxDomain, BitShift,                  k_not_supported_in_glow_fronted);
+  ONNX_OPERATOR_NOT_SUPPORTED(kOnnxDomain, BitwiseAnd,                k_not_supported_in_glow_fronted);
+  ONNX_OPERATOR_NOT_SUPPORTED(kOnnxDomain, BitwiseNot,                k_not_supported_in_glow_fronted);
+  ONNX_OPERATOR_NOT_SUPPORTED(kOnnxDomain, BitwiseOr,                 k_not_supported_in_glow_fronted);
+  ONNX_OPERATOR_NOT_SUPPORTED(kOnnxDomain, BitwiseXor,                k_not_supported_in_glow_fronted);
+  ONNX_OPERATOR_NOT_SUPPORTED(kOnnxDomain, BlackmanWindow,            k_not_supported_in_glow_fronted);
+  ONNX_OPERATOR_NOT_SUPPORTED(kOnnxDomain, CastLike,                  k_not_supported_by_ort_numerical_stability);
+  ONNX_OPERATOR_NOT_SUPPORTED(kOnnxDomain, Ceil,                      k_not_supported_in_glow_backend);
+  ONNX_OPERATOR_NOT_SUPPORTED(kOnnxDomain, Celu,                      k_not_supported_in_glow_fronted);
+  ONNX_OPERATOR_NOT_SUPPORTED(kOnnxDomain, CenterCropPad,             k_not_supported_in_glow_fronted);
+  ONNX_OPERATOR_NOT_SUPPORTED(kOnnxDomain, Col2Im,                    k_not_supported_in_glow_fronted);
+  ONNX_OPERATOR_NOT_SUPPORTED(kOnnxDomain, Cosh,                      k_not_supported_in_glow_fronted);
+  ONNX_OPERATOR_NOT_SUPPORTED(kOnnxDomain, Compress,                  k_not_supported_in_glow_fronted);
+  ONNX_OPERATOR_NOT_SUPPORTED(kOnnxDomain, ConcatFromSequence,        k_not_supported_in_glow_fronted);
+  ONNX_OPERATOR_NOT_SUPPORTED(kOnnxDomain, ConvInteger,               k_not_supported_in_glow_fronted);
+  ONNX_OPERATOR_NOT_SUPPORTED(kOnnxDomain, DFT,                       k_not_supported_in_glow_fronted);
+  ONNX_OPERATOR_NOT_SUPPORTED(kOnnxDomain, DeformConv,                k_not_supported_in_glow_fronted);
+  ONNX_OPERATOR_NOT_SUPPORTED(kOnnxDomain, DequantizeLinear,          k_not_correctly_implemented);
+  ONNX_OPERATOR_NOT_SUPPORTED(kOnnxDomain, Det,                       k_not_supported_in_glow_fronted);
+  ONNX_OPERATOR_NOT_SUPPORTED(kOnnxDomain, DynamicQuantizeLinear,     k_not_supported_in_glow_fronted);
+  ONNX_OPERATOR_NOT_SUPPORTED(kOnnxDomain, Elu,                       k_not_supported_in_glow_fronted);
+  ONNX_OPERATOR_NOT_SUPPORTED(kOnnxDomain, Floor,                     k_not_supported_in_glow_backend);
+  ONNX_OPERATOR_NOT_SUPPORTED(kOnnxDomain, EyeLike,                   k_not_supported_in_glow_fronted);
+  ONNX_OPERATOR_NOT_SUPPORTED(kOnnxDomain, GatherElements,            k_not_supported_in_glow_backend);
+  ONNX_OPERATOR_NOT_SUPPORTED(kOnnxDomain, GatherND,                  k_not_supported_in_glow_backend);
+  ONNX_OPERATOR_NOT_SUPPORTED(kOnnxDomain, Gelu,                      k_not_supported_in_glow_fronted); // obs: glow pytorch frontend supports Gelu
+  ONNX_OPERATOR_NOT_SUPPORTED(kOnnxDomain, GlobalLpPool,              k_not_supported_in_glow_fronted);
+  ONNX_OPERATOR_NOT_SUPPORTED(kOnnxDomain, GlobalMaxPool,             k_not_supported_in_glow_fronted);
+  ONNX_OPERATOR_NOT_SUPPORTED(kOnnxDomain, GridSample,                k_not_supported_in_glow_fronted);
+  ONNX_OPERATOR_NOT_SUPPORTED(kOnnxDomain, GroupNormalization,        k_not_supported_in_glow_fronted);
+  ONNX_OPERATOR_NOT_SUPPORTED(kOnnxDomain, HammingWindow,             k_not_supported_in_glow_fronted);
+  ONNX_OPERATOR_NOT_SUPPORTED(kOnnxDomain, HannWindow,                k_not_supported_in_glow_fronted);
+  ONNX_OPERATOR_NOT_SUPPORTED(kOnnxDomain, HardSwish,                 k_not_supported_in_glow_fronted);
+  ONNX_OPERATOR_NOT_SUPPORTED(kOnnxDomain, Hardmax,                   k_not_supported_in_glow_fronted);
+  ONNX_OPERATOR_NOT_SUPPORTED(kOnnxDomain, ImageDecoder,              k_not_supported_in_glow_fronted);
+  ONNX_OPERATOR_NOT_SUPPORTED(kOnnxDomain, IsInf,                     k_not_supported_in_glow_fronted);
+  ONNX_OPERATOR_NOT_SUPPORTED(kOnnxDomain, IsNaN,                     k_not_supported_in_glow_fronted);
+  ONNX_OPERATOR_NOT_SUPPORTED(kOnnxDomain, LessOrEqual,               k_not_supported_in_glow_fronted);
+  ONNX_OPERATOR_NOT_SUPPORTED(kOnnxDomain, LpNormalization,           k_not_supported_in_glow_fronted);
+  ONNX_OPERATOR_NOT_SUPPORTED(kOnnxDomain, LpPool,                    k_not_supported_in_glow_fronted);
+  ONNX_OPERATOR_NOT_SUPPORTED(kOnnxDomain, MatMulInteger,             k_not_supported_in_glow_fronted);
+  ONNX_OPERATOR_NOT_SUPPORTED(kOnnxDomain, MaxRoiPool,                k_not_supported_in_glow_fronted);
+  ONNX_OPERATOR_NOT_SUPPORTED(kOnnxDomain, MaxUnpool,                 k_not_supported_in_glow_fronted);
+  ONNX_OPERATOR_NOT_SUPPORTED(kOnnxDomain, MeanVarianceNormalization, k_not_supported_in_glow_fronted);
+  ONNX_OPERATOR_NOT_SUPPORTED(kOnnxDomain, MelWeightMatrix,           k_not_supported_in_glow_fronted);
+  ONNX_OPERATOR_NOT_SUPPORTED(kOnnxDomain, Mish,                      k_not_supported_in_glow_fronted);
+  ONNX_OPERATOR_NOT_SUPPORTED(kOnnxDomain, Mod,                       k_not_supported_in_glow_fronted);
+  ONNX_OPERATOR_NOT_SUPPORTED(kOnnxDomain, Multinomial,               k_not_supported_in_glow_fronted);
+  ONNX_OPERATOR_NOT_SUPPORTED(kOnnxDomain, NegativeLogLikelihoodLoss, k_not_supported_in_glow_fronted);
+  ONNX_OPERATOR_NOT_SUPPORTED(kOnnxDomain, OneHot,                    k_not_supported_in_glow_fronted); // obs: glow frontend supports BatchOneHot
+  ONNX_OPERATOR_NOT_SUPPORTED(kOnnxDomain, Optional,                  k_not_supported_in_glow_fronted);
+  ONNX_OPERATOR_NOT_SUPPORTED(kOnnxDomain, OptionalGetElement,        k_not_supported_in_glow_fronted);
+  ONNX_OPERATOR_NOT_SUPPORTED(kOnnxDomain, OptionalHasElement,        k_not_supported_in_glow_fronted);
+  ONNX_OPERATOR_NOT_SUPPORTED(kOnnxDomain, QLinearConv,               k_not_supported_in_glow_fronted);
+  ONNX_OPERATOR_NOT_SUPPORTED(kOnnxDomain, QLinearMatMul,             k_not_supported_in_glow_fronted);
+  ONNX_OPERATOR_NOT_SUPPORTED(kOnnxDomain, RandomNormal,              k_not_supported_in_glow_fronted);
+  ONNX_OPERATOR_NOT_SUPPORTED(kOnnxDomain, RandomNormalLike,          k_not_supported_in_glow_fronted);
+  ONNX_OPERATOR_NOT_SUPPORTED(kOnnxDomain, RandomUniform,             k_not_supported_in_glow_fronted);
+  ONNX_OPERATOR_NOT_SUPPORTED(kOnnxDomain, RandomUniformLike,         k_not_supported_in_glow_fronted);
+  ONNX_OPERATOR_NOT_SUPPORTED(kOnnxDomain, ReduceL1,                  k_not_supported_in_glow_fronted);
+  ONNX_OPERATOR_NOT_SUPPORTED(kOnnxDomain, ReduceLogSum,              k_not_supported_in_glow_fronted);
+  ONNX_OPERATOR_NOT_SUPPORTED(kOnnxDomain, ReduceLogSumExp,           k_not_supported_in_glow_fronted);
+  ONNX_OPERATOR_NOT_SUPPORTED(kOnnxDomain, ReduceMax,                 k_not_supported_in_glow_backend);
+  ONNX_OPERATOR_NOT_SUPPORTED(kOnnxDomain, ReduceProd,                k_not_supported_in_glow_backend);
+  ONNX_OPERATOR_NOT_SUPPORTED(kOnnxDomain, ReverseSequence,           k_not_supported_in_glow_backend);
+  ONNX_OPERATOR_NOT_SUPPORTED(kOnnxDomain, RoiAlign,                  k_not_supported_in_glow_backend);
+  ONNX_OPERATOR_NOT_SUPPORTED(kOnnxDomain, Round,                     k_not_supported_in_glow_backend);
+  ONNX_OPERATOR_NOT_SUPPORTED(kOnnxDomain, STFT,                      k_not_supported_in_glow_backend);
+  ONNX_OPERATOR_NOT_SUPPORTED(kOnnxDomain, Scan,                      k_not_supported_in_glow_fronted);
+  ONNX_OPERATOR_NOT_SUPPORTED(kOnnxDomain, Scatter,                   k_not_supported_in_glow_fronted); // obs: strangely glow supports ScatterData & ScatterAssign
+  ONNX_OPERATOR_NOT_SUPPORTED(kOnnxDomain, ScatterElements,           k_not_supported_in_glow_fronted); // obs: strangely glow supports ScatterData & ScatterAssign
+  ONNX_OPERATOR_NOT_SUPPORTED(kOnnxDomain, Selu,                      k_not_supported_in_glow_fronted);
+  ONNX_OPERATOR_NOT_SUPPORTED(kOnnxDomain, SequenceAt,                k_not_supported_in_glow_fronted);
+  ONNX_OPERATOR_NOT_SUPPORTED(kOnnxDomain, SequenceConstruct,         k_not_supported_in_glow_fronted);
+  ONNX_OPERATOR_NOT_SUPPORTED(kOnnxDomain, SequenceEmpty,             k_not_supported_in_glow_fronted);
+  ONNX_OPERATOR_NOT_SUPPORTED(kOnnxDomain, SequenceErase,             k_not_supported_in_glow_fronted);
+  ONNX_OPERATOR_NOT_SUPPORTED(kOnnxDomain, SequenceInsert,            k_not_supported_in_glow_fronted);
+  ONNX_OPERATOR_NOT_SUPPORTED(kOnnxDomain, SequenceLength,            k_not_supported_in_glow_fronted);
+  ONNX_OPERATOR_NOT_SUPPORTED(kOnnxDomain, SequenceMap,               k_not_supported_in_glow_fronted);
+  ONNX_OPERATOR_NOT_SUPPORTED(kOnnxDomain, Shrink,                    k_not_supported_in_glow_fronted);
+  ONNX_OPERATOR_NOT_SUPPORTED(kOnnxDomain, Sign,                      k_not_supported_in_glow_backend);
+  ONNX_OPERATOR_NOT_SUPPORTED(kOnnxDomain, Sinh,                      k_not_supported_in_glow_fronted);
+  ONNX_OPERATOR_NOT_SUPPORTED(kOnnxDomain, Size,                      k_not_supported_in_glow_fronted); // obs: glow pytorch frontend supports aten::size
+  ONNX_OPERATOR_NOT_SUPPORTED(kOnnxDomain, SoftmaxCrossEntropyLoss,   k_not_supported_in_glow_fronted);
+  ONNX_OPERATOR_NOT_SUPPORTED(kOnnxDomain, Softplus,                  k_not_supported_in_glow_fronted); // obs: glow pytorch frontend supports aten::softplus
+  ONNX_OPERATOR_NOT_SUPPORTED(kOnnxDomain, Softsign,                  k_not_supported_in_glow_fronted);
+  ONNX_OPERATOR_NOT_SUPPORTED(kOnnxDomain, SplitToSequence,           k_not_supported_in_glow_fronted);
+  ONNX_OPERATOR_NOT_SUPPORTED(kOnnxDomain, StringNormalizer,          k_not_supported_in_glow_fronted);
+  ONNX_OPERATOR_NOT_SUPPORTED(kOnnxDomain, TfIdfVectorizer,           k_not_supported_in_glow_fronted);
+  ONNX_OPERATOR_NOT_SUPPORTED(kOnnxDomain, ThresholdedRelu,           k_not_supported_in_glow_fronted);
+  ONNX_OPERATOR_NOT_SUPPORTED(kOnnxDomain, Tan,                       k_not_supported_in_glow_fronted);
+  ONNX_OPERATOR_NOT_SUPPORTED(kOnnxDomain, Unique,                    k_not_supported_in_glow_fronted);
+
+  // clang-format on
+  // any missing op will be considered non-supported
+}
+
+void EtGlowOpManager::UpdateCapabilityMode(CapabilityMode new_mode) {
+  if (mode_ != new_mode) {
+    mode_ = new_mode;
+    etglow_ops_map_.clear();
+    RegisterSupportedOps();
+  }
+}
+
+EtGlowNodeSupportResult EtGlowOpManager::IsNodeSupported(const Node* node, const GraphViewer& graph_viewer) const {
+  if (node == nullptr) {
+    return make_unsupported("node cannot be nullptr");
+  }
+  const auto key = node->Domain() + node->OpType();
+  auto it = etglow_ops_map_.find(key);
+  if (it == etglow_ops_map_.end()) {
+    return make_unsupported("EtGlowOpManager Unknown OpType: " + node->OpType() + " from Domain: " + node->Domain());
+  }
+  return it->second->Supported(*node, graph_viewer);
+}
+
+}  // namespace onnxruntime
diff --git a/onnxruntime/core/providers/etglow/etglow_op_manager.h b/onnxruntime/core/providers/etglow/etglow_op_manager.h
new file mode 100644
index 0000000000..52441c2584
--- /dev/null
+++ b/onnxruntime/core/providers/etglow/etglow_op_manager.h
@@ -0,0 +1,27 @@
+// Copyright (c) Microsoft Corporation. All rights reserved.
+// Licensed under the MIT License.
+
+#pragma once
+
+#include "etglow_node_capability.h"
+#include "etglow_execution_provider_info.h"
+#include <unordered_map>
+
+namespace onnxruntime {
+
+class EtGlowOpManager {
+ public:
+  explicit EtGlowOpManager(const ONNXSymbolsShapes& supp_param_shapes);
+
+  void RegisterSupportedOps();
+  void UpdateCapabilityMode(CapabilityMode new_mode);
+
+  EtGlowNodeSupportResult IsNodeSupported(const Node* node, const GraphViewer& graph_viewer) const;
+
+ private:
+  CapabilityMode mode_ = CapabilityMode::DEFAULT;
+  ONNXSymbolsShapes user_parametric_shapes_replacements_{};
+  std::unordered_map<std::string, std::unique_ptr<EtGlowNodeCapability>> etglow_ops_map_;
+};
+
+}  // namespace onnxruntime
diff --git a/onnxruntime/core/providers/etglow/etglow_profiler.cc b/onnxruntime/core/providers/etglow/etglow_profiler.cc
new file mode 100644
index 0000000000..48a74255e6
--- /dev/null
+++ b/onnxruntime/core/providers/etglow/etglow_profiler.cc
@@ -0,0 +1,265 @@
+#include "etglow_profiler.h"
+#include "etglow_execution_provider.h"
+#include "etglow_execution_provider_utils.h"
+
+#include <et-trace-utils/cereal/perfetto_trace_sink.h>
+#include <et-trace-utils/perfetto/trace_json_printer.h>
+#include <et-trace-utils/support/frequency.h>
+#include <et-trace-utils/trace_merge.h>
+#include <et-trace-utils/trace_error.h>
+#include <glow/GlowAPI/etsoc.h>
+
+#include <nlohmann/json.hpp>
+#include <unistd.h>
+#include <iostream>
+#include <thread>
+#include <regex>
+
+namespace onnxruntime::profiling {
+
+void RemoveQuotes(std::string& s) {
+  if (s.size() > 1 && s.front() == '"' && s.back() == '"') {
+    if (s.size() == 2) {
+      s.erase();
+    } else {
+      s.erase(s.begin());
+      s.erase(s.end() - 1);
+    }
+  }
+}
+
+std::unordered_map<std::string, std::string> GetEvtExtraArgs(std::optional<nlohmann::json> opt_args) {
+  std::unordered_map<std::string, std::string> result;
+  if (!opt_args.has_value()) {
+    return result;
+  }
+  if (const nlohmann::json& args = opt_args.value(); args.is_object()) {
+    for (const auto& [key, value] : args.get<std::unordered_map<std::string, nlohmann::json>>()) {
+      std::string s = to_string(value);
+      if (s.empty()) {
+        continue;
+      }
+      RemoveQuotes(s);
+      result[key] = s;
+    }
+  } else {
+    // fallback to dumping everything in "properties"
+    auto args_as_str = to_string(args);
+    if (!args_as_str.empty()) {
+      result["properties"] = args_as_str;
+    }
+  }
+  return result;
+}
+
+auto phaseToEventType(char phase) -> EventType {
+  switch (phase) {
+    case 'X':
+      return EventType::Complete;
+    case 'i':
+      return EventType::Instant;
+    case 'C':
+      return EventType::Counter;
+    case 'M':
+      return EventType::Metadata;
+  }
+  return EventType::Complete;
+};
+
+class EtGlowPerfettoEventRecordSink : public et_tut::perfetto::TraceSink {
+ public:
+  explicit EtGlowPerfettoEventRecordSink(TimePoint start_time, Events& events, const std::unordered_map<std::thread::id, unsigned int>& thread_map) : profiling_start_time_{start_time}, events_{events} {
+    auto convertThreadIdToUInt64 = [](const std::thread::id& key) -> uint64_t {
+      std::stringstream ss;
+      ss << key;
+      return std::stoull(ss.str());
+    };
+    for (const auto& [std_tid, syscall_tid] : thread_map) {
+      thread_map_[convertThreadIdToUInt64(std_tid)] = syscall_tid;
+    }
+  }
+
+  EtGlowPerfettoEventRecordSink& operator<<(const et_tut::perfetto::PerfettoEvents& event) override {
+    events_.emplace_back(std::visit(
+        [this](const auto& arg) {
+          using T = std::decay_t<decltype(arg)>;
+          EventRecord evt;
+          if constexpr (std::is_same_v<T, et_tut::perfetto::PerfettoEvent> ||
+                        std::is_same_v<T, et_tut::perfetto::PerfettoTimeEvent> ||
+                        std::is_same_v<T, et_tut::perfetto::PerfettoCompleteEvent>) {
+            evt.type = phaseToEventType(arg.get_phase());
+            evt.name = arg.get_name();
+            evt.pid = static_cast<int>(arg.get_pid());
+            evt.tid = static_cast<int>(arg.get_tid());
+            evt.cat = EventCategory::SESSION_EVENT;
+            evt.args = GetEvtExtraArgs(arg.get_args());
+            // Convert int64_t threadId to perfetto-compatible int32_t
+            // Ignore device events
+            if (arg.get_pid() != k_device_pid) {
+              auto [it, success] = thread_map_.try_emplace(arg.get_tid(), next_thread_id_);
+              if (success) next_thread_id_++;
+              evt.tid = it->second;
+            }
+            // Remove redundant thread ids from thread labels
+            if (arg.get_name() == "thread_name") {
+              // regex matches all trailing decimal characters
+              std::regex numbers_reg("[0-9]+$");
+              evt.args["name"] = std::regex_replace(evt.args["name"], numbers_reg, "");
+            }
+          }
+          if constexpr (std::is_same_v<T, et_tut::perfetto::PerfettoTimeEvent> ||
+                        std::is_same_v<T, et_tut::perfetto::PerfettoCompleteEvent>) {
+            evt.ts = TimeDiffMicroSeconds(profiling_start_time_, arg.get_timestamp());
+          }
+          if constexpr (std::is_same_v<T, et_tut::perfetto::PerfettoCompleteEvent>) {
+            evt.dur = TimeDiffMicroSeconds(arg.get_timestamp(), arg.get_timestamp() + arg.get_duration());
+          }
+          return evt;
+        },
+        event));
+    return *this;
+  }
+
+ private:
+  TimePoint profiling_start_time_;
+  Events& events_;
+  std::unordered_map<uint64_t, unsigned int> thread_map_;
+  unsigned int next_thread_id_{1};
+  const uint64_t k_device_pid{3};
+};
+
+struct MergeOptions {
+  std::optional<fs::path> glow_trace;
+  std::optional<fs::path> neuralizer_trace;
+  std::optional<fs::path> neuralizer_log;
+  std::optional<fs::path> runtime_trace;
+  fs::path output;
+  et_tut::OutputTraceType type = et_tut::OutputTraceType::Combined;
+};
+
+et_tut::MergeResult merge_traces(const MergeOptions& opts, TimePoint start_time, Events& events, const std::unordered_map<std::thread::id, unsigned int>& thread_map) {
+  enum class MakeIStreamErrorCode {
+    CouldNotGetOpenStream
+  };
+  using MakeIStreamResult = tl::expected<std::unique_ptr<std::ifstream>, et_tut::Error<MakeIStreamErrorCode>>;
+  auto make_istream = [](const std::optional<fs::path>& trace)
+      -> MakeIStreamResult {
+    if (!trace.has_value()) {
+      return nullptr;
+    }
+    auto ifs = std::make_unique<std::ifstream>(trace.value());
+    if (!ifs->good()) {
+      return et_tut::make_error(MakeIStreamErrorCode::CouldNotGetOpenStream,
+                                "Unable to open: " + trace.value().native());
+    }
+    return ifs;
+  };
+
+  auto glow_stream = make_istream(opts.glow_trace);
+  if (!glow_stream) {
+    return make_error(et_tut::MergeErrorCode::StreamError);
+  }
+  auto runtime_stream = make_istream(opts.runtime_trace);
+  if (!runtime_stream) {
+    return make_error(et_tut::MergeErrorCode::StreamError);
+  }
+  auto neuralizer_host_stream = make_istream(opts.neuralizer_trace);
+  if (!neuralizer_host_stream) {
+    return make_error(et_tut::MergeErrorCode::StreamError);
+  }
+  auto neuralizer_device_stream = make_istream(opts.neuralizer_log);
+  if (!neuralizer_device_stream) {
+    return make_error(et_tut::MergeErrorCode::StreamError);
+  }
+
+  auto perfettoEvent2OrtEventSink = std::make_unique<EtGlowPerfettoEventRecordSink>(start_time, events, thread_map);
+  auto cerealSink = make_cereal_sink(std::move(perfettoEvent2OrtEventSink), opts.type);
+  return merge_traces(*cerealSink, opts.type,
+                      std::move(glow_stream.value()),
+                      std::move(runtime_stream.value()),
+                      std::move(neuralizer_host_stream.value()),
+                      std::move(neuralizer_device_stream.value()));
+}
+
+inline const char* GetDateFormatString() {
+  return "%Y-%m-%d_%H-%M-%S";
+}
+
+std::string GetStartTimeString(TimePoint start_time) {
+  std::time_t in_time_t = TimePoint::clock::to_time_t(start_time);
+  std::tm start_time_tm = *std::localtime(&in_time_t);  // NOLINT
+  std::stringstream ss;
+  ss << std::put_time(&start_time_tm, GetDateFormatString());
+  return ss.str();
+}
+
+bool EtGlowProfiler::StartProfiling(TimePoint start_time) {
+  LOGS_DEFAULT(INFO) << "tracing started";
+  auto get_trace_path_if = [this](bool enable, const fs::path& trace_filename) {
+    return enable ? traces_dir_prefix_ / trace_filename : std::filesystem::path{};
+  };
+  std::filesystem::create_directory(traces_dir_prefix_);
+  std::string traces_file_prefix = GetStartTimeString(start_time);
+  etsoc::GlowAPI::TraceOptions toptions;
+  toptions.trace_glow_path = get_trace_path_if(trace_options_.trace_glow_enable, traces_file_prefix + "_glow.json");
+
+  etsoc::GlowAPI::ETSOCTraceOptions etsoc_toptions;
+  etsoc_toptions.trace_neura_path = get_trace_path_if(trace_options_.trace_neuralizer_enable, traces_file_prefix + "_neura.json");
+  etsoc_toptions.trace_runtime_path = get_trace_path_if(trace_options_.trace_runtime_enable, traces_file_prefix + "_runtime.json");
+  etsoc_toptions.include_neuralizer_nodes = trace_options_.trace_device_kernel_nodes_enable;
+  // OBS trace_kernels_path events are not meant to be merged here
+  etsoc_toptions.trace_kernels_path = get_trace_path_if(trace_options_.trace_device_kernel_inst_enable, traces_file_prefix + "_kernels");
+  toptions.backend_trace_options = etsoc_toptions;
+  if (auto error = glow_api_.startTrace(toptions); error) {
+    LOGS_DEFAULT(WARNING) << glowApiErrMsg(std::move(error), "starting trace");
+    return false;
+  }
+  trace_session_options_.try_emplace(start_time.time_since_epoch().count(), toptions);
+  return true;
+}
+
+void EtGlowProfiler::EndProfiling(TimePoint start_time, Events& events) {
+  LOGS_DEFAULT(INFO) << "tracing finished";
+  if (auto error = glow_api_.stopTrace(); error) {
+    LOGS_DEFAULT(WARNING) << glowApiErrMsg(std::move(error), "stopping trace. Skipping merging.");
+    return;
+  }
+
+  LOGS_DEFAULT(INFO) << "merging traces started";
+  etsoc::GlowAPI::TraceOptions toptions;
+  etsoc::GlowAPI::ETSOCTraceOptions etsoc_toptions;
+  if (auto it = trace_session_options_.find(start_time.time_since_epoch().count());
+      it != trace_session_options_.end()) {
+    toptions = it->second;
+    etsoc_toptions = std::any_cast<etsoc::GlowAPI::ETSOCTraceOptions>(toptions.backend_trace_options);
+    trace_session_options_.erase(it);
+  }
+
+  MergeOptions opts;
+  if (std::filesystem::exists(toptions.trace_glow_path)) {
+    opts.glow_trace = toptions.trace_glow_path;
+  }
+  if (std::filesystem::exists(etsoc_toptions.trace_runtime_path)) {
+    opts.runtime_trace = etsoc_toptions.trace_runtime_path;
+  }
+  if (std::filesystem::exists(etsoc_toptions.trace_neura_path)) {
+    opts.neuralizer_trace = etsoc_toptions.trace_neura_path;
+  }
+  Events generated_events;
+
+  if (auto ok = merge_traces(opts, start_time, generated_events, profiler_thread_map_);
+      !ok) {
+    ORT_THROW("[ETGLOW] Fatal error while merging traces ( " + ok.error().what() + ")");
+  }
+
+  // sort generated events and place them in ORT events
+  std::sort(generated_events.begin(), generated_events.end(), [](const EventRecord& a, const EventRecord& b) {
+    return a.ts < b.ts;
+  });
+  events.reserve(events.size() + generated_events.size());
+  events.insert(events.end(), generated_events.begin(), generated_events.end());
+
+  LOGS_DEFAULT(INFO) << "merging traces finished";
+}
+
+}  // namespace onnxruntime::profiling
diff --git a/onnxruntime/core/providers/etglow/etglow_profiler.h b/onnxruntime/core/providers/etglow/etglow_profiler.h
new file mode 100644
index 0000000000..c297d7e871
--- /dev/null
+++ b/onnxruntime/core/providers/etglow/etglow_profiler.h
@@ -0,0 +1,51 @@
+// Copyright (c) Microsoft Corporation. All rights reserved.
+// Licensed under the MIT License.
+
+#pragma once
+
+#include "core/common/profiler_common.h"
+
+#include "etglow_execution_provider_info.h"
+
+#include <glow/GlowAPI/etsoc.h>
+
+#include <atomic>
+#include <filesystem>
+#include <map>
+#include <mutex>
+#include <vector>
+
+namespace onnxruntime {
+class EtGlowExecutionProvider;
+
+namespace profiling {
+
+class EtGlowProfiler final : public EpProfiler {
+ public:
+  explicit EtGlowProfiler(etsoc::GlowAPI& glow_api, fs::path traces_dir_prefix, const TraceOptions& trace_options, const std::unordered_map<std::thread::id, unsigned int>& profiler_thread_map)
+      : glow_api_{glow_api}, traces_dir_prefix_{traces_dir_prefix}, trace_options_{trace_options}, profiler_thread_map_{profiler_thread_map} {
+    if (traces_dir_prefix.empty()) {
+      traces_dir_prefix_ = std::filesystem::current_path() / "traces";
+    }
+  }
+  ORT_DISALLOW_COPY_ASSIGNMENT_AND_MOVE(EtGlowProfiler);
+  ~EtGlowProfiler() override = default;
+  bool StartProfiling(TimePoint start_time) override;
+  void EndProfiling(TimePoint start_time, Events& events) override;
+  void Start(uint64_t) override {
+    // intentionally empty
+  }
+  void Stop(uint64_t) override{
+      // intentionally empty
+  };
+
+ private:
+  etsoc::GlowAPI& glow_api_;
+  fs::path traces_dir_prefix_;
+  TraceOptions trace_options_;
+  std::unordered_map<uint64_t, etsoc::GlowAPI::TraceOptions> trace_session_options_;
+  const std::unordered_map<std::thread::id, unsigned int>& profiler_thread_map_;
+};
+
+}  // namespace profiling
+}  // namespace onnxruntime
diff --git a/onnxruntime/core/providers/etglow/etglow_provider_factory.cc b/onnxruntime/core/providers/etglow/etglow_provider_factory.cc
new file mode 100644
index 0000000000..eaf9770098
--- /dev/null
+++ b/onnxruntime/core/providers/etglow/etglow_provider_factory.cc
@@ -0,0 +1,251 @@
+// Copyright (c) Microsoft Corporation. All rights reserved.
+// Licensed under the MIT License.
+
+#include "core/common/common.h"
+#include "core/providers/etglow/etglow_provider_factory.h"
+#include "core/providers/etglow/etglow_provider_factory_creator.h"
+#include "core/providers/etglow/etglow_provider_options.h"
+#include "core/providers/etglow/etglow_execution_provider_info.h"
+#include "core/providers/etglow/etglow_execution_provider.h"
+#include "core/providers/shared_library/provider_api.h"
+
+#include "etglow_execution_provider.h"
+#include "etglow_execution_provider_info.h"
+#include "etglow_execution_provider_utils.h"
+#include "etglow_provider_factory_creator.h"
+
+#include <utility>
+#include <iostream>
+
+namespace onnxruntime {
+
+void InitializeRegistry();
+void DeleteRegistry();
+
+struct EtGlowApiFactory {
+ public:
+  std::unique_ptr<etsoc::GlowAPI> CreateGlowAPI(const EtGlowExecutionProviderInfo& info);
+
+ private:
+  void InitGlowGlobalState(const EtGlowExecutionProviderInfo& info) const;
+
+  mutable std::once_flag glow_global_state_flag_;
+};
+
+std::unique_ptr<etsoc::GlowAPI> EtGlowApiFactory::CreateGlowAPI(const EtGlowExecutionProviderInfo& info) {
+  GlowAPI::HostConfig hc = GetGlowHostConfig(info.api_params);
+  const std::string backend_name = "ETSOC";
+  const std::string backend_human_readable_name = "ETSOC";
+  GlowAPI::StringMap<std::string> backend_parameters;
+  if (auto it = info.api_params.find(etglow::api_params::device_config::kBackendParameters); it != info.api_params.end()) {
+    backend_parameters.try_emplace(it->first, std::get<std::string>(it->second));
+  }
+
+  std::vector<std::unique_ptr<GlowAPI::DeviceConfig>> deviceConfigs;
+  deviceConfigs.emplace_back(std::make_unique<GlowAPI::DeviceConfig>(backend_name, backend_human_readable_name, backend_parameters));
+  deviceConfigs.back()->deviceID = info.device_id;
+  if (auto it = info.api_params.find(etglow::api_params::device_config::kDeviceMemory); it != info.api_params.end()) {
+    deviceConfigs.back()->setDeviceMemory(std::stoull(std::get<std::string>(it->second)));
+  }
+  InitGlowGlobalState(info);
+  return std::make_unique<GlowAPI>(backend_name, hc, std::move(deviceConfigs));
+}
+
+void EtGlowApiFactory::InitGlowGlobalState(const EtGlowExecutionProviderInfo& info) const {
+  std::call_once(glow_global_state_flag_, [info]() {
+    auto error = GlowAPI::setETSOCParameters(GetGlowGlobalParams(info.api_params));
+    if (error) {
+      ORT_THROW("GLOW API error setting ETSOC parameters");
+    }
+  });
+}
+
+struct EtGlowProviderFactory : IExecutionProviderFactory {
+  explicit EtGlowProviderFactory(EtGlowExecutionProviderInfo info, EtRt_StreamsRegistry& default_streams_reg) : info_{std::move(info)}, default_streams_reg_{default_streams_reg} {}
+
+  std::unique_ptr<IExecutionProvider> CreateProvider() override {
+    return std::make_unique<EtGlowExecutionProvider>(info_, GetEtGlow(), default_streams_reg_);
+  }
+  std::unique_ptr<etsoc::GlowAPI> GetEtGlow() {
+    return EtGlowApiFactory{}.CreateGlowAPI(info_);
+  }
+
+ private:
+  EtGlowExecutionProviderInfo info_;
+  EtRt_StreamsRegistry& default_streams_reg_;
+};
+
+struct ProviderInfo_EtGlow_Impl final : ProviderInfo_EtGlow {
+  struct StreamsRegistry_Impl final : EtRt_StreamsRegistry {
+    ~StreamsRegistry_Impl() {
+      try {
+        if (!glow_) {
+          // TODO: This condition should be removed and provide another way to check/access runtime streams.
+          std::cerr << "EtGlow provider attemped to access Glow but was already destroyed." << std::endl;
+          return;
+        }
+        auto& rt = GetEtRuntime();
+        for (auto device : rt.getDevices()) {
+          for (auto stream : device_streams_[device]) {
+            rt.destroyStream(stream);
+          }
+        }
+      } catch (const std::exception& e) {
+        LOGS_DEFAULT(WARNING) << "Exception thrown while destroying runtime streams: " << e.what();
+      }
+    }
+
+    rt::IRuntime& GetEtRuntime() {
+      static rt::IRuntime& rt = *(GetEtGlow()->getBackendRuntimeInstance());
+      return rt;
+    }
+
+    rt::StreamId GetStreamForDevice(int device_id) override {
+      std::scoped_lock lock(mutex_get_stream_);
+      if (device_streams_.empty()) {
+        auto& rt = GetEtRuntime();
+        for (auto device : rt.getDevices()) {
+          std::vector<rt::StreamId> streams;
+          for (size_t i = 0; i < kRuntimeStreams; i++) {
+            streams.emplace_back(rt.createStream(device));
+          }
+          device_streams_.try_emplace(device, std::move(streams));
+          next_stream_idx_[device] = 0UL;
+        }
+      }
+      const auto deviceId = static_cast<rt::DeviceId>(device_id);
+      const auto nextStreamPos = next_stream_idx_[deviceId];
+      const auto streamId = device_streams_[deviceId][nextStreamPos];
+      // update next streamId position
+      next_stream_idx_[deviceId] = (nextStreamPos + 1) % kRuntimeStreams;
+      return streamId;
+    }
+    common::Status Synchronize(rt::DeviceId device_id) override {
+      for (auto stream_id : device_streams_[device_id]) {
+        ORT_RETURN_IF_ERROR(Synchronize(stream_id));
+      }
+      return Status::OK();
+    }
+    common::Status Synchronize(rt::StreamId stream_id) override {
+      auto& rt = GetEtRuntime();
+      ORT_RETURN_IF_NOT(rt.waitForStream(stream_id), "[ETGLOW EP] error waiting for stream_id: " + std::to_string(static_cast<int>(stream_id)));
+      return Status::OK();
+    }
+
+    common::Status Synchronize(rt::EventId event_id, rt::StreamId stream_id) override {
+      auto& rt = GetEtRuntime();
+      ORT_RETURN_IF_NOT(rt.waitForEvent(event_id), "[ETGLOW EP] error waiting for event_id: " + std::to_string(static_cast<int>(event_id)) + " from stream_id: " + std::to_string(static_cast<int>(stream_id)));
+      return Status::OK();
+    };
+
+   private:
+    std::shared_ptr<etsoc::GlowAPI> GetEtGlow() {
+      std::scoped_lock lock(mutex_get_glow_);
+      if (!glow_) {
+        glow_ = EtGlowApiFactory{}.CreateGlowAPI(EtGlowExecutionProviderInfo{});
+      }
+      return glow_;
+    }
+
+    static constexpr size_t kRuntimeStreams = 4;
+
+    // we must keep glow alive to ensure IRuntime doesn't get destroyed before we destroy
+    // our default streams
+    std::mutex mutex_get_glow_;
+    std::shared_ptr<etsoc::GlowAPI> glow_;
+
+    // device_streams_ contains for each device a list of streams created
+    // next_stream_idx_, is a vector containing the ids of the stream of such device
+    std::mutex mutex_get_stream_;
+    std::unordered_map<rt::DeviceId, size_t> next_stream_idx_;
+    std::unordered_map<rt::DeviceId, std::vector<rt::StreamId>> device_streams_;
+  };
+  StreamsRegistry_Impl streams_registry_;
+
+  ProviderInfo_EtGlow_Impl() = default;
+
+  EtRt_StreamsRegistry& GetDefaultStreamsRegistry() override {
+    return streams_registry_;
+  }
+
+  int etrtGetDeviceCount() override {
+    auto& rt = streams_registry_.GetEtRuntime();
+    return static_cast<int>(rt.getDevices().size());
+  }
+
+  void etrtMemcpy_HostToDevice(int dst_device_id, void* dst, const void* src, size_t count) override {
+    auto& rt = streams_registry_.GetEtRuntime();
+    rt::StreamId streamId = streams_registry_.GetStreamForDevice(dst_device_id);
+    auto eventId = rt.memcpyHostToDevice(streamId, static_cast<const std::byte*>(src), static_cast<std::byte*>(dst), count);
+    ORT_THROW_IF_ERROR(streams_registry_.Synchronize(eventId, streamId));
+  }
+
+  void etrtMemcpy_DeviceToHost(int src_device_id, void* dst, const void* src, size_t count) override {
+    auto& rt = streams_registry_.GetEtRuntime();
+    rt::StreamId streamId = streams_registry_.GetStreamForDevice(src_device_id);
+    constexpr bool barrier = true;
+    auto eventId = rt.memcpyDeviceToHost(streamId, static_cast<const std::byte*>(src), static_cast<std::byte*>(dst), count, barrier);
+    ORT_THROW_IF_ERROR(streams_registry_.Synchronize(eventId, streamId));
+  }
+
+  void etrtDeviceSynchronize(int device_id) override {
+    ORT_THROW_IF_ERROR(streams_registry_.Synchronize(static_cast<rt::DeviceId>(device_id)));
+  }
+
+  std::shared_ptr<IAllocator> CreateEtGlowAllocator(int16_t device_id, size_t mem_limit, ArenaExtendStrategy arena_extend_strategy) override {
+    return EtGlowExecutionProvider::CreateEtGlowAllocator(device_id, mem_limit, arena_extend_strategy, &streams_registry_.GetEtRuntime());
+  }
+
+} g_info;
+
+struct EtGlow_Provider : Provider {
+  void* GetInfo() override {  // NOSONAR
+    return &g_info;
+  }
+
+  std::shared_ptr<IExecutionProviderFactory> CreateExecutionProviderFactory(const void* void_params) override {
+    const auto& provider_params = *static_cast<const OrtEtGlowProviderOptions*>(void_params);
+
+    EtGlowExecutionProviderInfo info{};
+    info.device_id = static_cast<OrtDevice::DeviceId>(provider_params.device_id);
+    info.device_mem_limit = provider_params.device_mem_limit;
+    info.arena_extend_strategy = static_cast<ArenaExtendStrategy>(provider_params.arena_extend_strategy);
+    info.compile_only = provider_params.et_compile_only != 0;
+    info.dump_subgraphs = provider_params.et_dump_subgraphs != 0;
+    info.greedy = provider_params.et_greedy != 0;
+    info.offline_mode = provider_params.et_offline_mode != 0;
+    info.fail_if_cannot_run_whole_graph = provider_params.et_fail_if_cannot_run_whole_graph != 0;
+    info.onnx_symbols = ParseONNXSymbolsFromUserString(provider_params.et_onnx_symbols);
+    info.api_params = ParseGlowAPIOptionsFromUserString(provider_params.et_glow_api_params);
+    info.bundle_cache_prefix = provider_params.et_bundle_cache_prefix == nullptr ? fs::path{} : provider_params.et_bundle_cache_prefix;
+    info.export_bundle_path = provider_params.et_export_bundle_path == nullptr ? fs::path{} : provider_params.et_export_bundle_path;
+    info.traces_dir_prefix = provider_params.et_traces_prefix == nullptr ? fs::path{} : provider_params.et_traces_prefix;
+    info.trace_options = TraceOptions(provider_params.et_traces_flags);
+
+    return std::make_shared<EtGlowProviderFactory>(info, static_cast<ProviderInfo_EtGlow*>(GetInfo())->GetDefaultStreamsRegistry());
+  }
+
+  ProviderOptions GetProviderOptions(const void* provider_options) override {
+    const auto& options = *static_cast<const OrtEtGlowProviderOptions*>(provider_options);
+    return onnxruntime::EtGlowExecutionProviderInfo::ToProviderOptions(options);
+  }
+
+  void UpdateProviderOptions(void* provider_options, const ProviderOptions& options) override {
+    EtGlowExecutionProviderInfo::UpdateProviderOptions(provider_options, options);
+  }
+
+  void Initialize() override {
+    InitializeRegistry();
+  }
+
+  void Shutdown() override {
+    DeleteRegistry();
+  }
+
+} g_provider;
+
+EtGlow_Provider* GetProvider() {
+  return &g_provider;
+}
+
+}  // end namespace onnxruntime
diff --git a/onnxruntime/core/providers/etglow/etglow_provider_factory.h b/onnxruntime/core/providers/etglow/etglow_provider_factory.h
new file mode 100644
index 0000000000..cd85a8f07e
--- /dev/null
+++ b/onnxruntime/core/providers/etglow/etglow_provider_factory.h
@@ -0,0 +1,50 @@
+// Copyright (c) Microsoft Corporation. All rights reserved.
+// Licensed under the MIT License.
+
+#pragma once
+
+#include <core/framework/arena_extend_strategy.h>
+#include "onnxruntime_c_api.h"
+#include "core/providers/etglow/etglow_provider_options.h"
+#include "core/common/common.h"
+
+namespace rt {
+enum class EventId : uint16_t;
+enum class StreamId : int;
+enum class DeviceId : int;
+}  // namespace rt
+
+namespace onnxruntime {
+class IAllocator;
+
+struct EtRt_StreamsRegistry {
+  virtual rt::StreamId GetStreamForDevice(int device_id) = 0;
+  virtual common::Status Synchronize(rt::DeviceId device_id) = 0;
+  virtual common::Status Synchronize(rt::StreamId stream_id) = 0;
+  virtual common::Status Synchronize(rt::EventId event_id, rt::StreamId stream_id) = 0;
+
+ protected:
+  ~EtRt_StreamsRegistry() = default;  // Can only be destroyed through a subclass instance
+};
+
+struct ProviderInfo_EtGlow {
+  // This function is the entry point to EtGlow EP's UT cases.
+  // All tests are only called from onnxruntime_test_all.
+  virtual void TestsEntrypoint() {
+    ORT_NOT_IMPLEMENTED(__FUNCTION__, " is only implements in test code path.");
+  }
+
+  virtual EtRt_StreamsRegistry& GetDefaultStreamsRegistry() = 0;
+
+  virtual int etrtGetDeviceCount() = 0;
+  virtual void etrtMemcpy_HostToDevice(int dst_device_id, void* dst, const void* src, size_t count) = 0;  // NOSONAR
+  virtual void etrtMemcpy_DeviceToHost(int src_device_id, void* dst, const void* src, size_t count) = 0;  // NOSONAR
+  virtual void etrtDeviceSynchronize(int device_id) = 0;
+
+  virtual std::shared_ptr<IAllocator> CreateEtGlowAllocator(int16_t device_id, size_t mem_limit, onnxruntime::ArenaExtendStrategy arena_extend_strategy) = 0;
+
+ protected:
+  ~ProviderInfo_EtGlow() = default;  // Can only be destroyed through a subclass instance
+};
+
+}  // end namespace onnxruntime
diff --git a/onnxruntime/core/providers/etglow/etglow_provider_factory_creator.h b/onnxruntime/core/providers/etglow/etglow_provider_factory_creator.h
new file mode 100644
index 0000000000..5704bc9bc4
--- /dev/null
+++ b/onnxruntime/core/providers/etglow/etglow_provider_factory_creator.h
@@ -0,0 +1,24 @@
+// Copyright (c) Microsoft Corporation. All rights reserved.
+// Licensed under the MIT License.
+
+#pragma once
+
+#include <memory>
+
+#include "core/providers/providers.h"
+#include "core/framework/provider_options.h"
+
+#include <unordered_map>
+
+struct OrtEtGlowProviderOptions;
+
+namespace onnxruntime {
+struct SessionOptions;
+// defined in provider_bridge_ort.cc
+struct EtGlowProviderFactoryCreator {
+  static std::shared_ptr<IExecutionProviderFactory> Create(ProviderOptions* provider_options_map,
+                                                           const SessionOptions* session_options);
+  static std::shared_ptr<IExecutionProviderFactory> Create(const OrtEtGlowProviderOptions* provider_options);
+};
+
+}  // namespace onnxruntime
diff --git a/onnxruntime/core/providers/etglow/etglow_provider_interface.cc b/onnxruntime/core/providers/etglow/etglow_provider_interface.cc
new file mode 100644
index 0000000000..d8a74b5894
--- /dev/null
+++ b/onnxruntime/core/providers/etglow/etglow_provider_interface.cc
@@ -0,0 +1,18 @@
+// Copyright (c) Microsoft Corporation. All rights reserved.
+// Licensed under the MIT License.
+
+#include "core/session/onnxruntime_c_api.h"
+
+namespace onnxruntime {
+struct Provider;
+struct EtGlow_Provider;
+EtGlow_Provider* GetProvider();
+}  // namespace onnxruntime
+
+extern "C" {
+
+ORT_API(onnxruntime::Provider*, GetProvider) {
+  return reinterpret_cast<onnxruntime::Provider*>(onnxruntime::GetProvider());
+}
+
+}  // end extern "C"
diff --git a/onnxruntime/core/providers/etglow/etglow_stream_handle.cc b/onnxruntime/core/providers/etglow/etglow_stream_handle.cc
new file mode 100644
index 0000000000..561bb682d0
--- /dev/null
+++ b/onnxruntime/core/providers/etglow/etglow_stream_handle.cc
@@ -0,0 +1,74 @@
+// Copyright (c) Microsoft Corporation. All rights reserved.
+// Licensed under the MIT License.
+#include "core/providers/etglow/etglow_stream_handle.h"
+#include "core/providers/etglow/etglow_provider_factory.h"
+#include "core/providers/shared_library/provider_api.h"
+
+#include "runtime/IRuntime.h"
+
+namespace onnxruntime {
+
+EtRuntimeNotification::EtRuntimeNotification(EtGlowStream& s) : synchronize::Notification(s){};
+
+void EtRuntimeNotification::Activate() {
+  // Esperanto host runtime does not have an API to record user events in a stream.
+  // ---
+  // Instead we'll force synchronizing the whole stream to be conservative
+}
+
+void EtRuntimeNotification::wait_on_device(Stream& device_stream) const {
+  ORT_ENFORCE(device_stream.GetDevice().Type() == OrtDevice::GPU);
+  // Esperanto host runtime does not have an API to synchronize 2 streams asynchronously.
+  // Instead, we'll block the current thread and flush both streams.
+  auto& etglow_stream = dynamic_cast<EtGlowStream&>(device_stream);
+  etglow_stream.Flush();
+  wait_on_host();
+}
+
+void EtRuntimeNotification::wait_on_host() const {
+  // Esperanto host runtime does not have an API to record user events in a stream.
+  // ---
+  // Instead we'll force synchronizing the whole stream to be conservative
+  auto& etglow_stream = dynamic_cast<EtGlowStream&>(stream_);
+  etglow_stream.Flush();
+}
+
+////
+
+EtGlowStream::EtGlowStream(rt::StreamId stream, const OrtDevice& device, EtRt_StreamsRegistry& streams_reg)
+    : Stream(static_cast<void*>(&stream_), device), streams_reg_{streams_reg}, stream_{stream} {
+}
+
+std::unique_ptr<synchronize::Notification> EtGlowStream::CreateNotification(size_t) {
+  return std::make_unique<EtRuntimeNotification>(*this);
+}
+
+void EtGlowStream::Flush() {
+  LOGS_DEFAULT(INFO) << "[ETGLOW EP] flushing stream_id: " << static_cast<int>(stream_);
+  ORT_THROW_IF_ERROR(streams_reg_.Synchronize(stream_));
+}
+
+void WaitEtRuntimeNotificationOnDevice(Stream& stream, synchronize::Notification& notification) {
+  // 'stream' corresponds to a 'GPU' stream (in this case a EtGlowStream)
+  dynamic_cast<EtRuntimeNotification*>(&notification)->wait_on_device(stream);
+};
+
+void WaitEtRuntimeNotificationOnHost(Stream& /*stream*/, synchronize::Notification& notification) {
+  // 'stream' corresponds to a 'CPU' stream (which cpu EP doesn't implement)
+  // EtRuntimeNotification must be able to synchronize the host by itself
+  dynamic_cast<EtRuntimeNotification*>(&notification)->wait_on_host();
+};
+
+void RegisterEtRuntimeStreamHandles(IStreamCommandHandleRegistry& stream_handle_registry, OrtDevice::DeviceType device_type, const AllocatorPtr&, EtRt_StreamsRegistry& streams_reg) {
+  const OrtDevice::DeviceType notification_device_type = device_type;
+  stream_handle_registry.RegisterWaitFn(notification_device_type, device_type, WaitEtRuntimeNotificationOnDevice);
+  stream_handle_registry.RegisterWaitFn(notification_device_type, OrtDevice::CPU, WaitEtRuntimeNotificationOnHost);
+
+  CreateStreamFn create_stream_fn = [&streams_reg](const OrtDevice& device) {
+    auto stream_id = streams_reg.GetStreamForDevice(device.Id());
+    return std::make_unique<EtGlowStream>(stream_id, device, streams_reg);
+  };
+  stream_handle_registry.RegisterCreateStreamFn(device_type, std::move(create_stream_fn));
+}
+
+}  // end namespace onnxruntime
diff --git a/onnxruntime/core/providers/etglow/etglow_stream_handle.h b/onnxruntime/core/providers/etglow/etglow_stream_handle.h
new file mode 100644
index 0000000000..8348b700ea
--- /dev/null
+++ b/onnxruntime/core/providers/etglow/etglow_stream_handle.h
@@ -0,0 +1,46 @@
+// Copyright (c) Microsoft Corporation. All rights reserved.
+// Licensed under the MIT License.
+
+#pragma once
+
+#include "core/framework/stream_handles.h"
+#include "runtime/IRuntime.h"
+
+#include <stack>
+
+namespace onnxruntime {
+struct EtRt_StreamsRegistry;
+struct EtGlowStream;
+
+struct EtRuntimeNotification : public synchronize::Notification {
+  using synchronize::Notification::Notification;  // Directly inherit parent constructor
+  explicit EtRuntimeNotification(EtGlowStream& s);
+
+  void Activate() override;
+  void wait_on_device(Stream& device_stream) const;
+  void wait_on_host() const;
+};
+
+struct EtGlowStream : Stream {
+  EtGlowStream(rt::StreamId stream, const OrtDevice& device, EtRt_StreamsRegistry& streams_reg);
+  ~EtGlowStream() override = default;
+
+  EtGlowStream(const EtGlowStream&) = delete;
+  EtGlowStream& operator=(const EtGlowStream&) = delete;
+
+  EtGlowStream(EtGlowStream&&) noexcept = delete;
+  EtGlowStream& operator=(EtGlowStream&&) noexcept = delete;
+
+  std::unique_ptr<synchronize::Notification> CreateNotification(size_t /*num_consumers*/) override;
+  void Flush() override;
+
+  rt::StreamId GetStreamId() const { return stream_; }
+
+ private:
+  EtRt_StreamsRegistry& streams_reg_;
+  rt::StreamId stream_;
+};
+
+void RegisterEtRuntimeStreamHandles(IStreamCommandHandleRegistry& stream_handle_registry, OrtDevice::DeviceType device_type, const AllocatorPtr&, EtRt_StreamsRegistry& streams_reg);
+
+}  // end namespace onnxruntime
\ No newline at end of file
diff --git a/onnxruntime/core/providers/etglow/exported_symbols.lst b/onnxruntime/core/providers/etglow/exported_symbols.lst
new file mode 100644
index 0000000000..f4c4141259
--- /dev/null
+++ b/onnxruntime/core/providers/etglow/exported_symbols.lst
@@ -0,0 +1 @@
+_GetProvider
diff --git a/onnxruntime/core/providers/etglow/symbols.def b/onnxruntime/core/providers/etglow/symbols.def
new file mode 100644
index 0000000000..4ec2f7914c
--- /dev/null
+++ b/onnxruntime/core/providers/etglow/symbols.def
@@ -0,0 +1,2 @@
+EXPORTS
+   GetProvider
diff --git a/onnxruntime/core/providers/etglow/version_script.lds b/onnxruntime/core/providers/etglow/version_script.lds
new file mode 100644
index 0000000000..2c8e9c4b3e
--- /dev/null
+++ b/onnxruntime/core/providers/etglow/version_script.lds
@@ -0,0 +1,9 @@
+#_init and _fini should be local
+VERS_1.0 {
+  global:
+    GetProvider;
+
+  # Hide everything else.
+  local:
+    *;
+};
